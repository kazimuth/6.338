% Created 2019-12-16 Mon 10:40
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage{iclr2020_conference,times}
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\newcommand{\xv}[0]{\mathbf{x}}
\newcommand{\yv}[0]{\mathbf{y}}
\newcommand{\zv}[0]{\mathbf{z}}
\newcommand{\fv}[0]{\mathbf{f}}
\newcommand{\J}[0]{\mathbf{J}}
\newcommand{\gv}[0]{\mathbf{g}}
\newcommand{\hv}[0]{\mathbf{h}}
\newcommand{\hxo}[0]{\mathbf{h}_0}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother
\newcommand*{\approxident}{%
\mathrel{\vcenter{\offinterlineskip
\hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}\vskip}}}
% uhhh
\renewcommand*{\tableofcontents}{}
\iclrfinaltrue
\author{James Gilles}
\date{01 November 2019}
\title{The Effect of Pruning on Adversarial Vulnerability}
\hypersetup{
 pdfauthor={James Gilles},
 pdftitle={The Effect of Pruning on Adversarial Vulnerability},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.2.6)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\today

\section{The effect of pruning in standard training on adversarial accuracy}
\label{sec:org708dca6}

The simplest question I've been trying to answer is: what is the effect of deep neural network pruning on adversarial vulnerability?
Does it provide any form of defense, or increase vulnerability?

Adversarial examples present thorny theoretical and practical problems.
Models that are not resilient to small perturbations are not safe to deploy in many situations;
moreover, they highlight our fundamental lack of understanding of the internal function of deep neural networks.
Developing defenses against them is critical for improving the trustworthiness of deep networks.

One key defense, originally identified in \citep{TowardsResistantAdversarial}, is simply adding more capacity to networks: doubling or quadrupling
the number of channels in every layer of a network. This improves resilience, but bloats network model sizes and increases runtime cost.

One approach to ameliorating these issues is network pruning: zeroing weights or whole channels, to reclaim space, and potentially speed up network
evaluation. Unfortunately, pruning can often reduce network accuracy; it's also possible it would make networks more or less vulnerable to adversaries.
Several other works have attempted to apply various kinds of pruning/compression in the adversarial setting
\citep{ToCompressOrnot} \citep{RobustnessOrCompression} \citep{DefensiveQuantization}.

The difference in my approach is that I have access to models trained using "network rewinding", as introduced by \citep{StabilizingLTH}. This is a
new style of network pruning that, while computationally expensive, can actually \textbf{increase} network accuracy, and also retain accuracy longer into the pruning
regimen.

Here I'll try attacking some pruned networks over the course of training, comparing their standard and adversarial accuracies.

I'm comparing two types of pruning, iterated fine-tuning and iterated rewinding. The networks are ResNetV1-20s trained on CIFAR10. The attack I'm using is
untargeted Projected Gradient Descent, using 5 iterations, the \(l_\infty\) norm, and an epsilon of .004. \footnote{This is the smallest reasonable epsilon to use, corresponding to changing each 8-bit RGBA channel in the image by a maximum of 1. Larger epsilons saturate adversarial accuracy immediately, making them not useful for comparison. Epsilon is applied on an image with channel values in the range \([0,1]\), and clipped; that is, the attack is applied before data normalization.]

We see that adversarial accuracy starts out very low and is \textbf{reduced} by additional pruning, similar to standard accuracy.

Note: to read the x-axis, know that each pruning iteration reduces network density by approximately 20\%, and then fully retrains the network.
See figure \ref{fig:marginal} for a plot of network density over the course of pruning.

\newpage

To compare how quickly adversarial accuracy is reduced compared to standard accuracy, we can normalize all training runs by their starting
accuracy.

We can now see that adversarial accuracy drops off \textbf{faster} than standard accuracy over the course of network pruning. It's also more chaotic,
for reasons that aren't clear.

Note: currently these experiments don't show the entire accuracy regime. The available data only goes to 20 rewinding/finetuning iterations,
in which time accuracy has not dropped to 0.

Over this data axis, rewinding doesn't seem to have any consistent advantage over fine-tuning.

We can visualize the changes made to the input images; see figure \ref{fig:sample}.}

We can also examine the influence of changing attack parameters on a particular pruned network.
Here, I fix the pruning iteration at 10, and vary the parameters of PGD.

This smooth continuum of epsilon is, of course, not realistic for real-world attacks, since most image formats quantize
all channels to 8 bits. However, it's useful to understand the dynamics of the network.

We can see that increasing epsilon continuously degrades performance, while increasing PGD strength saturates fairly quickly. Interestingly,
lottery-ticket-style rewinding does seem to have a small advantage in a particular epsilon regime, even when the data are normalized by starting accuracy.
However, this advantage vanishes as the adversary increases in strength.

So we see that lottery-ticket-style training with rewinding does not provide any meaningful defense on its own.
However, it's an open question whether it can be usefully combined with other adversarial defenses.

\section{Work for the next month}
\label{sec:org7c08006}

"Winning tickets" are networks that can be trained in a pruned state, often yielding higher
ending accuracy. They're found by training the network fully, pruning the final network,
and then rewinding to a step early in training.

Now that I've got my infrastructure set up, it should be relatively straightforward to
attempt combining this strategy with adversarial training. I'd like to investigate the
potential existence of adversarial lottery tickets,
as well as effects of swapping training regimes before / after pruning.

If we use oneshot pruning, we can draw a dividing line at the rewinding operation, and use one type of training
beforehand, and a possibly different style of training. We can contextualize this set of possible experiments as a box, with the post-training regime on the x
axis and the pre-training regime on the y axis:

\begin{center}
\begin{tabular}{lll}
 & standard post-training & adversarial post-training\\
\hline
standard pre-training & \textbf{standard LTH} & \\
adversarial pre-training &  & \\
\end{tabular}
\end{center}

The questions I'd like to answer fit into this box. Standard LTH training is the top-left quadrant: standard
training for both regimes.

\subsection{Question 1: are there "winning tickets" in the context of adversarial training?}
\label{sec:org839be2e}

That is, can we find networks that recover \textbf{adversarial} accuracy when trained
adversarially, pruned,
and then rewound and re-trained adversarially?

\begin{center}
\begin{tabular}{lll}
 & standard post-training & adversarial post-training\\
\hline
standard pre-training & standard LTH & \\
adversarial pre-training &  & \textbf{Question 1}\\
\end{tabular}
\end{center}

\citep{RobustnessOrCompression} attempted to answer this question, but failed because they didn't use rewinding;
\citep{anonymous2020boosting} also attempted to answer the question without rewinding, instead using hyperparameter search
to find functional networks.

I'll need to sweep over the rewinding iteration, because the necessary iteration has been found to vary between tasks and architectures.

Independent variables:
\begin{itemize}
\item Oneshot pruning rate
\item Rewinding iteration
\end{itemize}

Dependent variables:
\begin{itemize}
\item Adversarial accuracy
\end{itemize}

Possible outcomes:
\begin{itemize}
\item \textbf{Winning tickets recover pre-pruning accuracy.}
This outcome has practical implications for adversarial training.

\item \textbf{Winning tickets do not recover pre-pruning accuracy}.
This outcome would be an interesting counterexample to the lottery ticket phenomenon.
\end{itemize}

I have some preliminary data suggesting that tickets do exist, but I'd like to evaluate that more systematically.

\subsection{Question 2: can I re-train standard lottery tickets in the adversarial setting?}
\label{sec:org59c5664}
Adversarial training is expensive; next we can ask, is it possible to use adversarially training with a standard winning ticket,
in order to gain the benefits of lottery ticket training?

\begin{center}
\begin{tabular}{lll}
 & standard post-training & adversarial post-training\\
\hline
standard pre-training & standard LTH & \textbf{Question 2}\\
adversarial pre-training &  & Question 1\\
\end{tabular}
\end{center}

\citep{anonymous2020boosting} attempted to answer this question, but, again, didn't use rewinding. They also varied the strength of the
adversary in initial training (the number of PGD iterations), since a weak adversary is still much cheaper to compute than a strong adversary.

Independent variable:
\begin{itemize}
\item Whether training before pruning is adversarial or not.
\item Pruning rate
\item Pre-training adversary strength
\end{itemize}

Dependent variable:
\begin{itemize}
\item Adversarial accuracy
\end{itemize}

Possible outcomes:
\begin{itemize}
\item \textbf{Standard winning tickets reach the same accuracy} as adversarial winning tickets when
retrained adversarially:
This outcome has useful practical implications. It also would show that tickets are in some sense "general".

\item \textbf{Weak-adversary winning tickets reach the same accuracy} as strong-adversary winning tickets when
retrained adversarially:
This result would replicate \citep{anonymous2020boosting} without (as much) expensive hyperparameter search.

\item \textbf{Standard winning tickets reach a lower accuracy} as adversarial winning tickets when
retrained adversarially:
This outcome would show that tickets are not transferable between tasks.
\end{itemize}

\subsection{Question 3: can a standard training maintain the robustness of an adversarially-trained winning ticket?}
\label{sec:orgd4a0873}
We can also ask the inverse question: will an adversarially-robust winning ticket maintain robustness \footnote{"Robustness" is Madry's term for adversarial resilience, i.e., adversarial accuracy, roughly speaking.]} when retrained with a weaker
(or, no) adversary?

\begin{center}
\begin{tabular}{lll}
 & standard post-training & adversarial post-training\\
\hline
standard pre-training & standard LTH & Question 2\\
adversarial pre-training & \textbf{Question 3} & Question 1\\
\end{tabular}
\end{center}

Independent variable:
\begin{itemize}
\item Whether training before pruning is adversarial or not.
\item Pruning rate
\item Post-training adversary strength
\end{itemize}

Dependent variable:
\begin{itemize}
\item Adversarial accuracy
\end{itemize}

Possible outcomes:
\begin{itemize}
\item \textbf{Adversarial tickets do not need to be retrained adversarially} to maintain robustness:
This would be a \textbf{very} interesting result. It has implications connected to the other work
related to stability and basins of attraction going on in the lab.

This would provide strong evidence for the lottery ticket hypothesis, suggesting that the basin of attraction found by early training
determines most network properties.

\item \textbf{Strong adversarial tickets need only a weak post-training adversary} to maintain robustness:
Similar to the above, this suggests that a strong adversary is only needed to find the initial basin of attraction, and a weak
adversary is sufficient to stay in it.

\item \textbf{Adversarial tickets do need to be retrained adversarially} to maintain robustness:
The null hypothesis. I strongly suspect this will be the case.
\end{itemize}

\subsection{Question 4: how small can I make an adversarially trained network?}
\label{sec:orgc883343}
\citep{TowardsResistantAdversarial} found that simply adding capacity to adversarially-trained networks can
improve accuracy. They suggest conceiving of adversarial training as a min-max problem:

$$\min_\theta \rho(\theta), \mathrm{where}\; \rho(\theta) = E_{(x,y)\sim D}[\max_{\delta \in S} L(\theta, x+\delta, y) ]$$

where:

\(D\): training set

\(L\): network loss

\(\theta\): parameters

\(S \subseteq \mathbb{R}^d\), some space of allowed perturbations

That is, adversarial training is the problem of empirically minimizing training loss, when an adversary is permitted to perturb all
inputs in some subspace to maximize training loss. According to this view, adversarial training requires learning more complex
decision boundaries; it's a harder problem.

However, some more recent work (\citep{NotBugsFeatures}, \citep{HighFreqGeneralization}) has found that adversarial training can actually be thought of as training the network not to use "human-uninterpretable input features". This line of thinking suggests that many datasets have \textbf{predictive} input features that make no sense to humans. According to this view, it might be possible to prune adversarial networks even further than standard networks. Perhaps, then, the additional capacity is only needed during parameter search; or maybe the extra activations are needed,
but the same or fewer numbers of parameters are required in adversarial training.

By training Wide ResNets \citep{WideResNets} I can easily vary network width as a parameter.

This brings us back to the bottom-right corner:

\begin{center}
\begin{tabular}{lll}
 & standard post-training & adversarial post-training\\
\hline
standard pre-training & standard LTH & Question 2\\
adversarial pre-training & Question 3 & Question 1, \textbf{Question 4}\\
\end{tabular}
\end{center}

Independent variables:
\begin{itemize}
\item Network width
\item Pruning rate
\end{itemize}

Dependent variables:
\begin{itemize}
\item Adversarial accuracy
\end{itemize}

Possible outcomes:
\begin{itemize}
\item \textbf{Wider networks can be pruned to the same parameter count} as un-widened networks:
Perhaps provides indirect evidence for \citep{NotBugsFeatures}.
Has practical implications.

\item \textbf{Wider networks can be pruned of the same percentage of weights} as un-widened networks:
The null hypothesis.
\end{itemize}


\subsection{Engineering plan / timeline}
\label{sec:org1760af4}
I'll be attempting to answer the above question in the order written; Question 1 will require me to build out some more infrastructure,
which I can then use to straightforwardly answer the other questions.

First, I'll need to build out infrastructure to parallelize training grids, since my current code only trains on a single node.
Implementing this from scratch would be a herculean task, so I intend to leverage the Dask (\url{https://dask.org/}) python library.

Dask is a very widely-used distributed data science library. It allows users to define a grid -- or, more generally, a DAG -- of
tasks. A "task" can be any idempotent python function which returns a python object; for example, an epoch of network training.
Outputs from one task can be fed to later tasks.

Once you've defined your DAG, you boot up a set of machines, have them all connect to a supervisor machine, and submit your DAG. Dask will then
evaluate it in parallel across all the machines, transparently handling node failure and reporting of intermediate results. This works
with hardware accelerators on the machines; you just use whatever python libraries you normally would.

I'm hoping dask will make it quite straightforward to parallelize my current grid evaluation code. My rough timeline is:

\begin{itemize}
\item 1 week: setup training in parallel with Dask on TPU
\item 1 week: make sure training is replicating previous results + answer question 1
\item 1 week: answer questions 2 + 3
\item 1 week: answer question 4
\end{itemize}

We'll see how well this goes.


\section{Future work}
\label{sec:orgc971220}
Here's some brief descriptions of potential future work after this round of experiments.

\subsection{Can pruning be combined with other network augmentations?}
\label{sec:org696201c}
Recent work has suggested that removing BatchNorm layers \citep{BatchNormBad} and regularizing network filters to have lipschitz constants less than 1 \citep{ParsevalNetworks} can significantly improve adversarial robustness when combined with adversarial training. It would be interesting
to see whether these augmentations can be combined with pruning. It might be necessary to add other regularizations such as Dropout when removing
BatchNorm on very wide layers.

\subsection{Pruning rates under nonsense datasets}
\label{sec:orgb75c7a5}
Swapping out either the inputs or labels of a training dataset for random values can show a variety of results. \citep{NotBugsFeatures} trained a network on reassigned adversarial labels to show that adversarial features are predictive; other works
have studied the capacity of networks for memorization under random features and labels.

Studying possible pruning rates in this setting would let me study how relatively "difficult" different tasks are, in terms of necessary parameter counts.

An alternative metric to investigate would be Network Dissection IoU \citep{netdissect2017}, or the modified netdissect IoU termed "network consistency" developed in \citep{AdvTowardsInterpretability}.

There may also be connections between Network Dissections and the stability work going on in other parts of the lab.

\section{Conclusion}
\label{sec:orgfb25c79}
That's my results for this month and my plans for next month. Let me know if you have any feedback.

\bibliography{../notes/literature/everything.bib}
\bibliographystyle{iclr2020_conference}
\end{document}
