#+TITLE: Abstract: A High-Performance Implementation of ABCNets in Flux.jl

#+AUTHOR: James Gilles
#+EMAIL: jhgilles@mit.edu
#+DATE: 01 November 2019
#+OPTIONS: tex:t latex:t
#+STARTUP: latexpreview

#+LATEX_CLASS: article

#+LATEX_HEADER: \ifdefined\orglatexfragmentpreview
#+LATEX_HEADER: \else
#+LATEX_HEADER: \usepackage{iclr2020_conference,times}
#+LATEX_HEADER: \fi

#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{url}

#+LATEX_HEADER: % Optional math commands from https://github.com/goodfeli/dlbook_notation.
#+LATEX_HEADER: \input{math_commands.tex}

#+LATEX_HEADER: \newcommand{\xv}[0]{\mathbf{x}}
#+LATEX_HEADER: \newcommand{\yv}[0]{\mathbf{y}}
#+LATEX_HEADER: \newcommand{\zv}[0]{\mathbf{z}}
#+LATEX_HEADER: \newcommand{\fv}[0]{\mathbf{f}}
#+LATEX_HEADER: \newcommand{\J}[0]{\mathbf{J}}
#+LATEX_HEADER: \newcommand{\gv}[0]{\mathbf{g}}
#+LATEX_HEADER: \newcommand{\hv}[0]{\mathbf{h}}
#+LATEX_HEADER: \newcommand{\hxo}[0]{\mathbf{h}_0}
#+LATEX_HEADER: \newcommand{\dd}[1]{\mathrm{d}#1}
#+LATEX_HEADER: \newcommand{\piv}[0]{\boldsymbol{\pi}}
#+LATEX_HEADER: \newcommand{\av}[0]{\mathbf{a}}
#+LATEX_HEADER: \newcommand*{\Oc}[0]{\mathcal{O}}
#+LATEX_HEADER: \newcommand*{\obsint}[1]{\langle #1 \rangle}
#+LATEX_HEADER: \newcommand*{\Wv}[0]{\mathbf{W}}
#+LATEX_HEADER: \newcommand*{\Av}[0]{\mathbf{A}}
#+LATEX_HEADER: \newcommand*{\Wa}[0]{\widetilde{\mathbf{W}}}
#+LATEX_HEADER: \newcommand*{\Aa}[0]{\widetilde{\mathbf{A}}}

#+LATEX_HEADER: \newcommand*{\approxident}{%
#+LATEX_HEADER: \mathrel{\vcenter{\offinterlineskip
#+LATEX_HEADER: \hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}}}}

#+LATEX_HEADER: \usepackage{mathtools}

#+LATEX_HEADER:
#+LATEX_HEADER: \DeclarePairedDelimiter\abs{\lvert}{\rvert}%
#+LATEX_HEADER: \DeclarePairedDelimiter\norm{\lVert}{\rVert}%
#+LATEX_HEADER:
#+LATEX_HEADER: % Swap the definition of \abs* and \norm*, so that \abs
#+LATEX_HEADER: % and \norm resizes the size of the brackets, and the
#+LATEX_HEADER: % starred version does not.
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \let\oldabs\abs
#+LATEX_HEADER: \def\abs{\@ifstar{\oldabs}{\oldabs*}}
#+LATEX_HEADER: %
#+LATEX_HEADER: \let\oldnorm\norm
#+LATEX_HEADER: \def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
#+LATEX_HEADER: \makeatother

#+LATEX_HEADER: \renewcommand*{\maketitle}[0]{}
#+LATEX_HEADER: \renewcommand*{\tableofcontents}[0]{}

#+LATEX_HEADER: \iclrfinaltrue

#+LATEX_HEADER: \setlength{\abovedisplayskip}{0pt}
#+LATEX_HEADER: \setlength{\belowdisplayskip}{0pt}

\textsc{Abstract: A High-Performance Implementation of ABC-Nets in Flux.jl}

James Gilles

\today

In the past decade, deep neural networks have achieved rapid uptake in many fields
and applications. However, deep neural networks require large amounts
of memory and compute to execute. This prevents them from being deployed in resource
-constrained environments like mobile devices [[citep:MobileNets]]. In addition,
network training is expensive; Full
development of a new deep network architecture has been estimated to have an equivalent
carbon footprint to the lifetime footprint of five consumer cars [[citep:EmitCarbon]].

To address this issue, researchers are looking into techniques for model compression,
such as *model quantization*, which reduces the bit-widths used in neural network
computations. One proposed quantization
technique is XNORNets, and their extension ABCNets [[citep:XNORNets]][[citep:ABCNets]].
XNORNETs quantize network weights and activations to a single
bit, pack these bits into 32- or 64-bit integers, and compute the dot products of
these vectors using the XNOR operation. This allows extremely efficient network
evaluation, taking the dot product of 64 weights and activations in 2
machine instructions. However, single-bit quantization causes major accuracy losses.
ABCNets address this problem by approximating weights and
activations using *linear combinations* of *binary arrays*.

Looking at matrix multiplication -- which can be generalized to batched convolution --
we approximate weights and activations as follows:
$$\Wv \approxident \Wa = \alpha_1 \Wa_1 + \alpha_2 \Wa_2 + ... + \alpha_M \Wa_M = \sum_i^M \alpha_i \Wa_i$$
$$\Av \approxident \Aa = \beta_1 \Aa_1 + \beta_2 \Aa_2 + ... + \beta_N \Aa_N = \sum_j^N \beta_j \Aa_j $$
Where $\Wa_i$ and $\Aa_j$ are binarized versions of weights and activations, learned by a
custom linear-regression based routine during training (not described here for space
reasons).
Then, we have:
$$\Wv\Av \approxident \Wa\Aa = (\sum_i^M \alpha_i \Wa_i) (\sum_j^N \beta_j \Aa_j) \\
                            = \sum_i^M \sum_j^N \alpha_i \beta_j \Wa_i \Aa_j$$
That is, the product of the approximated weights and activations can be
computed in $M*N$ XNOR-matrix-multiplications, where $M$ and $N$ are the bit-widths of
weights and activations respectively.

ABCNets have been found to retain most of their accuracy compared to non-quantized networks,
with only six percent accuracy loss
in ResNet20 on CIFAR10 when quantizing to $M=5,N=3$. However, their potential performance
gains have not been measured in practice; researchers implemented the proposed quantization
routines using floating point numbers, instead of an actual accelerated implementation.

For my 18.337 final project, I'll be implementing a GPU-accelerated version of ABC-Nets
using Flux.jl and CUDAnative.jl [[citep:FluxJL]][[citep:CUDAnativeJL]]. I'll measure the
range of acceptable $M,N$ values for which ABCNet-convolution is faster than
32-bit and 16-bit floating point convolution baselines.

I'll train WideResNets [[citep:WideResNets]], a widely-used benchmark network,
using my new quantization primitives, and measure total performance gains / accuracy losses
on the CIFAR10 dataset.

I'll also implement the adjoint operation, to allow training to be accelerated;
and investigate quantizing gradients using the same strategy during training.

I'll also investigate an extension proposed in the original paper, which used different
quantization constants per-output-channel to allow more flexibility in training.

This work will empirically verify whether or not ABCNets are a useful optimization in
practice. It will also result in an open-source ABCNet-convolution implementation that
I hope to get merged into Flux.jl.






#+LATEX: \bibliography{./everything.bib}
#+LATEX: \bibliographystyle{iclr2020_conference}
