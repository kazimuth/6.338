{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote\n",
    "using CuArrays\n",
    "using CUDAnative\n",
    "using CUDAdrv\n",
    "using NNlib\n",
    "using Test\n",
    "using BenchmarkTools\n",
    "using Flux\n",
    "using Plots\n",
    "using Images\n",
    "using Colors\n",
    "using ProgressMeter\n",
    "using StaticArrays\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zygote.@adjoint gpu(a :: Array) = gpu(a), a̅ -> (cpu(a̅),)\n",
    "Zygote.@adjoint function Base.convert(::Type{T}, xs::Array{K,N}) where {T<:CuArray, K, N}\n",
    "  Base.convert(T, xs), Δ -> (nothing, Base.convert(Array, Δ),)\n",
    "end\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CuArrays.allowscalar(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight_masks (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function weight_masks(W, us)\n",
    "    dims = size(W)\n",
    "\n",
    "    W̅ = W .- mean(W)\n",
    "\n",
    "    std_W = std(W)\n",
    "\n",
    "    W̃s = cat((sign.(W̅ .+ (u * std_W)) for u in us)...,\n",
    "            dims=length(size(W)) + 1)\n",
    "\n",
    "    W̃s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarize_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function binarize_weights(W, us)\n",
    "    W̃s = weight_masks(W, us)\n",
    "\n",
    "    dims = size(W)\n",
    "\n",
    "    Wv = reshape(W, :)\n",
    "    W̃vs = reshape(W̃s, :, length(us))\n",
    "\n",
    "    αs = W̃vs \\ Wv\n",
    "\n",
    "    W̃v = W̃vs * αs\n",
    "    \n",
    "    W̃ = reshape(W̃v, dims...)\n",
    "\n",
    "    W̃, αs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zygote.@adjoint function binarize_weights(W, us) \n",
    "    W̃ = binarize_weights(W, us)\n",
    "    function adjoint((∇_W̃, ∇_αs)) \n",
    "        # the \"straight-through estimator\"\n",
    "        (∇_W̃, nothing)\n",
    "    end\n",
    "    W̃, adjoint\n",
    "end\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "even_err (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_us(M) = if M == 1\n",
    "        [0.0f0]\n",
    "    else\n",
    "        Array(range(-1.0f0, stop=1.0f0, length=M))\n",
    "end\n",
    "function even_err(M, W)\n",
    "    us = even_us(M)\n",
    "    W̃, _ = binarize_weights(W, us)\n",
    "    mean((W .- W̃).^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = randn(Float32, 3,3,10,10)\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize_weights(W, even_us(3))\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo write gradient test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip8900\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip8900)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip8901\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip8900)\" d=\"\n",
       "M242.516 1425.62 L2352.76 1425.62 L2352.76 47.2441 L242.516 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip8902\">\n",
       "    <rect x=\"242\" y=\"47\" width=\"2111\" height=\"1379\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  523.439,1425.62 523.439,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  965.838,1425.62 965.838,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1408.24,1425.62 1408.24,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1850.63,1425.62 1850.63,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2293.03,1425.62 2293.03,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,1209.81 2352.76,1209.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,975.371 2352.76,975.371 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,740.931 2352.76,740.931 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,506.49 2352.76,506.49 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,272.05 2352.76,272.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,1425.62 242.516,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  523.439,1425.62 523.439,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  965.838,1425.62 965.838,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1408.24,1425.62 1408.24,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1850.63,1425.62 1850.63,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2293.03,1425.62 2293.03,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,1209.81 274.17,1209.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,975.371 274.17,975.371 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,740.931 274.17,740.931 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,506.49 274.17,506.49 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,272.05 274.17,272.05 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 523.439, 1479.62)\" x=\"523.439\" y=\"1479.62\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 965.838, 1479.62)\" x=\"965.838\" y=\"1479.62\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1408.24, 1479.62)\" x=\"1408.24\" y=\"1479.62\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1850.63, 1479.62)\" x=\"1850.63\" y=\"1479.62\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2293.03, 1479.62)\" x=\"2293.03\" y=\"1479.62\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 1227.31)\" x=\"218.516\" y=\"1227.31\">0.10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 992.871)\" x=\"218.516\" y=\"992.871\">0.15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 758.431)\" x=\"218.516\" y=\"758.431\">0.20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 523.99)\" x=\"218.516\" y=\"523.99\">0.25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 289.55)\" x=\"218.516\" y=\"289.55\">0.30</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1297.64, 1559.48)\" x=\"1297.64\" y=\"1559.48\">M</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 736.431)\" x=\"89.2861\" y=\"736.431\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip8902)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  302.24,86.2547 523.439,479.524 744.638,1148.36 965.838,1284.77 1187.04,1335.92 1408.24,1358.85 1629.43,1371.31 1850.63,1381.42 2071.83,1384.38 2293.03,1386.61 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip8900)\" d=\"\n",
       "M1989.93 251.724 L2280.76 251.724 L2280.76 130.764 L1989.93 130.764  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,251.724 2280.76,251.724 2280.76,130.764 1989.93,130.764 1989.93,251.724 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip8900)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,191.244 2157.93,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip8900)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 208.744)\" x=\"2181.93\" y=\"208.744\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ms = 1:10\n",
    "errs = [even_err(M, W) for M in Ms]\n",
    "plot(Ms, errs, xlabel=\"M\", ylabel=\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't solve for alphas\n",
    "function binarize_weights(W, us, αs)\n",
    "    W̃s = weight_masks(W, us)\n",
    "\n",
    "    dims = size(W)\n",
    "\n",
    "    Wv = reshape(W, :)\n",
    "    W̃vs = reshape(W̃s, :, length(us))\n",
    "\n",
    "    W̃v = W̃vs * αs\n",
    "    \n",
    "    W̃ = reshape(W̃v, dims...)\n",
    "\n",
    "    W̃\n",
    "end\n",
    "\n",
    "Zygote.@adjoint function binarize_weights(W, us, αs)\n",
    "    # not sure why you'd need this, but might as well...\n",
    "    W̃ = binarize_weights(W, us, αs)\n",
    "    function adjoint(∇_W̃) \n",
    "        # the \"straight-through estimator\"\n",
    "        (∇_W̃, nothing, nothing)\n",
    "    end\n",
    "    W̃, adjoint\n",
    "end\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us = even_us(5)\n",
    "W̃, αs = binarize_weights(W, us)\n",
    "W̃1 = binarize_weights(W, us, αs)\n",
    "\n",
    "@test W̃ == W̃1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇_W, ∇_us, ∇_αs = Zygote.gradient((W, us, αs) -> sum(binarize_weights(W, us, αs)), W, us, αs)\n",
    "@test ∇_W == ones(Float32, size(W))\n",
    "@test ∇_us == nothing\n",
    "@test ∇_αs == nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarize_weights (generic function with 4 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function binarize_weights(W :: CuArray, us)\n",
    "    W̃, αs = binarize_weights(cpu(W), cpu(us))\n",
    "    gpu(W̃), cpu(αs) # note alphas are still on CPU!!\n",
    "end\n",
    "function binarize_weights(W :: CuArray, us, αs)\n",
    "    W̃ = binarize_weights(cpu(W), cpu(us), cpu(αs))\n",
    "    gpu(W̃)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  109.511 μs (201 allocations: 109.77 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@btime binarize_weights(W, even_us(5))\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  184.381 μs (228 allocations: 119.30 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wg = gpu(W)\n",
    "#@btime binarize_weights(Wg, even_us(5))\n",
    "()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "function z(q)\n",
    "    if q > 0.5f0\n",
    "        1.0f0\n",
    "    else\n",
    "        -1.0f0\n",
    "    end\n",
    "end\n",
    "\n",
    "function _zrev(q, ∇_q)\n",
    "    if 0.0f0 < q < 1.0f0\n",
    "        ∇_q\n",
    "    else\n",
    "        0.0f0\n",
    "    end\n",
    "end\n",
    "\n",
    "Zygote.@adjoint z(q) = z(q), (∇_q) -> _zrev(q, ∇_q)\n",
    "\n",
    "function zb(Q)\n",
    "    z.(Q)\n",
    "end\n",
    "\n",
    "Zygote.@adjoint zb(Q) = zb(Q), (∇_Q) -> (_zrev.(Q, ∇_Q),)\n",
    "\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "function binarize_activations(A, vs, βs)\n",
    "    shape = size(A)\n",
    "    \n",
    "    Av_x = reshape( (A), :, 1)\n",
    "    \n",
    "    vs_x = reshape( (vs), 1, :)\n",
    "    βs_x = reshape( (βs), 1, :)\n",
    "    \n",
    "    Av1 =  (Av_x) .+  (vs_x)\n",
    "    #Av2 = z.( (Av1))\n",
    "    Av2 = zb( (Av1))\n",
    "    Av3 =  (Av2) .*  (βs_x)\n",
    "    \n",
    "    Ãv = sum( (Av3), dims=2)\n",
    "    \n",
    "    result = reshape( (Ãv), shape)\n",
    "        \n",
    "    result\n",
    "end\n",
    "\n",
    "Zygote.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float32,2}:\n",
       " -0.147147   -0.335079   -0.718732  1.53579 \n",
       "  1.01813     0.719564    0.137259  0.317213\n",
       "  0.129429   -0.123068   -0.403681  2.57137 \n",
       "  0.0835204   0.0199713  -0.902396  1.68367 "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = randn(Float32, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float32,2}:\n",
       " 0.0  0.0  -2.0  2.0\n",
       " 0.0  0.0   0.0  0.0\n",
       " 0.0  0.0   0.0  2.0\n",
       " 0.0  0.0  -2.0  2.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = even_us(2)\n",
    "bs = βs = ones(Float32, 2)\n",
    "\n",
    "binarize_activations(A, vs, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[1.0 1.0 1.0 1.0; 1.0 0.0 0.0 0.0; 0.0 1.0 1.0 0.0; 0.0 0.0 1.0 1.0], Float32[3.0, 6.0], Float32[-10.0, 12.0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient((A, vs, βs) -> sum(binarize_activations(A, vs, βs)), A, vs, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 CuArray{Float32,2,CuArray{Float32,2,Nothing}}:\n",
       " 0.0  0.0  -2.0  2.0\n",
       " 0.0  0.0   0.0  0.0\n",
       " 0.0  0.0   0.0  2.0\n",
       " 0.0  0.0  -2.0  2.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize_activations(gpu(A), gpu(even_us(2)), gpu(ones(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0f0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((A, vs, βs) -> sum(binarize_activations(A, vs, βs)))(gpu(A), gpu(even_us(2)), gpu(ones(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[1.0 1.0 1.0 1.0; 1.0 0.0 0.0 0.0; 0.0 1.0 1.0 0.0; 0.0 0.0 1.0 1.0], Float32[3.0, 6.0], Float32[-10.0, 12.0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient((A, vs, βs) -> sum(binarize_activations(A, vs, βs)), gpu(A), gpu(even_us(2)), gpu(ones(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal_error_plot (generic function with 2 methods)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normal_error_plot(N, steps=1000)\n",
    "    vs = gpu(even_us(2))\n",
    "    bs = βs = gpu(ones(Float32, 2))\n",
    "\n",
    "    opt = ADAM()\n",
    "    ps = Params([vs, βs])\n",
    "    errors = Float32[]\n",
    "    @showprogress for _ in 1:steps\n",
    "        # note: in loop!\n",
    "        A = gpu(randn(100, 100))\n",
    "        err, adjoint = pullback(ps) do\n",
    "            Ã = binarize_activations(A, vs, βs)\n",
    "            mean((A .- Ã).^2)\n",
    "        end\n",
    "        gs = adjoint(1.0)\n",
    "\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "        push!(errors, err)\n",
    "    end\n",
    "    plot(errors, xlabel=\"gradient descent step\", ylabel=\"MSE\", title=\"error with N=$N bits\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:34\u001b[39m\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip9300\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip9300)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9301\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip9300)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 121.675 L215.754 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9302\">\n",
       "    <rect x=\"215\" y=\"121\" width=\"2138\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1345.5 2352.76,1345.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1030.55 2352.76,1030.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,715.601 2352.76,715.601 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,400.652 2352.76,400.652 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1345.5 247.809,1345.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1030.55 247.809,1030.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,715.601 247.809,715.601 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,400.652 247.809,400.652 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 274.217, 1479.62)\" x=\"274.217\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 778.732, 1479.62)\" x=\"778.732\" y=\"1479.62\">250</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1283.25, 1479.62)\" x=\"1283.25\" y=\"1479.62\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1787.76, 1479.62)\" x=\"1787.76\" y=\"1479.62\">750</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2292.27, 1479.62)\" x=\"2292.27\" y=\"1479.62\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1363)\" x=\"191.754\" y=\"1363\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1048.05)\" x=\"191.754\" y=\"1048.05\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 733.101)\" x=\"191.754\" y=\"733.101\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 418.152)\" x=\"191.754\" y=\"418.152\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 73.2)\" x=\"1284.25\" y=\"73.2\">error with N=2 bits</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">gradient descent step</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9302)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,160.29 278.253,196.548 280.271,158.579 282.289,177.849 284.307,180.83 286.326,207.641 288.344,208.23 290.362,208.683 292.38,205.086 294.398,250.987 \n",
       "  296.416,225.748 298.434,233.005 300.452,226.357 302.47,248.022 304.488,275.073 306.506,229.662 308.524,257.692 310.542,278.408 312.56,271.709 314.578,268.968 \n",
       "  316.596,256.989 318.614,286.358 320.633,317.685 322.651,294.744 324.669,322.122 326.687,327.405 328.705,324.784 330.723,345.407 332.741,329.69 334.759,332.231 \n",
       "  336.777,368.767 338.795,355.305 340.813,355.303 342.831,328.478 344.849,375.517 346.867,332.743 348.885,384.691 350.903,408.182 352.921,376.666 354.94,416.901 \n",
       "  356.958,394.038 358.976,423.374 360.994,384.782 363.012,413.69 365.03,435.803 367.048,424.374 369.066,440.141 371.084,399.042 373.102,458.484 375.12,428.008 \n",
       "  377.138,442.335 379.156,412.886 381.174,446.633 383.192,440.208 385.21,462.095 387.228,460.629 389.246,486.497 391.265,475.886 393.283,486.749 395.301,487.973 \n",
       "  397.319,505.827 399.337,494.003 401.355,494.951 403.373,525.314 405.391,512.868 407.409,534.335 409.427,536.764 411.445,496.105 413.463,519.262 415.481,541.765 \n",
       "  417.499,542.64 419.517,533.027 421.535,538.232 423.553,550.624 425.572,549.09 427.59,561.801 429.608,538.757 431.626,559.546 433.644,579.121 435.662,609.29 \n",
       "  437.68,573.955 439.698,575.788 441.716,597.651 443.734,591.534 445.752,576.256 447.77,585.424 449.788,579.221 451.806,607.405 453.824,607.415 455.842,600.319 \n",
       "  457.86,599.324 459.879,621.676 461.897,632.015 463.915,602.758 465.933,613.351 467.951,667.115 469.969,633.247 471.987,643.245 474.005,634.941 476.023,671.769 \n",
       "  478.041,647.582 480.059,648.048 482.077,654.782 484.095,657.674 486.113,652.943 488.131,637.374 490.149,676.967 492.167,687.378 494.185,699.785 496.204,689.342 \n",
       "  498.222,706.91 500.24,702.535 502.258,708.308 504.276,696.596 506.294,719.049 508.312,693.348 510.33,709.127 512.348,731.951 514.366,699.027 516.384,719.079 \n",
       "  518.402,740.019 520.42,730.612 522.438,724.508 524.456,707.487 526.474,731.813 528.492,736.898 530.511,749.187 532.529,737.374 534.547,726.505 536.565,750.8 \n",
       "  538.583,748.14 540.601,743.894 542.619,746.658 544.637,751.152 546.655,778.912 548.673,769.255 550.691,774.08 552.709,761.776 554.727,767.54 556.745,760.345 \n",
       "  558.763,787.181 560.781,773.783 562.799,783.802 564.817,794.716 566.836,803.421 568.854,796.701 570.872,798.43 572.89,804.58 574.908,802.939 576.926,790.539 \n",
       "  578.944,807.853 580.962,797.82 582.98,823.936 584.998,803.787 587.016,834.313 589.034,800.141 591.052,852.514 593.07,820.922 595.088,813.217 597.106,834.003 \n",
       "  599.124,829.17 601.143,825.939 603.161,825.951 605.179,847.337 607.197,841.077 609.215,831.079 611.233,837.659 613.251,858.792 615.269,839.83 617.287,855.819 \n",
       "  619.305,850.983 621.323,851.531 623.341,863.064 625.359,872.258 627.377,883.626 629.395,852.415 631.413,862.529 633.431,879.398 635.45,882.872 637.468,840.196 \n",
       "  639.486,858.742 641.504,894.557 643.522,890.831 645.54,877.32 647.558,873.452 649.576,888.492 651.594,931.21 653.612,886.592 655.63,880.286 657.648,903.177 \n",
       "  659.666,916.591 661.684,863.317 663.702,906.026 665.72,923.772 667.738,919.355 669.756,923.099 671.775,922.953 673.793,888.205 675.811,921.039 677.829,916.705 \n",
       "  679.847,895.673 681.865,920.645 683.883,935.521 685.901,943.752 687.919,906.053 689.937,923.312 691.955,938.742 693.973,935.275 695.991,915.483 698.009,934.713 \n",
       "  700.027,939.515 702.045,933.612 704.063,923.797 706.082,955.006 708.1,949.41 710.118,937.793 712.136,941.09 714.154,949.869 716.172,948.104 718.19,964.939 \n",
       "  720.208,945.826 722.226,944.441 724.244,967.008 726.262,955.911 728.28,956.327 730.298,949.051 732.316,966.844 734.334,966.384 736.352,969.656 738.37,981.38 \n",
       "  740.388,971.952 742.407,982.193 744.425,990.325 746.443,987.616 748.461,966.433 750.479,985.082 752.497,973.97 754.515,976.81 756.533,972.442 758.551,976.649 \n",
       "  760.569,987.564 762.587,981.688 764.605,990.825 766.623,1005.35 768.641,999.703 770.659,993.094 772.677,983.031 774.695,993.44 776.714,999.401 778.732,1004.55 \n",
       "  780.75,992.555 782.768,1003.21 784.786,1018.37 786.804,1027.21 788.822,1007.72 790.84,1019.41 792.858,1016.86 794.876,1022.61 796.894,999.667 798.912,1006.62 \n",
       "  800.93,1019.85 802.948,1021.26 804.966,1029.18 806.984,1029.95 809.002,1034.59 811.021,1006.65 813.039,1029.85 815.057,1023.38 817.075,1030.05 819.093,1020.58 \n",
       "  821.111,1058.38 823.129,1038.36 825.147,1025.44 827.165,1061.18 829.183,1025.14 831.201,1020.48 833.219,1033.57 835.237,1043.91 837.255,1037.55 839.273,1036.14 \n",
       "  841.291,1037.77 843.309,1006.37 845.327,1038.71 847.346,1049.98 849.364,1063.72 851.382,1059.52 853.4,1033.7 855.418,1055.5 857.436,1066.25 859.454,1064.68 \n",
       "  861.472,1042.65 863.49,1072.52 865.508,1071.08 867.526,1066.85 869.544,1055.64 871.562,1065.36 873.58,1067.76 875.598,1065.37 877.616,1077.66 879.634,1086.33 \n",
       "  881.653,1068.76 883.671,1046.58 885.689,1049.67 887.707,1083.25 889.725,1067.7 891.743,1073.4 893.761,1075.54 895.779,1070.04 897.797,1077.06 899.815,1092.94 \n",
       "  901.833,1075.64 903.851,1072.48 905.869,1083.88 907.887,1079.82 909.905,1087.73 911.923,1093.82 913.941,1087.95 915.959,1090.98 917.978,1103.24 919.996,1093.15 \n",
       "  922.014,1087.74 924.032,1076.87 926.05,1092.9 928.068,1098.17 930.086,1082.29 932.104,1093.98 934.122,1099.93 936.14,1088.23 938.158,1089.57 940.176,1085.35 \n",
       "  942.194,1092.2 944.212,1108.64 946.23,1095.52 948.248,1093.21 950.266,1128.53 952.285,1092.34 954.303,1109.6 956.321,1111.76 958.339,1109.61 960.357,1121.44 \n",
       "  962.375,1108.43 964.393,1118.95 966.411,1103.01 968.429,1106.37 970.447,1115.22 972.465,1117.95 974.483,1125.03 976.501,1126.05 978.519,1104.03 980.537,1103.59 \n",
       "  982.555,1130.76 984.573,1115.22 986.592,1129.71 988.61,1125.24 990.628,1119.01 992.646,1112.08 994.664,1122.69 996.682,1113.14 998.7,1102.54 1000.72,1145.77 \n",
       "  1002.74,1126.79 1004.75,1116.49 1006.77,1130.86 1008.79,1147.37 1010.81,1140.71 1012.83,1136.7 1014.84,1134.35 1016.86,1133.13 1018.88,1121.67 1020.9,1131.86 \n",
       "  1022.92,1146.17 1024.93,1131.94 1026.95,1134.97 1028.97,1140.58 1030.99,1125.86 1033.01,1117.54 1035.02,1153.85 1037.04,1137.52 1039.06,1137.76 1041.08,1134.58 \n",
       "  1043.1,1135.37 1045.12,1128.58 1047.13,1166.35 1049.15,1144.86 1051.17,1155.56 1053.19,1135.99 1055.21,1154.26 1057.22,1158.2 1059.24,1166.9 1061.26,1154.34 \n",
       "  1063.28,1157.66 1065.3,1152.22 1067.31,1155.59 1069.33,1163.67 1071.35,1153.12 1073.37,1151.84 1075.39,1148.74 1077.4,1158.2 1079.42,1162.58 1081.44,1167.14 \n",
       "  1083.46,1160.86 1085.48,1160.25 1087.49,1163.59 1089.51,1154.06 1091.53,1151.87 1093.55,1175.95 1095.57,1185.62 1097.58,1170.86 1099.6,1175.83 1101.62,1163.59 \n",
       "  1103.64,1152.23 1105.66,1160.36 1107.67,1189.29 1109.69,1180.1 1111.71,1176.43 1113.73,1173.93 1115.75,1173.86 1117.77,1172.56 1119.78,1167.16 1121.8,1158.11 \n",
       "  1123.82,1182.28 1125.84,1171.08 1127.86,1199.55 1129.87,1179.29 1131.89,1193.11 1133.91,1191.51 1135.93,1190.26 1137.95,1184.28 1139.96,1175.02 1141.98,1181.5 \n",
       "  1144,1183.8 1146.02,1177 1148.04,1188.72 1150.05,1179.19 1152.07,1182.29 1154.09,1189.7 1156.11,1193.7 1158.13,1188.2 1160.14,1179.68 1162.16,1183.02 \n",
       "  1164.18,1181.79 1166.2,1181.62 1168.22,1191.22 1170.23,1200.57 1172.25,1195.22 1174.27,1195.51 1176.29,1205.18 1178.31,1210.04 1180.33,1202.91 1182.34,1189.45 \n",
       "  1184.36,1194.15 1186.38,1192.2 1188.4,1191.16 1190.42,1191.25 1192.43,1193.16 1194.45,1195.61 1196.47,1206.97 1198.49,1210.63 1200.51,1198.87 1202.52,1226.61 \n",
       "  1204.54,1200.38 1206.56,1193.55 1208.58,1220.36 1210.6,1223.8 1212.61,1198.24 1214.63,1207.98 1216.65,1197.7 1218.67,1214.65 1220.69,1211.69 1222.7,1195.38 \n",
       "  1224.72,1202.93 1226.74,1205.48 1228.76,1215.8 1230.78,1215.87 1232.79,1224.68 1234.81,1203.95 1236.83,1231.72 1238.85,1217.12 1240.87,1218.44 1242.88,1211.33 \n",
       "  1244.9,1216.9 1246.92,1205.62 1248.94,1218.5 1250.96,1220.62 1252.98,1197.9 1254.99,1200.45 1257.01,1215.34 1259.03,1215.36 1261.05,1234.7 1263.07,1234.02 \n",
       "  1265.08,1221.75 1267.1,1235.04 1269.12,1211.58 1271.14,1213.79 1273.16,1204.13 1275.17,1228.92 1277.19,1231.96 1279.21,1235.05 1281.23,1205.7 1283.25,1230.51 \n",
       "  1285.26,1200.77 1287.28,1245.68 1289.3,1227.88 1291.32,1223.64 1293.34,1224.64 1295.35,1229.14 1297.37,1225.34 1299.39,1234.23 1301.41,1229.31 1303.43,1227.85 \n",
       "  1305.44,1237.69 1307.46,1220.52 1309.48,1255.99 1311.5,1234.85 1313.52,1230.89 1315.53,1231.1 1317.55,1249.17 1319.57,1224.15 1321.59,1244.7 1323.61,1239.11 \n",
       "  1325.63,1238.92 1327.64,1238.75 1329.66,1240.77 1331.68,1238.09 1333.7,1230.93 1335.72,1231.1 1337.73,1230.82 1339.75,1234.78 1341.77,1249.13 1343.79,1238.69 \n",
       "  1345.81,1214.97 1347.82,1246.69 1349.84,1243.54 1351.86,1244.13 1353.88,1240.34 1355.9,1232.51 1357.91,1248.96 1359.93,1248.88 1361.95,1269.6 1363.97,1271.49 \n",
       "  1365.99,1256.68 1368,1243.86 1370.02,1244.05 1372.04,1233.96 1374.06,1236.39 1376.08,1270.77 1378.09,1260.75 1380.11,1254.56 1382.13,1241.73 1384.15,1257.76 \n",
       "  1386.17,1242.08 1388.18,1244.14 1390.2,1247.1 1392.22,1247.29 1394.24,1267.4 1396.26,1235.41 1398.28,1261.31 1400.29,1251.23 1402.31,1250.29 1404.33,1247.2 \n",
       "  1406.35,1271.78 1408.37,1262.92 1410.38,1251.57 1412.4,1256.45 1414.42,1263.68 1416.44,1253.4 1418.46,1253.02 1420.47,1247.84 1422.49,1257.18 1424.51,1265.03 \n",
       "  1426.53,1253.77 1428.55,1259.87 1430.56,1272.53 1432.58,1271.32 1434.6,1266.47 1436.62,1264.29 1438.64,1254.07 1440.65,1258.65 1442.67,1286.11 1444.69,1270.97 \n",
       "  1446.71,1264.25 1448.73,1266.19 1450.74,1255.55 1452.76,1257.88 1454.78,1295.06 1456.8,1270.9 1458.82,1262.74 1460.84,1276.44 1462.85,1271.74 1464.87,1248.09 \n",
       "  1466.89,1265.73 1468.91,1268.32 1470.93,1265.94 1472.94,1262.33 1474.96,1251.55 1476.98,1279.72 1479,1271.23 1481.02,1283.97 1483.03,1285.43 1485.05,1281.62 \n",
       "  1487.07,1274.47 1489.09,1270.91 1491.11,1264.23 1493.12,1251.93 1495.14,1266.19 1497.16,1258.34 1499.18,1255.6 1501.2,1283.58 1503.21,1266.66 1505.23,1291.79 \n",
       "  1507.25,1278.65 1509.27,1278.56 1511.29,1268.5 1513.3,1298.81 1515.32,1286.38 1517.34,1291.61 1519.36,1286.96 1521.38,1298.73 1523.39,1264.9 1525.41,1265.97 \n",
       "  1527.43,1293.68 1529.45,1294.12 1531.47,1284.96 1533.49,1279.23 1535.5,1286.31 1537.52,1290.69 1539.54,1273.1 1541.56,1269.56 1543.58,1288.72 1545.59,1279.22 \n",
       "  1547.61,1264.71 1549.63,1268.96 1551.65,1282.59 1553.67,1276.25 1555.68,1261.56 1557.7,1285.43 1559.72,1294.57 1561.74,1289.92 1563.76,1299.11 1565.77,1277.83 \n",
       "  1567.79,1293.45 1569.81,1299.29 1571.83,1277.61 1573.85,1285.88 1575.86,1290.77 1577.88,1296.93 1579.9,1310.9 1581.92,1304.05 1583.94,1291.21 1585.95,1302.22 \n",
       "  1587.97,1281.19 1589.99,1291.43 1592.01,1307.63 1594.03,1301.08 1596.04,1284.33 1598.06,1300.54 1600.08,1292.69 1602.1,1274.39 1604.12,1269.35 1606.14,1301.97 \n",
       "  1608.15,1274.2 1610.17,1286.61 1612.19,1283.67 1614.21,1293.18 1616.23,1297.82 1618.24,1307.9 1620.26,1310.36 1622.28,1282.77 1624.3,1300.8 1626.32,1298.36 \n",
       "  1628.33,1296.16 1630.35,1284.67 1632.37,1289.52 1634.39,1295.27 1636.41,1310.57 1638.42,1300.35 1640.44,1294.27 1642.46,1298.5 1644.48,1304.33 1646.5,1311.61 \n",
       "  1648.51,1295.59 1650.53,1288.06 1652.55,1301.95 1654.57,1306.89 1656.59,1298.13 1658.6,1291.05 1660.62,1318.64 1662.64,1307.42 1664.66,1298.05 1666.68,1306.91 \n",
       "  1668.69,1296.68 1670.71,1314.4 1672.73,1313.23 1674.75,1308.96 1676.77,1308.45 1678.79,1306.76 1680.8,1309.68 1682.82,1308.89 1684.84,1294.59 1686.86,1322.95 \n",
       "  1688.88,1306.07 1690.89,1303.33 1692.91,1307.02 1694.93,1312.33 1696.95,1321.69 1698.97,1316.71 1700.98,1319.53 1703,1302.6 1705.02,1302.32 1707.04,1311.74 \n",
       "  1709.06,1305.96 1711.07,1318.37 1713.09,1294.24 1715.11,1304.83 1717.13,1320.62 1719.15,1289.35 1721.16,1305.62 1723.18,1310.1 1725.2,1314.14 1727.22,1323.71 \n",
       "  1729.24,1287.97 1731.25,1328.69 1733.27,1324.82 1735.29,1313.62 1737.31,1322.76 1739.33,1307.36 1741.35,1302.92 1743.36,1310.6 1745.38,1306.86 1747.4,1328.06 \n",
       "  1749.42,1332.64 1751.44,1314.95 1753.45,1302.13 1755.47,1316.63 1757.49,1305.51 1759.51,1324.66 1761.53,1326.32 1763.54,1314.55 1765.56,1317.22 1767.58,1307.38 \n",
       "  1769.6,1311.83 1771.62,1316.71 1773.63,1321.87 1775.65,1325.52 1777.67,1316.69 1779.69,1332.91 1781.71,1313.49 1783.72,1317.39 1785.74,1300.18 1787.76,1330.84 \n",
       "  1789.78,1312.9 1791.8,1314.04 1793.81,1316.25 1795.83,1311.86 1797.85,1324.12 1799.87,1303.98 1801.89,1337.79 1803.9,1328.42 1805.92,1301.13 1807.94,1321.01 \n",
       "  1809.96,1328.7 1811.98,1329.7 1814,1313.53 1816.01,1317.21 1818.03,1325.02 1820.05,1331.55 1822.07,1339.74 1824.09,1331.48 1826.1,1314.23 1828.12,1315.36 \n",
       "  1830.14,1305.52 1832.16,1326.19 1834.18,1306.76 1836.19,1322.66 1838.21,1337.36 1840.23,1341.97 1842.25,1323.01 1844.27,1349.42 1846.28,1337.45 1848.3,1317.73 \n",
       "  1850.32,1329.66 1852.34,1323.86 1854.36,1340.96 1856.37,1326.85 1858.39,1333.75 1860.41,1333.35 1862.43,1321.77 1864.45,1331.09 1866.46,1342.76 1868.48,1354.22 \n",
       "  1870.5,1335.62 1872.52,1341.03 1874.54,1335.26 1876.55,1306.67 1878.57,1322.83 1880.59,1337.01 1882.61,1333.88 1884.63,1311.12 1886.65,1318.43 1888.66,1338.93 \n",
       "  1890.68,1342.5 1892.7,1339.02 1894.72,1316.12 1896.74,1326.63 1898.75,1334.92 1900.77,1333.74 1902.79,1345.04 1904.81,1339.59 1906.83,1336.71 1908.84,1337.94 \n",
       "  1910.86,1333.32 1912.88,1344.82 1914.9,1332.75 1916.92,1334.46 1918.93,1326.78 1920.95,1351.36 1922.97,1346.42 1924.99,1330.24 1927.01,1339.44 1929.02,1334.31 \n",
       "  1931.04,1332.77 1933.06,1330.53 1935.08,1341.56 1937.1,1345.57 1939.11,1335.49 1941.13,1360.24 1943.15,1338.59 1945.17,1349.16 1947.19,1339.45 1949.2,1341.18 \n",
       "  1951.22,1363.11 1953.24,1344.08 1955.26,1337.4 1957.28,1332.79 1959.3,1317.95 1961.31,1321.72 1963.33,1338.22 1965.35,1352.03 1967.37,1328.28 1969.39,1336.16 \n",
       "  1971.4,1322.48 1973.42,1345.6 1975.44,1363.09 1977.46,1338.15 1979.48,1331.2 1981.49,1335.03 1983.51,1332.6 1985.53,1339 1987.55,1343.62 1989.57,1355.09 \n",
       "  1991.58,1335 1993.6,1322.05 1995.62,1340.95 1997.64,1331.22 1999.66,1328.02 2001.67,1344.01 2003.69,1349.78 2005.71,1339.21 2007.73,1334.99 2009.75,1320.72 \n",
       "  2011.76,1312.53 2013.78,1327.2 2015.8,1338.64 2017.82,1336.76 2019.84,1335.74 2021.86,1341.98 2023.87,1342.82 2025.89,1336.29 2027.91,1339.36 2029.93,1351.75 \n",
       "  2031.95,1358.44 2033.96,1348.53 2035.98,1337.67 2038,1338 2040.02,1337.83 2042.04,1355.81 2044.05,1347.38 2046.07,1358.55 2048.09,1357.59 2050.11,1349.02 \n",
       "  2052.13,1328.66 2054.14,1353.94 2056.16,1344.57 2058.18,1346.38 2060.2,1359.27 2062.22,1348.1 2064.23,1348.99 2066.25,1365.69 2068.27,1345.43 2070.29,1339.17 \n",
       "  2072.31,1349.21 2074.32,1351.41 2076.34,1336.1 2078.36,1334.04 2080.38,1347.91 2082.4,1336.38 2084.41,1353.71 2086.43,1347.88 2088.45,1352.16 2090.47,1343.97 \n",
       "  2092.49,1338.57 2094.51,1339.15 2096.52,1347.09 2098.54,1342.51 2100.56,1352.86 2102.58,1341.4 2104.6,1336.36 2106.61,1360.18 2108.63,1354.18 2110.65,1340.78 \n",
       "  2112.67,1336.92 2114.69,1358.33 2116.7,1338.63 2118.72,1355.37 2120.74,1350.52 2122.76,1353.83 2124.78,1345.75 2126.79,1358.27 2128.81,1353.27 2130.83,1371.69 \n",
       "  2132.85,1363.44 2134.87,1359.04 2136.88,1336.88 2138.9,1347.92 2140.92,1357.75 2142.94,1351.56 2144.96,1360 2146.97,1367.62 2148.99,1357.35 2151.01,1356.39 \n",
       "  2153.03,1370.49 2155.05,1336.05 2157.06,1374.65 2159.08,1348.41 2161.1,1344.55 2163.12,1341.38 2165.14,1362.32 2167.16,1355.72 2169.17,1348.03 2171.19,1359.39 \n",
       "  2173.21,1372.69 2175.23,1363.95 2177.25,1349.15 2179.26,1353.14 2181.28,1351.75 2183.3,1351.99 2185.32,1362.24 2187.34,1364.58 2189.35,1355 2191.37,1333.06 \n",
       "  2193.39,1365.65 2195.41,1351.21 2197.43,1331.93 2199.44,1333.21 2201.46,1365.91 2203.48,1357.58 2205.5,1351.26 2207.52,1338.06 2209.53,1345.26 2211.55,1326.92 \n",
       "  2213.57,1365.88 2215.59,1361.62 2217.61,1367.1 2219.62,1365.34 2221.64,1345.31 2223.66,1355.94 2225.68,1356.21 2227.7,1358.76 2229.71,1338.41 2231.73,1367.26 \n",
       "  2233.75,1344.92 2235.77,1388.71 2237.79,1343.41 2239.81,1351.65 2241.82,1352.72 2243.84,1359.77 2245.86,1369.89 2247.88,1357.46 2249.9,1351.42 2251.91,1361.05 \n",
       "  2253.93,1343.78 2255.95,1356.85 2257.97,1359.41 2259.99,1334.15 2262,1360.01 2264.02,1363 2266.04,1362.95 2268.06,1356.18 2270.08,1351.11 2272.09,1358.25 \n",
       "  2274.11,1364.15 2276.13,1357.44 2278.15,1370.93 2280.17,1360.13 2282.18,1351.46 2284.2,1363.74 2286.22,1381.5 2288.24,1363.73 2290.26,1346.67 2292.27,1347 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip9300)\" d=\"\n",
       "M1989.93 326.155 L2280.76 326.155 L2280.76 205.195 L1989.93 205.195  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,326.155 2280.76,326.155 2280.76,205.195 1989.93,205.195 1989.93,326.155 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9300)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,265.675 2157.93,265.675 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 283.175)\" x=\"2181.93\" y=\"283.175\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_error_plot(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip9700\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip9700)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9701\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip9700)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 121.675 L215.754 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9702\">\n",
       "    <rect x=\"215\" y=\"121\" width=\"2138\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1353.26 2352.76,1353.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1046.5 2352.76,1046.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,739.728 2352.76,739.728 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,432.96 2352.76,432.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,126.192 2352.76,126.192 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1353.26 247.809,1353.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1046.5 247.809,1046.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,739.728 247.809,739.728 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,432.96 247.809,432.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,126.192 247.809,126.192 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 274.217, 1479.62)\" x=\"274.217\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 778.732, 1479.62)\" x=\"778.732\" y=\"1479.62\">250</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1283.25, 1479.62)\" x=\"1283.25\" y=\"1479.62\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1787.76, 1479.62)\" x=\"1787.76\" y=\"1479.62\">750</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2292.27, 1479.62)\" x=\"2292.27\" y=\"1479.62\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1370.76)\" x=\"191.754\" y=\"1370.76\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1064)\" x=\"191.754\" y=\"1064\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 757.228)\" x=\"191.754\" y=\"757.228\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 450.46)\" x=\"191.754\" y=\"450.46\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 143.692)\" x=\"191.754\" y=\"143.692\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 73.2)\" x=\"1284.25\" y=\"73.2\">error with N=3 bits</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">gradient descent step</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9702)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,158.579 278.253,212.032 280.271,167.467 282.289,229.569 284.307,179.19 286.326,226.855 288.344,219.693 290.362,246.431 292.38,252.853 294.398,306.433 \n",
       "  296.416,247.198 298.434,295.27 300.452,300.73 302.47,288.77 304.488,294.462 306.506,285.63 308.524,266.569 310.542,293.888 312.56,298.855 314.578,311.853 \n",
       "  316.596,343.998 318.614,324.576 320.633,330.336 322.651,346.736 324.669,316.787 326.687,360.015 328.705,339.593 330.723,375.75 332.741,350.836 334.759,386.958 \n",
       "  336.777,349.584 338.795,365.838 340.813,391.44 342.831,384.771 344.849,406.827 346.867,397.428 348.885,447.714 350.903,435.159 352.921,411.095 354.94,408.983 \n",
       "  356.958,435.18 358.976,433.78 360.994,435.831 363.012,445.317 365.03,424.278 367.048,442.302 369.066,458.843 371.084,435.203 373.102,463.478 375.12,511.803 \n",
       "  377.138,450.497 379.156,497.845 381.174,506.251 383.192,480.17 385.21,491.314 387.228,476.041 389.246,506.999 391.265,497.613 393.283,514.056 395.301,492.117 \n",
       "  397.319,508.877 399.337,549.721 401.355,532.713 403.373,544.612 405.391,531.167 407.409,528.57 409.427,543.474 411.445,547.063 413.463,567.573 415.481,569.308 \n",
       "  417.499,555.429 419.517,558.705 421.535,555.512 423.553,552.89 425.572,595.647 427.59,626.512 429.608,571.831 431.626,571.433 433.644,582.448 435.662,621.91 \n",
       "  437.68,601.819 439.698,598.696 441.716,634.92 443.734,605.692 445.752,645.757 447.77,627.329 449.788,621.39 451.806,614.14 453.824,628.037 455.842,618.732 \n",
       "  457.86,615.736 459.879,623.381 461.897,665.758 463.915,643.136 465.933,634.743 467.951,637.433 469.969,666.039 471.987,656.627 474.005,672.847 476.023,679.313 \n",
       "  478.041,669.562 480.059,664.312 482.077,683.1 484.095,692.1 486.113,697.634 488.131,706.384 490.149,696.766 492.167,712.006 494.185,719.856 496.204,720.433 \n",
       "  498.222,711.173 500.24,712.535 502.258,689.218 504.276,713.534 506.294,745.561 508.312,726.636 510.33,723.511 512.348,740.838 514.366,752.493 516.384,746.732 \n",
       "  518.402,743.489 520.42,741.484 522.438,770.212 524.456,728.583 526.474,749.874 528.492,777.704 530.511,743.7 532.529,771.995 534.547,769.172 536.565,788.728 \n",
       "  538.583,773.633 540.601,765.343 542.619,796.526 544.637,778.47 546.655,768.458 548.673,794.152 550.691,790.887 552.709,814.387 554.727,809.338 556.745,805.665 \n",
       "  558.763,773.333 560.781,802.241 562.799,812.231 564.817,816.371 566.836,804.88 568.854,801.757 570.872,823.835 572.89,823.517 574.908,819.18 576.926,832.654 \n",
       "  578.944,812.232 580.962,825.992 582.98,832.159 584.998,841.674 587.016,826.104 589.034,845.389 591.052,847.527 593.07,872.511 595.088,867.206 597.106,833.718 \n",
       "  599.124,850.706 601.143,854.432 603.161,869.566 605.179,862.526 607.197,855.085 609.215,869.306 611.233,887.501 613.251,858.66 615.269,863.196 617.287,869.312 \n",
       "  619.305,862.568 621.323,881.526 623.341,873.532 625.359,895.865 627.377,911.525 629.395,876.672 631.413,876.01 633.431,884.374 635.45,896.428 637.468,886.121 \n",
       "  639.486,900.196 641.504,887.204 643.522,911.258 645.54,889.216 647.558,902.418 649.576,917.203 651.594,902.995 653.612,912.576 655.63,918.302 657.648,930.456 \n",
       "  659.666,918.245 661.684,932.423 663.702,900.643 665.72,934.474 667.738,909.415 669.756,936.675 671.775,921.21 673.793,929.365 675.811,911.67 677.829,935.47 \n",
       "  679.847,945.114 681.865,946.348 683.883,948.055 685.901,938.033 687.919,948.746 689.937,951.581 691.955,952.842 693.973,938.103 695.991,954.524 698.009,961.513 \n",
       "  700.027,954.469 702.045,951.472 704.063,935.927 706.082,943.359 708.1,962.226 710.118,948.342 712.136,968.825 714.154,960.294 716.172,969.988 718.19,961.687 \n",
       "  720.208,972.984 722.226,969.407 724.244,980.871 726.262,984.454 728.28,985.855 730.298,981.043 732.316,984.567 734.334,991.066 736.352,961.603 738.37,971.512 \n",
       "  740.388,999.317 742.407,992.313 744.425,1004.24 746.443,993.807 748.461,983.164 750.479,998.113 752.497,995.949 754.515,1018.57 756.533,1003.45 758.551,998.345 \n",
       "  760.569,1000.26 762.587,1002.98 764.605,999.951 766.623,1014.1 768.641,1016.26 770.659,1012.07 772.677,1007.38 774.695,1006.58 776.714,1016.25 778.732,1016.17 \n",
       "  780.75,1010.95 782.768,1000.09 784.786,1027.02 786.804,1011.01 788.822,1032.37 790.84,1023.99 792.858,1032.11 794.876,1035.02 796.894,1038.21 798.912,1017.03 \n",
       "  800.93,1046.45 802.948,1029.76 804.966,1040.14 806.984,1028.71 809.002,1030.8 811.021,1028.17 813.039,1041.74 815.057,1070.6 817.075,1042.43 819.093,1057.97 \n",
       "  821.111,1044.43 823.129,1066.82 825.147,1049.35 827.165,1024.92 829.183,1066.85 831.201,1050.54 833.219,1071.26 835.237,1066.89 837.255,1049.38 839.273,1067.73 \n",
       "  841.291,1060.58 843.309,1054.79 845.327,1062.91 847.346,1065.65 849.364,1040.37 851.382,1059.72 853.4,1069.31 855.418,1071.49 857.436,1067.28 859.454,1083.01 \n",
       "  861.472,1080.05 863.49,1065.59 865.508,1056.04 867.526,1066.42 869.544,1083.28 871.562,1085.79 873.58,1072.59 875.598,1073.6 877.616,1079.02 879.634,1063.09 \n",
       "  881.653,1071.3 883.671,1073.02 885.689,1093.6 887.707,1078.2 889.725,1087.56 891.743,1080.57 893.761,1085.36 895.779,1088.54 897.797,1075.7 899.815,1092.68 \n",
       "  901.833,1093.79 903.851,1089.79 905.869,1075.34 907.887,1096.77 909.905,1110.02 911.923,1088.12 913.941,1112.91 915.959,1124.37 917.978,1090.57 919.996,1093.46 \n",
       "  922.014,1110.6 924.032,1091.34 926.05,1111.61 928.068,1109.21 930.086,1112.44 932.104,1107.86 934.122,1106.03 936.14,1109.4 938.158,1114.82 940.176,1111.12 \n",
       "  942.194,1119.57 944.212,1122.03 946.23,1099.99 948.248,1123.31 950.266,1107.56 952.285,1121.01 954.303,1120.64 956.321,1109.89 958.339,1123.28 960.357,1112.07 \n",
       "  962.375,1112.78 964.393,1135.4 966.411,1126.13 968.429,1111.77 970.447,1116.51 972.465,1131.15 974.483,1135.11 976.501,1110.88 978.519,1135.98 980.537,1142.07 \n",
       "  982.555,1124.37 984.573,1133.59 986.592,1131.74 988.61,1138.14 990.628,1152.12 992.646,1139.82 994.664,1118.02 996.682,1115.68 998.7,1129.57 1000.72,1129.86 \n",
       "  1002.74,1142.06 1004.75,1146.73 1006.77,1140.91 1008.79,1131.73 1010.81,1151.02 1012.83,1157.54 1014.84,1154.32 1016.86,1146.5 1018.88,1139.05 1020.9,1138.79 \n",
       "  1022.92,1152.61 1024.93,1157.87 1026.95,1151.77 1028.97,1144.35 1030.99,1139.98 1033.01,1151.43 1035.02,1154.56 1037.04,1154.53 1039.06,1145.6 1041.08,1169.25 \n",
       "  1043.1,1175.25 1045.12,1147.07 1047.13,1144.75 1049.15,1172.33 1051.17,1174.69 1053.19,1167.09 1055.21,1154.3 1057.22,1187.58 1059.24,1167.38 1061.26,1174.44 \n",
       "  1063.28,1170.97 1065.3,1150.34 1067.31,1157.65 1069.33,1158.72 1071.35,1159.25 1073.37,1176.61 1075.39,1153.14 1077.4,1153.46 1079.42,1176.28 1081.44,1159.69 \n",
       "  1083.46,1178.19 1085.48,1177.56 1087.49,1177 1089.51,1177.99 1091.53,1180.53 1093.55,1169.81 1095.57,1162.2 1097.58,1172.85 1099.6,1175.48 1101.62,1182.06 \n",
       "  1103.64,1175.7 1105.66,1154.02 1107.67,1184.22 1109.69,1178.75 1111.71,1182.82 1113.73,1188.45 1115.75,1175.04 1117.77,1195.98 1119.78,1184.38 1121.8,1192.77 \n",
       "  1123.82,1201.03 1125.84,1179.22 1127.86,1175.65 1129.87,1183.91 1131.89,1188.83 1133.91,1185.58 1135.93,1196.62 1137.95,1175.49 1139.96,1197.12 1141.98,1183.58 \n",
       "  1144,1189.7 1146.02,1197.9 1148.04,1206.2 1150.05,1190.23 1152.07,1207.26 1154.09,1198.48 1156.11,1212.88 1158.13,1197.34 1160.14,1205.15 1162.16,1202.58 \n",
       "  1164.18,1207.81 1166.2,1183.75 1168.22,1197.62 1170.23,1205.4 1172.25,1217.78 1174.27,1202.67 1176.29,1207.38 1178.31,1228.48 1180.33,1193.02 1182.34,1203.81 \n",
       "  1184.36,1206.99 1186.38,1197.5 1188.4,1197.11 1190.42,1223.51 1192.43,1218.54 1194.45,1192.43 1196.47,1198.3 1198.49,1221.3 1200.51,1214.54 1202.52,1228.08 \n",
       "  1204.54,1218.53 1206.56,1234.97 1208.58,1200.84 1210.6,1203.61 1212.61,1211.62 1214.63,1216.06 1216.65,1216.71 1218.67,1212.32 1220.69,1221.35 1222.7,1211.94 \n",
       "  1224.72,1204.14 1226.74,1209.74 1228.76,1218.57 1230.78,1239.24 1232.79,1229.39 1234.81,1224.48 1236.83,1231.01 1238.85,1221.57 1240.87,1203.83 1242.88,1218.15 \n",
       "  1244.9,1209.61 1246.92,1248.5 1248.94,1218.53 1250.96,1223.36 1252.98,1224.5 1254.99,1225.11 1257.01,1226.34 1259.03,1224.26 1261.05,1230.57 1263.07,1228.68 \n",
       "  1265.08,1236.01 1267.1,1228.86 1269.12,1231.81 1271.14,1232.01 1273.16,1221.31 1275.17,1231.59 1277.19,1244.66 1279.21,1240.07 1281.23,1241.3 1283.25,1229.39 \n",
       "  1285.26,1250.32 1287.28,1243.88 1289.3,1244.79 1291.32,1240.3 1293.34,1215.02 1295.35,1221.42 1297.37,1239.44 1299.39,1241.39 1301.41,1228.89 1303.43,1245.87 \n",
       "  1305.44,1234.7 1307.46,1232.25 1309.48,1239.34 1311.5,1248.54 1313.52,1246.27 1315.53,1240.59 1317.55,1230 1319.57,1247.74 1321.59,1251.84 1323.61,1262.7 \n",
       "  1325.63,1228.97 1327.64,1243.56 1329.66,1235.66 1331.68,1230.79 1333.7,1243.8 1335.72,1271.99 1337.73,1232.03 1339.75,1250.93 1341.77,1275.84 1343.79,1238.83 \n",
       "  1345.81,1234.99 1347.82,1261.05 1349.84,1243.81 1351.86,1245.41 1353.88,1242.3 1355.9,1265.17 1357.91,1229.34 1359.93,1258.02 1361.95,1229 1363.97,1259.19 \n",
       "  1365.99,1244.96 1368,1280.94 1370.02,1259.19 1372.04,1268.98 1374.06,1255.09 1376.08,1245.03 1378.09,1268.87 1380.11,1258.91 1382.13,1261.78 1384.15,1246.3 \n",
       "  1386.17,1252.94 1388.18,1252.24 1390.2,1261.98 1392.22,1265.98 1394.24,1275.71 1396.26,1270.22 1398.28,1254.54 1400.29,1266.12 1402.31,1268.82 1404.33,1281.85 \n",
       "  1406.35,1263.98 1408.37,1269.58 1410.38,1272.44 1412.4,1258.92 1414.42,1272.94 1416.44,1277.85 1418.46,1256.35 1420.47,1263.13 1422.49,1271.07 1424.51,1274.75 \n",
       "  1426.53,1266.98 1428.55,1276.67 1430.56,1268.82 1432.58,1287.16 1434.6,1266.04 1436.62,1263.53 1438.64,1275.88 1440.65,1266.87 1442.67,1284.47 1444.69,1268.8 \n",
       "  1446.71,1280.1 1448.73,1277.41 1450.74,1283.07 1452.76,1267.52 1454.78,1267.3 1456.8,1276.41 1458.82,1289.95 1460.84,1286.81 1462.85,1286.26 1464.87,1278.2 \n",
       "  1466.89,1279.34 1468.91,1288.98 1470.93,1292.4 1472.94,1289.49 1474.96,1273.84 1476.98,1293.15 1479,1281.23 1481.02,1276.3 1483.03,1275.72 1485.05,1277.04 \n",
       "  1487.07,1289.42 1489.09,1272.04 1491.11,1317.41 1493.12,1274.25 1495.14,1279.28 1497.16,1299.78 1499.18,1297.17 1501.2,1286.59 1503.21,1290.23 1505.23,1287.14 \n",
       "  1507.25,1282.17 1509.27,1296.54 1511.29,1292.18 1513.3,1299.6 1515.32,1275.57 1517.34,1293.09 1519.36,1275.77 1521.38,1293.12 1523.39,1298.79 1525.41,1281.12 \n",
       "  1527.43,1279.7 1529.45,1291.51 1531.47,1306.08 1533.49,1287.82 1535.5,1304.36 1537.52,1286.16 1539.54,1286.09 1541.56,1297.58 1543.58,1290.12 1545.59,1306.31 \n",
       "  1547.61,1319.24 1549.63,1285.01 1551.65,1275.03 1553.67,1297.87 1555.68,1306.22 1557.7,1278.52 1559.72,1302.14 1561.74,1306.09 1563.76,1303.97 1565.77,1297.21 \n",
       "  1567.79,1289.25 1569.81,1296.67 1571.83,1302.69 1573.85,1298.05 1575.86,1289.34 1577.88,1307.15 1579.9,1300.65 1581.92,1280.72 1583.94,1295.3 1585.95,1297.88 \n",
       "  1587.97,1301.99 1589.99,1306.65 1592.01,1305.89 1594.03,1290.24 1596.04,1311.32 1598.06,1279.28 1600.08,1326.99 1602.1,1295.86 1604.12,1299.45 1606.14,1305.08 \n",
       "  1608.15,1308.4 1610.17,1310.27 1612.19,1308.69 1614.21,1294.36 1616.23,1319.2 1618.24,1300.61 1620.26,1308.68 1622.28,1303.27 1624.3,1309.9 1626.32,1297.71 \n",
       "  1628.33,1304.91 1630.35,1307.82 1632.37,1314.61 1634.39,1294.71 1636.41,1316.1 1638.42,1310.94 1640.44,1295.63 1642.46,1312.66 1644.48,1302.58 1646.5,1316.65 \n",
       "  1648.51,1315.48 1650.53,1307.96 1652.55,1308.42 1654.57,1298.02 1656.59,1316.18 1658.6,1307.07 1660.62,1319.22 1662.64,1307.03 1664.66,1305.35 1666.68,1327.09 \n",
       "  1668.69,1313.76 1670.71,1314.28 1672.73,1323.89 1674.75,1321.51 1676.77,1327.02 1678.79,1302.56 1680.8,1313.42 1682.82,1306.04 1684.84,1313.64 1686.86,1297.73 \n",
       "  1688.88,1313.3 1690.89,1316.91 1692.91,1311.78 1694.93,1333.2 1696.95,1324.53 1698.97,1315.79 1700.98,1305.83 1703,1316.45 1705.02,1310.59 1707.04,1304.48 \n",
       "  1709.06,1328.01 1711.07,1309.1 1713.09,1305.06 1715.11,1314.96 1717.13,1317.5 1719.15,1329.51 1721.16,1329.38 1723.18,1316.21 1725.2,1322.24 1727.22,1325.35 \n",
       "  1729.24,1339.43 1731.25,1321.15 1733.27,1323.16 1735.29,1315.24 1737.31,1314.5 1739.33,1322.08 1741.35,1334.87 1743.36,1317.62 1745.38,1317.85 1747.4,1326.7 \n",
       "  1749.42,1346.78 1751.44,1327.8 1753.45,1317.75 1755.47,1314.95 1757.49,1314.64 1759.51,1323.37 1761.53,1345.25 1763.54,1329.06 1765.56,1324.9 1767.58,1314.67 \n",
       "  1769.6,1304.78 1771.62,1336.79 1773.63,1315.43 1775.65,1307.41 1777.67,1320.59 1779.69,1319.9 1781.71,1325.01 1783.72,1332.33 1785.74,1328.45 1787.76,1329.88 \n",
       "  1789.78,1317.47 1791.8,1322.35 1793.81,1348.52 1795.83,1325.64 1797.85,1317.24 1799.87,1326.37 1801.89,1325.08 1803.9,1339.53 1805.92,1328.02 1807.94,1323.75 \n",
       "  1809.96,1335.08 1811.98,1346.6 1814,1346.47 1816.01,1333.78 1818.03,1318.06 1820.05,1325.55 1822.07,1328.05 1824.09,1331.61 1826.1,1333.72 1828.12,1338.47 \n",
       "  1830.14,1318.03 1832.16,1345.65 1834.18,1317.31 1836.19,1325.02 1838.21,1336.8 1840.23,1345.57 1842.25,1332.69 1844.27,1324.31 1846.28,1357.83 1848.3,1337.39 \n",
       "  1850.32,1320.99 1852.34,1329.76 1854.36,1313.14 1856.37,1330.02 1858.39,1349.01 1860.41,1328.4 1862.43,1344.24 1864.45,1341.45 1866.46,1336.13 1868.48,1330.41 \n",
       "  1870.5,1345.26 1872.52,1325.59 1874.54,1333.49 1876.55,1339.17 1878.57,1343.65 1880.59,1335.55 1882.61,1338.13 1884.63,1330.91 1886.65,1326.43 1888.66,1322.82 \n",
       "  1890.68,1356.48 1892.7,1331.61 1894.72,1339.09 1896.74,1358.14 1898.75,1340.5 1900.77,1351.19 1902.79,1333.76 1904.81,1341.47 1906.83,1354 1908.84,1337.05 \n",
       "  1910.86,1338.98 1912.88,1327.64 1914.9,1335.75 1916.92,1357.52 1918.93,1344.34 1920.95,1338.23 1922.97,1340.69 1924.99,1346.38 1927.01,1344.02 1929.02,1347.22 \n",
       "  1931.04,1333.64 1933.06,1346.59 1935.08,1347.77 1937.1,1336.87 1939.11,1330.61 1941.13,1342.5 1943.15,1344.55 1945.17,1334.46 1947.19,1334.64 1949.2,1340.77 \n",
       "  1951.22,1351.78 1953.24,1354.49 1955.26,1339.81 1957.28,1349 1959.3,1353.44 1961.31,1330.45 1963.33,1352.6 1965.35,1373.42 1967.37,1344.46 1969.39,1349.24 \n",
       "  1971.4,1345.62 1973.42,1358.79 1975.44,1358.29 1977.46,1353.93 1979.48,1345.54 1981.49,1358.17 1983.51,1333.38 1985.53,1343.43 1987.55,1347.82 1989.57,1343.53 \n",
       "  1991.58,1341.55 1993.6,1350.24 1995.62,1358.91 1997.64,1370.69 1999.66,1342.19 2001.67,1343.62 2003.69,1339.73 2005.71,1355.37 2007.73,1354.72 2009.75,1349.69 \n",
       "  2011.76,1353.47 2013.78,1353.31 2015.8,1360.68 2017.82,1334.52 2019.84,1344.03 2021.86,1362.12 2023.87,1333.18 2025.89,1348.81 2027.91,1357.78 2029.93,1352.71 \n",
       "  2031.95,1341.3 2033.96,1346.6 2035.98,1348.43 2038,1356.76 2040.02,1360.87 2042.04,1351.39 2044.05,1338.01 2046.07,1347.5 2048.09,1356.05 2050.11,1346.25 \n",
       "  2052.13,1360.55 2054.14,1360.43 2056.16,1336.57 2058.18,1347.42 2060.2,1341.21 2062.22,1345.28 2064.23,1333.95 2066.25,1357.37 2068.27,1346.85 2070.29,1345.38 \n",
       "  2072.31,1352.89 2074.32,1344.76 2076.34,1377.67 2078.36,1339.67 2080.38,1346.16 2082.4,1348.02 2084.41,1349.39 2086.43,1362.57 2088.45,1355.1 2090.47,1352.06 \n",
       "  2092.49,1327.76 2094.51,1322.13 2096.52,1346.7 2098.54,1358.23 2100.56,1356.69 2102.58,1344.63 2104.6,1350.66 2106.61,1354.34 2108.63,1368.46 2110.65,1363.84 \n",
       "  2112.67,1353.65 2114.69,1355.47 2116.7,1354.31 2118.72,1365.96 2120.74,1375.16 2122.76,1344.24 2124.78,1364.72 2126.79,1340.76 2128.81,1359.75 2130.83,1353.33 \n",
       "  2132.85,1352.11 2134.87,1349.12 2136.88,1360.34 2138.9,1350.49 2140.92,1348.16 2142.94,1351.91 2144.96,1368.63 2146.97,1359.64 2148.99,1356.84 2151.01,1356.13 \n",
       "  2153.03,1383.61 2155.05,1369.55 2157.06,1360.57 2159.08,1354.96 2161.1,1348.07 2163.12,1365.09 2165.14,1360.22 2167.16,1368.93 2169.17,1352.98 2171.19,1355.51 \n",
       "  2173.21,1363.18 2175.23,1366.42 2177.25,1359.51 2179.26,1365.9 2181.28,1365.84 2183.3,1364.32 2185.32,1381.79 2187.34,1357.82 2189.35,1368.58 2191.37,1361.5 \n",
       "  2193.39,1354.47 2195.41,1363.34 2197.43,1362.06 2199.44,1361.8 2201.46,1358.7 2203.48,1335.19 2205.5,1367.64 2207.52,1355.69 2209.53,1352.77 2211.55,1372.95 \n",
       "  2213.57,1361.33 2215.59,1351.1 2217.61,1376.48 2219.62,1341.56 2221.64,1378.76 2223.66,1360.91 2225.68,1357.56 2227.7,1380.05 2229.71,1368.68 2231.73,1339.57 \n",
       "  2233.75,1388.71 2235.77,1382.07 2237.79,1365.06 2239.81,1349.65 2241.82,1355.74 2243.84,1369.58 2245.86,1363.76 2247.88,1351.42 2249.9,1381.02 2251.91,1383.73 \n",
       "  2253.93,1374.36 2255.95,1364.8 2257.97,1363.83 2259.99,1366.1 2262,1368.13 2264.02,1357.79 2266.04,1375.56 2268.06,1366.74 2270.08,1352.62 2272.09,1363.02 \n",
       "  2274.11,1371.01 2276.13,1366.77 2278.15,1372.92 2280.17,1354.43 2282.18,1361.27 2284.2,1358.99 2286.22,1380.58 2288.24,1374.79 2290.26,1369.97 2292.27,1377.1 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip9700)\" d=\"\n",
       "M1989.93 326.155 L2280.76 326.155 L2280.76 205.195 L1989.93 205.195  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,326.155 2280.76,326.155 2280.76,205.195 1989.93,205.195 1989.93,326.155 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9700)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,265.675 2157.93,265.675 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 283.175)\" x=\"2181.93\" y=\"283.175\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_error_plot(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip0100\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0100)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0101\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0100)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 121.675 L215.754 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0102\">\n",
       "    <rect x=\"215\" y=\"121\" width=\"2138\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  275.227,1425.62 275.227,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  779.489,1425.62 779.489,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1283.75,1425.62 1283.75,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1788.01,1425.62 1788.01,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1328.77 2352.76,1328.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1019.97 2352.76,1019.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,711.178 2352.76,711.178 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,402.384 2352.76,402.384 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  275.227,1425.62 275.227,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  779.489,1425.62 779.489,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1283.75,1425.62 1283.75,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1788.01,1425.62 1788.01,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1328.77 247.809,1328.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1019.97 247.809,1019.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,711.178 247.809,711.178 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,402.384 247.809,402.384 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 275.227, 1479.62)\" x=\"275.227\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 779.489, 1479.62)\" x=\"779.489\" y=\"1479.62\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1283.75, 1479.62)\" x=\"1283.75\" y=\"1479.62\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1788.01, 1479.62)\" x=\"1788.01\" y=\"1479.62\">1500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2292.27, 1479.62)\" x=\"2292.27\" y=\"1479.62\">2000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1346.27)\" x=\"191.754\" y=\"1346.27\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1037.47)\" x=\"191.754\" y=\"1037.47\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 728.678)\" x=\"191.754\" y=\"728.678\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 419.884)\" x=\"191.754\" y=\"419.884\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 73.2)\" x=\"1284.25\" y=\"73.2\">error with N=3 bits</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">gradient descent step</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,166.747 277.244,191.981 278.252,207.921 279.261,158.579 280.269,191.896 281.278,179.293 282.286,217.548 283.295,215.509 284.303,211.712 285.312,205.955 \n",
       "  286.321,209.532 287.329,256.903 288.338,278.71 289.346,243.287 290.355,283.94 291.363,249.336 292.372,279.428 293.38,286.726 294.389,305.951 295.397,300.671 \n",
       "  296.406,307.673 297.414,294.5 298.423,283.427 299.431,332.935 300.44,337.435 301.448,340.864 302.457,314.984 303.465,359.195 304.474,290.723 305.482,341.988 \n",
       "  306.491,350.838 307.5,372.374 308.508,384.257 309.517,378.576 310.525,376.084 311.534,366.448 312.542,400.096 313.551,376.96 314.559,401.257 315.568,411.529 \n",
       "  316.576,378.439 317.585,397.515 318.593,410.348 319.602,411.785 320.61,431.957 321.619,435.831 322.627,413.641 323.636,437.095 324.644,415.577 325.653,453.982 \n",
       "  326.661,442.688 327.67,440.716 328.679,463.554 329.687,506.812 330.696,442.864 331.704,481.956 332.713,485.912 333.721,472.039 334.73,452.202 335.738,477.569 \n",
       "  336.747,512.15 337.755,494.53 338.764,490.028 339.772,509.321 340.781,536.016 341.789,494.96 342.798,517.676 343.806,525.273 344.815,544.868 345.823,550.427 \n",
       "  346.832,524.476 347.84,547.52 348.849,541.552 349.858,558.859 350.866,544.382 351.875,545.255 352.883,551.12 353.892,538.063 354.9,566.549 355.909,596.273 \n",
       "  356.917,598.919 357.926,580.385 358.934,584.151 359.943,601.579 360.951,607.059 361.96,603.022 362.968,606.148 363.977,615.715 364.985,582.457 365.994,605.583 \n",
       "  367.002,592.545 368.011,624.728 369.019,624.553 370.028,628.859 371.037,631.475 372.045,612.621 373.054,635.551 374.062,643.347 375.071,630.286 376.079,641.028 \n",
       "  377.088,665.295 378.096,651.483 379.105,652.134 380.113,667.083 381.122,691.914 382.13,682.451 383.139,685.241 384.147,681.197 385.156,672.862 386.164,668.088 \n",
       "  387.173,686.466 388.181,697.052 389.19,701.986 390.198,717.226 391.207,733.681 392.216,725.15 393.224,734.166 394.233,705.889 395.241,701.985 396.25,682.431 \n",
       "  397.258,702.858 398.267,712.555 399.275,718.127 400.284,735.716 401.292,754.698 402.301,725.626 403.309,730.671 404.318,740.198 405.326,729.324 406.335,748.224 \n",
       "  407.343,759.206 408.352,723.606 409.36,742.504 410.369,761.221 411.377,766.144 412.386,768.148 413.395,757.986 414.403,751.958 415.412,782.686 416.42,776.199 \n",
       "  417.429,770.096 418.437,751.589 419.446,785.814 420.454,778.927 421.463,797.525 422.471,774.018 423.48,789.045 424.488,783.191 425.497,802.569 426.505,802.557 \n",
       "  427.514,802.071 428.522,791.669 429.531,771.197 430.539,805.724 431.548,815.415 432.556,788.962 433.565,800.419 434.574,797.619 435.582,807.093 436.591,816.206 \n",
       "  437.599,804.491 438.608,843.654 439.616,846.398 440.625,826.716 441.633,836.927 442.642,816.797 443.65,836.868 444.659,838.503 445.667,838.012 446.676,862.045 \n",
       "  447.684,835.721 448.693,842.278 449.701,856.951 450.71,856.252 451.718,850.127 452.727,832.941 453.735,856.051 454.744,839.358 455.753,878.334 456.761,868.886 \n",
       "  457.77,867.137 458.778,885.879 459.787,860.119 460.795,899.136 461.804,867.962 462.812,856.755 463.821,910.052 464.829,881.817 465.838,891.704 466.846,892.677 \n",
       "  467.855,894.709 468.863,857.286 469.872,893.004 470.88,884.895 471.889,903.719 472.897,899.225 473.906,903.441 474.914,903.316 475.923,887.235 476.932,925.3 \n",
       "  477.94,879.167 478.949,905.199 479.957,915.662 480.966,924.608 481.974,901.277 482.983,928.587 483.991,924.96 485,934.117 486.008,919.513 487.017,927.34 \n",
       "  488.025,928.971 489.034,928.296 490.042,951.595 491.051,950.805 492.059,939.124 493.068,930.061 494.076,983.518 495.085,937.429 496.093,956.78 497.102,929.321 \n",
       "  498.111,954.962 499.119,950.374 500.128,952.402 501.136,950.87 502.145,940.221 503.153,940.123 504.162,959.077 505.17,952.124 506.179,958.506 507.187,969.813 \n",
       "  508.196,951.33 509.204,964.663 510.213,954.378 511.221,956.113 512.23,974.634 513.238,965.675 514.247,983.405 515.255,975.938 516.264,976.129 517.273,988.874 \n",
       "  518.281,969.539 519.29,980.515 520.298,972.797 521.307,972.083 522.315,988.457 523.324,983.265 524.332,982.04 525.341,998.904 526.349,973.136 527.358,995.86 \n",
       "  528.366,987.513 529.375,1000.25 530.383,986.77 531.392,999.477 532.4,993.346 533.409,994.759 534.417,1004.74 535.426,1002.63 536.434,999.776 537.443,1009.84 \n",
       "  538.452,1000.53 539.46,997.199 540.469,1005.93 541.477,1017.75 542.486,998.87 543.494,1014.25 544.503,1013.7 545.511,1019.86 546.52,1015.16 547.528,1008.98 \n",
       "  548.537,1007.09 549.545,1026.77 550.554,1021.56 551.562,1022.44 552.571,1010.51 553.579,1038.13 554.588,1035.68 555.596,1028.7 556.605,1009.92 557.613,1013.1 \n",
       "  558.622,1013.39 559.631,1024.35 560.639,1037.14 561.648,1036.27 562.656,1048.75 563.665,1048.17 564.673,1053.79 565.682,1036.23 566.69,1044.6 567.699,1040.83 \n",
       "  568.707,1050.72 569.716,1033.05 570.724,1040.83 571.733,1031.95 572.741,1040.34 573.75,1060.1 574.758,1050.18 575.767,1051.73 576.775,1055.91 577.784,1058.58 \n",
       "  578.792,1072.92 579.801,1041.91 580.81,1057.18 581.818,1065.19 582.827,1073.76 583.835,1059.41 584.844,1066.88 585.852,1052.87 586.861,1058.22 587.869,1075.34 \n",
       "  588.878,1054.9 589.886,1063.22 590.895,1077.56 591.903,1049.01 592.912,1072.49 593.92,1071.54 594.929,1079.87 595.937,1066.01 596.946,1071.19 597.954,1072.12 \n",
       "  598.963,1081.01 599.971,1063.05 600.98,1084.51 601.989,1081.57 602.997,1090.08 604.006,1080.83 605.014,1083.2 606.023,1088.27 607.031,1083.52 608.04,1071.74 \n",
       "  609.048,1091.6 610.057,1091.4 611.065,1083.31 612.074,1075.08 613.082,1109.22 614.091,1084.64 615.099,1082.41 616.108,1095.75 617.116,1088.06 618.125,1109.46 \n",
       "  619.133,1104.87 620.142,1095.65 621.15,1099.84 622.159,1130.35 623.168,1115.5 624.176,1102.86 625.185,1097.9 626.193,1105.48 627.202,1096.47 628.21,1113 \n",
       "  629.219,1110.96 630.227,1102.24 631.236,1084.78 632.244,1108.27 633.253,1124.45 634.261,1101.01 635.27,1103.56 636.278,1125.4 637.287,1127.17 638.295,1118.19 \n",
       "  639.304,1112.98 640.312,1124.72 641.321,1113.34 642.329,1102.75 643.338,1119.79 644.347,1112.06 645.355,1141.23 646.364,1128.19 647.372,1129.95 648.381,1119.12 \n",
       "  649.389,1120 650.398,1128.72 651.406,1145.83 652.415,1117.78 653.423,1126.27 654.432,1112.29 655.44,1117.61 656.449,1141.12 657.457,1135.93 658.466,1132.84 \n",
       "  659.474,1131.85 660.483,1147.03 661.491,1146.27 662.5,1133.71 663.508,1161.05 664.517,1150.58 665.526,1138.79 666.534,1148.65 667.543,1140.91 668.551,1149.23 \n",
       "  669.56,1142.02 670.568,1157.06 671.577,1143.47 672.585,1136.06 673.594,1125.84 674.602,1133.08 675.611,1131.66 676.619,1153.86 677.628,1149.68 678.636,1152.38 \n",
       "  679.645,1143.11 680.653,1157.05 681.662,1139.97 682.67,1136.2 683.679,1149.85 684.687,1169.96 685.696,1146.9 686.705,1141.91 687.713,1148.6 688.722,1170.06 \n",
       "  689.73,1154.84 690.739,1159.31 691.747,1147.84 692.756,1146.09 693.764,1146.84 694.773,1175.51 695.781,1161.37 696.79,1174.36 697.798,1161.1 698.807,1164.79 \n",
       "  699.815,1160.71 700.824,1169.06 701.832,1155.03 702.841,1185.32 703.849,1145.76 704.858,1183.99 705.866,1167.89 706.875,1158.01 707.884,1169.5 708.892,1172.45 \n",
       "  709.901,1182.88 710.909,1171.02 711.918,1144.33 712.926,1188.95 713.935,1160.4 714.943,1172.86 715.952,1163.83 716.96,1170.36 717.969,1170.51 718.977,1184.66 \n",
       "  719.986,1192.9 720.994,1176.84 722.003,1190.42 723.011,1193.83 724.02,1179.44 725.028,1185.45 726.037,1180.51 727.045,1194.42 728.054,1174.83 729.063,1181.52 \n",
       "  730.071,1172.81 731.08,1180.63 732.088,1197.28 733.097,1184.76 734.105,1195.16 735.114,1211.98 736.122,1190.49 737.131,1198.21 738.139,1189.89 739.148,1191.33 \n",
       "  740.156,1177.77 741.165,1185.12 742.173,1196.79 743.182,1192.05 744.19,1186.48 745.199,1194.91 746.207,1198.86 747.216,1198.72 748.224,1199.87 749.233,1206.22 \n",
       "  750.242,1207 751.25,1196.9 752.259,1192.48 753.267,1209.08 754.276,1186.81 755.284,1199.14 756.293,1177.92 757.301,1202.09 758.31,1193.31 759.318,1191.54 \n",
       "  760.327,1195.27 761.335,1184.98 762.344,1192.98 763.352,1205.37 764.361,1199.05 765.369,1187.55 766.378,1211.89 767.386,1203.9 768.395,1204.23 769.403,1201.41 \n",
       "  770.412,1200.2 771.421,1207.82 772.429,1206.42 773.438,1217.67 774.446,1218.17 775.455,1204.1 776.463,1221.02 777.472,1201.62 778.48,1204.82 779.489,1219.22 \n",
       "  780.497,1214.31 781.506,1199.94 782.514,1224.4 783.523,1221.19 784.531,1216.88 785.54,1201.52 786.548,1210.25 787.557,1222.42 788.565,1202.12 789.574,1216.91 \n",
       "  790.583,1211.34 791.591,1198.92 792.6,1233.53 793.608,1228.91 794.617,1221.35 795.625,1224.08 796.634,1218.42 797.642,1216.25 798.651,1236.75 799.659,1220.7 \n",
       "  800.668,1232.94 801.676,1221.6 802.685,1240.2 803.693,1219.77 804.702,1210.52 805.71,1244.95 806.719,1242.97 807.727,1244.9 808.736,1219.39 809.744,1234.47 \n",
       "  810.753,1224.07 811.762,1242.15 812.77,1225.97 813.779,1233.92 814.787,1231.47 815.796,1237.05 816.804,1222.05 817.813,1242.38 818.821,1247.47 819.83,1240.91 \n",
       "  820.838,1243.37 821.847,1246.7 822.855,1236.07 823.864,1235.04 824.872,1240.18 825.881,1233.04 826.889,1221.47 827.898,1236.75 828.906,1233.34 829.915,1259.27 \n",
       "  830.923,1244.85 831.932,1233.37 832.941,1251.04 833.949,1252.46 834.958,1226.48 835.966,1235.82 836.975,1239.76 837.983,1243.28 838.992,1245.76 840,1247.87 \n",
       "  841.009,1245.3 842.017,1256.69 843.026,1262.81 844.034,1245.97 845.043,1235.86 846.051,1251.2 847.06,1246.12 848.068,1236.89 849.077,1240.46 850.085,1239.93 \n",
       "  851.094,1249.63 852.102,1264.2 853.111,1244.11 854.12,1238.3 855.128,1237.22 856.137,1253.43 857.145,1254.8 858.154,1258.13 859.162,1262.47 860.171,1239.52 \n",
       "  861.179,1243.32 862.188,1243.89 863.196,1250.37 864.205,1249.24 865.213,1239.04 866.222,1252.46 867.23,1254.49 868.239,1258.95 869.247,1249.56 870.256,1265.31 \n",
       "  871.264,1272.45 872.273,1269.89 873.281,1251.03 874.29,1257.95 875.299,1245.12 876.307,1263.67 877.316,1249.24 878.324,1268.96 879.333,1260.33 880.341,1249.39 \n",
       "  881.35,1255.53 882.358,1254.07 883.367,1243.69 884.375,1249.51 885.384,1260.99 886.392,1245.3 887.401,1249.73 888.409,1270.48 889.418,1262.99 890.426,1253.44 \n",
       "  891.435,1243.15 892.443,1263.95 893.452,1265.39 894.46,1249.17 895.469,1274.98 896.478,1291.03 897.486,1268.91 898.495,1273.26 899.503,1287.39 900.512,1269.89 \n",
       "  901.52,1255.77 902.529,1265.08 903.537,1269.94 904.546,1270.98 905.554,1267.46 906.563,1280.04 907.571,1262.97 908.58,1262.82 909.588,1275.06 910.597,1278.88 \n",
       "  911.605,1254.07 912.614,1276.93 913.622,1265.38 914.631,1267.72 915.639,1267.59 916.648,1283.09 917.657,1277.65 918.665,1271.6 919.674,1267.59 920.682,1268.42 \n",
       "  921.691,1258.29 922.699,1261.34 923.708,1265.63 924.716,1282.01 925.725,1283.5 926.733,1278.9 927.742,1240.62 928.75,1279.71 929.759,1272.65 930.767,1278.78 \n",
       "  931.776,1272.75 932.784,1281.94 933.793,1281.41 934.801,1284.64 935.81,1290.74 936.818,1261.12 937.827,1267.6 938.836,1284.6 939.844,1284.88 940.853,1266.99 \n",
       "  941.861,1273.17 942.87,1290.8 943.878,1282.1 944.887,1296.33 945.895,1283.68 946.904,1294.17 947.912,1293.64 948.921,1274.5 949.929,1283.62 950.938,1285.9 \n",
       "  951.946,1275.26 952.955,1285.94 953.963,1265.27 954.972,1284.53 955.98,1301.33 956.989,1281.57 957.997,1279.89 959.006,1291.65 960.015,1278.82 961.023,1289.52 \n",
       "  962.032,1290.63 963.04,1285.72 964.049,1300.21 965.057,1284.18 966.066,1267.89 967.074,1309.18 968.083,1283.05 969.091,1291.7 970.1,1284.51 971.108,1302.93 \n",
       "  972.117,1294.43 973.125,1283.67 974.134,1302.36 975.142,1288.34 976.151,1299.08 977.159,1291.34 978.168,1310.31 979.176,1288.25 980.185,1294.24 981.194,1295.23 \n",
       "  982.202,1303.35 983.211,1280.62 984.219,1291.4 985.228,1298.71 986.236,1274.83 987.245,1285.88 988.253,1300.98 989.262,1302.22 990.27,1296.54 991.279,1294.84 \n",
       "  992.287,1308 993.296,1286.78 994.304,1315.42 995.313,1300.21 996.321,1318.55 997.33,1292.08 998.338,1311.14 999.347,1286.14 1000.36,1291.29 1001.36,1287.9 \n",
       "  1002.37,1283.23 1003.38,1288.17 1004.39,1291.63 1005.4,1304.96 1006.41,1305.76 1007.42,1289.04 1008.42,1301.68 1009.43,1314.43 1010.44,1287.86 1011.45,1300.46 \n",
       "  1012.46,1290.64 1013.47,1296.43 1014.47,1314.8 1015.48,1306.97 1016.49,1301.48 1017.5,1305.75 1018.51,1314.5 1019.52,1304.48 1020.53,1300.69 1021.53,1306.06 \n",
       "  1022.54,1285.22 1023.55,1302.77 1024.56,1297.1 1025.57,1308.08 1026.58,1310.85 1027.59,1320.08 1028.59,1300.77 1029.6,1284.98 1030.61,1300.81 1031.62,1295.01 \n",
       "  1032.63,1301.19 1033.64,1310.39 1034.65,1293.83 1035.65,1326.65 1036.66,1300.19 1037.67,1320.28 1038.68,1307.31 1039.69,1297.81 1040.7,1287.56 1041.7,1306.14 \n",
       "  1042.71,1301.34 1043.72,1325.61 1044.73,1297.45 1045.74,1303.78 1046.75,1306.1 1047.76,1307.21 1048.76,1320.34 1049.77,1301.71 1050.78,1310.66 1051.79,1288.71 \n",
       "  1052.8,1306.68 1053.81,1325.49 1054.82,1293.7 1055.82,1300.19 1056.83,1306.8 1057.84,1308.7 1058.85,1317.95 1059.86,1299.65 1060.87,1300.8 1061.88,1302.83 \n",
       "  1062.88,1321.97 1063.89,1302.3 1064.9,1314.69 1065.91,1306.11 1066.92,1313.42 1067.93,1309.47 1068.94,1316.06 1069.94,1320.4 1070.95,1306.76 1071.96,1306.19 \n",
       "  1072.97,1311.83 1073.98,1302 1074.99,1316.38 1075.99,1305.05 1077,1322.73 1078.01,1319.9 1079.02,1305.46 1080.03,1324.16 1081.04,1314.09 1082.05,1326.4 \n",
       "  1083.05,1330.63 1084.06,1322.4 1085.07,1328.52 1086.08,1305.44 1087.09,1312.97 1088.1,1301.59 1089.11,1300.12 1090.11,1330.36 1091.12,1312.83 1092.13,1314.78 \n",
       "  1093.14,1344.46 1094.15,1319 1095.16,1316.84 1096.17,1329.15 1097.17,1308.76 1098.18,1313.07 1099.19,1318.32 1100.2,1299.4 1101.21,1334.24 1102.22,1323.6 \n",
       "  1103.22,1327.87 1104.23,1308.04 1105.24,1338.43 1106.25,1308.9 1107.26,1304.18 1108.27,1334.94 1109.28,1327.31 1110.28,1313.27 1111.29,1312.08 1112.3,1313.94 \n",
       "  1113.31,1333.15 1114.32,1321.07 1115.33,1323.24 1116.34,1302.85 1117.34,1320.41 1118.35,1324.71 1119.36,1334.27 1120.37,1309 1121.38,1350.11 1122.39,1319.27 \n",
       "  1123.4,1330.92 1124.4,1327.23 1125.41,1329.93 1126.42,1312.02 1127.43,1331.37 1128.44,1320.54 1129.45,1334.58 1130.46,1327.49 1131.46,1327.23 1132.47,1326.49 \n",
       "  1133.48,1331.07 1134.49,1322.62 1135.5,1310.55 1136.51,1324.65 1137.51,1326.19 1138.52,1314.98 1139.53,1321.37 1140.54,1333.48 1141.55,1319.71 1142.56,1315.41 \n",
       "  1143.57,1321.96 1144.57,1319.95 1145.58,1334.24 1146.59,1316.36 1147.6,1314.75 1148.61,1330.42 1149.62,1326.91 1150.63,1351.04 1151.63,1324.45 1152.64,1338.98 \n",
       "  1153.65,1328.03 1154.66,1314.8 1155.67,1333.12 1156.68,1336.65 1157.69,1327.55 1158.69,1329.99 1159.7,1316.38 1160.71,1325.07 1161.72,1311.72 1162.73,1327.54 \n",
       "  1163.74,1314.14 1164.74,1321.95 1165.75,1344.7 1166.76,1331.36 1167.77,1327.68 1168.78,1318.36 1169.79,1319.85 1170.8,1328.21 1171.8,1317.3 1172.81,1327.45 \n",
       "  1173.82,1331.29 1174.83,1326.33 1175.84,1356.57 1176.85,1315.14 1177.86,1316.28 1178.86,1324.22 1179.87,1345.42 1180.88,1328.68 1181.89,1323.6 1182.9,1306.22 \n",
       "  1183.91,1330.42 1184.92,1317.43 1185.92,1330.4 1186.93,1344.38 1187.94,1330.94 1188.95,1348.01 1189.96,1352.95 1190.97,1338.72 1191.98,1332.22 1192.98,1346.63 \n",
       "  1193.99,1330.01 1195,1334.79 1196.01,1348.44 1197.02,1334.87 1198.03,1332.68 1199.03,1320.09 1200.04,1337.8 1201.05,1332.53 1202.06,1326.46 1203.07,1336.15 \n",
       "  1204.08,1333.63 1205.09,1332.49 1206.09,1355.99 1207.1,1338.35 1208.11,1335.44 1209.12,1319.47 1210.13,1331.33 1211.14,1331.07 1212.15,1343.32 1213.15,1328.07 \n",
       "  1214.16,1315.33 1215.17,1326.34 1216.18,1348.06 1217.19,1310.8 1218.2,1322.29 1219.21,1339.71 1220.21,1347.87 1221.22,1334.13 1222.23,1326.23 1223.24,1323.06 \n",
       "  1224.25,1318.65 1225.26,1310.1 1226.26,1346.78 1227.27,1350.43 1228.28,1343.33 1229.29,1333.75 1230.3,1338.37 1231.31,1332.41 1232.32,1349.03 1233.32,1343.52 \n",
       "  1234.33,1351.76 1235.34,1335.83 1236.35,1337.84 1237.36,1321.81 1238.37,1334.47 1239.38,1347.33 1240.38,1333.03 1241.39,1311.14 1242.4,1367.39 1243.41,1346.23 \n",
       "  1244.42,1328.14 1245.43,1352.36 1246.44,1318.36 1247.44,1340.16 1248.45,1333.7 1249.46,1342.42 1250.47,1350 1251.48,1328.43 1252.49,1352.45 1253.5,1346.29 \n",
       "  1254.5,1362.55 1255.51,1337.21 1256.52,1358.53 1257.53,1331.74 1258.54,1332.24 1259.55,1347.17 1260.55,1340.32 1261.56,1335.43 1262.57,1338.26 1263.58,1343.38 \n",
       "  1264.59,1328.75 1265.6,1353.37 1266.61,1339.48 1267.61,1338.09 1268.62,1323.68 1269.63,1330.41 1270.64,1357.81 1271.65,1338.88 1272.66,1361.77 1273.67,1321.42 \n",
       "  1274.67,1347.4 1275.68,1344.16 1276.69,1356.76 1277.7,1345.18 1278.71,1323.64 1279.72,1352.22 1280.73,1339.91 1281.73,1335.59 1282.74,1336.87 1283.75,1340.71 \n",
       "  1284.76,1330.96 1285.77,1351.43 1286.78,1336.25 1287.78,1344.12 1288.79,1350.99 1289.8,1359.08 1290.81,1356.24 1291.82,1355.13 1292.83,1347.11 1293.84,1340.4 \n",
       "  1294.84,1347.59 1295.85,1347.26 1296.86,1330.38 1297.87,1339.59 1298.88,1339.06 1299.89,1326.06 1300.9,1350.98 1301.9,1357.84 1302.91,1351.78 1303.92,1372.27 \n",
       "  1304.93,1345.43 1305.94,1343.6 1306.95,1345.71 1307.96,1343.23 1308.96,1340.15 1309.97,1350.79 1310.98,1347.89 1311.99,1336.18 1313,1345.86 1314.01,1331.5 \n",
       "  1315.01,1360.6 1316.02,1338.71 1317.03,1349.26 1318.04,1335.77 1319.05,1364.61 1320.06,1328.41 1321.07,1345.61 1322.07,1349.43 1323.08,1339.98 1324.09,1326.33 \n",
       "  1325.1,1355.72 1326.11,1343.97 1327.12,1330.54 1328.13,1359.29 1329.13,1354.15 1330.14,1340.56 1331.15,1357.91 1332.16,1360.08 1333.17,1341.22 1334.18,1363.34 \n",
       "  1335.19,1343.11 1336.19,1324.89 1337.2,1343.53 1338.21,1352.17 1339.22,1360.09 1340.23,1353.02 1341.24,1355.45 1342.25,1356.86 1343.25,1348.15 1344.26,1348.22 \n",
       "  1345.27,1334.59 1346.28,1344.18 1347.29,1340.17 1348.3,1349.58 1349.3,1354.42 1350.31,1342.09 1351.32,1349.25 1352.33,1349.73 1353.34,1351.82 1354.35,1351.87 \n",
       "  1355.36,1338.71 1356.36,1349.87 1357.37,1340.86 1358.38,1336.16 1359.39,1334.21 1360.4,1345.31 1361.41,1338.41 1362.42,1359.34 1363.42,1356.04 1364.43,1336.66 \n",
       "  1365.44,1347.26 1366.45,1350.61 1367.46,1339.99 1368.47,1345.73 1369.48,1342.33 1370.48,1336.51 1371.49,1335.85 1372.5,1347.02 1373.51,1361.5 1374.52,1339.91 \n",
       "  1375.53,1363.19 1376.53,1331.79 1377.54,1355.93 1378.55,1352.1 1379.56,1344.26 1380.57,1322.53 1381.58,1356.81 1382.59,1354.03 1383.59,1352.76 1384.6,1341.74 \n",
       "  1385.61,1351.41 1386.62,1348.89 1387.63,1345 1388.64,1336.97 1389.65,1342.04 1390.65,1339.25 1391.66,1348.07 1392.67,1359.25 1393.68,1352.98 1394.69,1356.64 \n",
       "  1395.7,1353.93 1396.71,1357.25 1397.71,1362.02 1398.72,1341.76 1399.73,1342.22 1400.74,1367.91 1401.75,1339.7 1402.76,1351.24 1403.77,1347.59 1404.77,1340.12 \n",
       "  1405.78,1344.89 1406.79,1360.07 1407.8,1360.65 1408.81,1350.99 1409.82,1354.9 1410.82,1345.56 1411.83,1347.27 1412.84,1334.89 1413.85,1336.69 1414.86,1339.69 \n",
       "  1415.87,1356.38 1416.88,1336.95 1417.88,1360.13 1418.89,1353.4 1419.9,1337.73 1420.91,1351.29 1421.92,1369.98 1422.93,1339.79 1423.94,1351.45 1424.94,1357.08 \n",
       "  1425.95,1351.72 1426.96,1341.93 1427.97,1344.52 1428.98,1371.81 1429.99,1362.53 1431,1356.36 1432,1359.48 1433.01,1333.26 1434.02,1354.63 1435.03,1339.35 \n",
       "  1436.04,1351.44 1437.05,1364.97 1438.05,1338.86 1439.06,1360.59 1440.07,1344.79 1441.08,1341.9 1442.09,1356.4 1443.1,1347.04 1444.11,1354.76 1445.11,1360.48 \n",
       "  1446.12,1342.22 1447.13,1342.83 1448.14,1362.01 1449.15,1338.61 1450.16,1331.55 1451.17,1345.64 1452.17,1344.54 1453.18,1363.7 1454.19,1354.19 1455.2,1345.87 \n",
       "  1456.21,1344.96 1457.22,1340.2 1458.23,1345.97 1459.23,1343.91 1460.24,1338.37 1461.25,1355.3 1462.26,1351.95 1463.27,1359.43 1464.28,1352.07 1465.29,1355.5 \n",
       "  1466.29,1368.75 1467.3,1363.46 1468.31,1361.22 1469.32,1354.87 1470.33,1362.83 1471.34,1370.06 1472.34,1359.95 1473.35,1384.07 1474.36,1353.12 1475.37,1358.5 \n",
       "  1476.38,1345.96 1477.39,1350.3 1478.4,1343.03 1479.4,1344.84 1480.41,1354.83 1481.42,1359.66 1482.43,1350.17 1483.44,1331.95 1484.45,1348.83 1485.46,1365.81 \n",
       "  1486.46,1347.37 1487.47,1337.71 1488.48,1362.96 1489.49,1345.5 1490.5,1337.41 1491.51,1356.49 1492.52,1350.9 1493.52,1356.57 1494.53,1353.96 1495.54,1354.64 \n",
       "  1496.55,1346.57 1497.56,1357.59 1498.57,1334.74 1499.57,1372.3 1500.58,1348.9 1501.59,1331.2 1502.6,1373.65 1503.61,1342.29 1504.62,1361.54 1505.63,1333.71 \n",
       "  1506.63,1340.61 1507.64,1339.24 1508.65,1376.61 1509.66,1361.78 1510.67,1359.53 1511.68,1340.54 1512.69,1362.28 1513.69,1348.42 1514.7,1365.17 1515.71,1379.64 \n",
       "  1516.72,1353.89 1517.73,1362.97 1518.74,1374.22 1519.75,1352.18 1520.75,1341.65 1521.76,1350.37 1522.77,1358.17 1523.78,1347.61 1524.79,1366.22 1525.8,1342.56 \n",
       "  1526.81,1368.72 1527.81,1358.2 1528.82,1363.38 1529.83,1346.08 1530.84,1362.27 1531.85,1355.54 1532.86,1354.36 1533.86,1364.92 1534.87,1353.37 1535.88,1365.84 \n",
       "  1536.89,1371.54 1537.9,1362.31 1538.91,1364.03 1539.92,1344.26 1540.92,1347.77 1541.93,1361.36 1542.94,1364.14 1543.95,1353.06 1544.96,1358.07 1545.97,1359.57 \n",
       "  1546.98,1353.66 1547.98,1351.93 1548.99,1343.25 1550,1354.04 1551.01,1320.89 1552.02,1345.69 1553.03,1364.8 1554.04,1333.83 1555.04,1344.38 1556.05,1340.33 \n",
       "  1557.06,1349.93 1558.07,1349.82 1559.08,1338.16 1560.09,1324.9 1561.09,1360.27 1562.1,1362.93 1563.11,1354.23 1564.12,1336.92 1565.13,1368.56 1566.14,1351.09 \n",
       "  1567.15,1362.03 1568.15,1356.61 1569.16,1347.88 1570.17,1340.84 1571.18,1354.46 1572.19,1362.83 1573.2,1369.41 1574.21,1352.85 1575.21,1333.66 1576.22,1338.32 \n",
       "  1577.23,1342 1578.24,1361.55 1579.25,1366.58 1580.26,1358.28 1581.27,1352.8 1582.27,1360.54 1583.28,1355.95 1584.29,1353.5 1585.3,1352.84 1586.31,1368.31 \n",
       "  1587.32,1333.21 1588.32,1352.08 1589.33,1346.53 1590.34,1368.63 1591.35,1361.32 1592.36,1359.08 1593.37,1345.33 1594.38,1349.56 1595.38,1366.54 1596.39,1345.12 \n",
       "  1597.4,1351.4 1598.41,1329.01 1599.42,1342.97 1600.43,1356.55 1601.44,1339.64 1602.44,1348.55 1603.45,1360.57 1604.46,1349.61 1605.47,1348.91 1606.48,1374.57 \n",
       "  1607.49,1341.55 1608.5,1356.19 1609.5,1348.12 1610.51,1339.6 1611.52,1349.66 1612.53,1343.6 1613.54,1353.35 1614.55,1370.58 1615.56,1345.79 1616.56,1361.55 \n",
       "  1617.57,1356.4 1618.58,1352.7 1619.59,1343.43 1620.6,1355.19 1621.61,1351.05 1622.61,1341.41 1623.62,1334.82 1624.63,1347.15 1625.64,1371.12 1626.65,1351.61 \n",
       "  1627.66,1362.04 1628.67,1349.28 1629.67,1346.85 1630.68,1348.97 1631.69,1344.26 1632.7,1362.7 1633.71,1388.71 1634.72,1333.23 1635.73,1344.72 1636.73,1354.08 \n",
       "  1637.74,1359.62 1638.75,1341.39 1639.76,1342.38 1640.77,1352.9 1641.78,1356.27 1642.79,1360.9 1643.79,1358.45 1644.8,1354.7 1645.81,1369.91 1646.82,1352.21 \n",
       "  1647.83,1357.21 1648.84,1340.28 1649.84,1358.04 1650.85,1356.04 1651.86,1367.76 1652.87,1365.79 1653.88,1358.58 1654.89,1377.35 1655.9,1347.26 1656.9,1342.89 \n",
       "  1657.91,1344.65 1658.92,1351.6 1659.93,1344.48 1660.94,1362.34 1661.95,1336.11 1662.96,1344.48 1663.96,1363.97 1664.97,1360.51 1665.98,1338.77 1666.99,1366.32 \n",
       "  1668,1353.99 1669.01,1362.2 1670.02,1360.49 1671.02,1360.74 1672.03,1367.89 1673.04,1352.17 1674.05,1362.71 1675.06,1366.89 1676.07,1349.58 1677.08,1357.14 \n",
       "  1678.08,1354.71 1679.09,1362.7 1680.1,1346.14 1681.11,1366.04 1682.12,1360.43 1683.13,1357.73 1684.13,1337.9 1685.14,1358.27 1686.15,1337.12 1687.16,1361.55 \n",
       "  1688.17,1368.32 1689.18,1362.39 1690.19,1358.09 1691.19,1358.45 1692.2,1341.96 1693.21,1336.15 1694.22,1357.58 1695.23,1356.42 1696.24,1336.62 1697.25,1342.87 \n",
       "  1698.25,1353.74 1699.26,1349.34 1700.27,1354.3 1701.28,1338.05 1702.29,1352.48 1703.3,1343.81 1704.31,1362.36 1705.31,1366.48 1706.32,1347.73 1707.33,1337.29 \n",
       "  1708.34,1318.63 1709.35,1353.34 1710.36,1337.29 1711.36,1361.3 1712.37,1343.78 1713.38,1348.54 1714.39,1347.84 1715.4,1348.86 1716.41,1368.31 1717.42,1363.91 \n",
       "  1718.42,1353.88 1719.43,1358.67 1720.44,1372.44 1721.45,1353.95 1722.46,1345.8 1723.47,1350.59 1724.48,1371.54 1725.48,1365.46 1726.49,1343.24 1727.5,1338.78 \n",
       "  1728.51,1347.33 1729.52,1352.82 1730.53,1364.48 1731.54,1351.67 1732.54,1368.08 1733.55,1365.24 1734.56,1342.02 1735.57,1338.91 1736.58,1360.83 1737.59,1353.93 \n",
       "  1738.6,1355.2 1739.6,1358.86 1740.61,1340.65 1741.62,1361.57 1742.63,1359.98 1743.64,1359.69 1744.65,1357.32 1745.65,1342.85 1746.66,1363.26 1747.67,1357.61 \n",
       "  1748.68,1348.49 1749.69,1336.53 1750.7,1357.3 1751.71,1347.65 1752.71,1357.23 1753.72,1349.34 1754.73,1361.19 1755.74,1335.32 1756.75,1339.18 1757.76,1359.78 \n",
       "  1758.77,1362.36 1759.77,1369.75 1760.78,1351.08 1761.79,1359.98 1762.8,1355.98 1763.81,1367.96 1764.82,1362.89 1765.83,1352.67 1766.83,1328.4 1767.84,1365.73 \n",
       "  1768.85,1341.27 1769.86,1347.98 1770.87,1348.77 1771.88,1356.62 1772.88,1353.16 1773.89,1365.34 1774.9,1358.61 1775.91,1352.44 1776.92,1367.37 1777.93,1352.27 \n",
       "  1778.94,1352.41 1779.94,1343.49 1780.95,1374.13 1781.96,1378.77 1782.97,1375.86 1783.98,1358.9 1784.99,1351.45 1786,1355.08 1787,1372.64 1788.01,1362.11 \n",
       "  1789.02,1356.03 1790.03,1361.83 1791.04,1351.43 1792.05,1356.39 1793.06,1352.23 1794.06,1368.87 1795.07,1365.32 1796.08,1361.29 1797.09,1326.74 1798.1,1352.31 \n",
       "  1799.11,1368.73 1800.12,1361.67 1801.12,1354.22 1802.13,1353.04 1803.14,1356.37 1804.15,1369.51 1805.16,1345.27 1806.17,1353.42 1807.17,1368.4 1808.18,1340.71 \n",
       "  1809.19,1351.2 1810.2,1361.49 1811.21,1355.83 1812.22,1365.49 1813.23,1346.36 1814.23,1364.23 1815.24,1344.53 1816.25,1358.95 1817.26,1354.55 1818.27,1348.78 \n",
       "  1819.28,1345 1820.29,1373.27 1821.29,1350.16 1822.3,1361.84 1823.31,1363.74 1824.32,1370.94 1825.33,1352.43 1826.34,1354.81 1827.35,1343.79 1828.35,1353.43 \n",
       "  1829.36,1367.7 1830.37,1370.24 1831.38,1329.2 1832.39,1382.63 1833.4,1363.89 1834.4,1344.98 1835.41,1378.56 1836.42,1358.04 1837.43,1346.02 1838.44,1338.91 \n",
       "  1839.45,1355.82 1840.46,1338.39 1841.46,1368.78 1842.47,1372.5 1843.48,1352.68 1844.49,1360.55 1845.5,1358.71 1846.51,1351.58 1847.52,1348.45 1848.52,1349.75 \n",
       "  1849.53,1355.07 1850.54,1351.41 1851.55,1366.26 1852.56,1342.6 1853.57,1338.76 1854.58,1351.48 1855.58,1351.3 1856.59,1372.06 1857.6,1365.75 1858.61,1374.66 \n",
       "  1859.62,1356.8 1860.63,1339.1 1861.63,1360.28 1862.64,1360.14 1863.65,1355.12 1864.66,1350.62 1865.67,1367 1866.68,1331.36 1867.69,1364.31 1868.69,1358.42 \n",
       "  1869.7,1353.64 1870.71,1347.68 1871.72,1349.7 1872.73,1359.45 1873.74,1361.21 1874.75,1347.59 1875.75,1368.12 1876.76,1366.37 1877.77,1353.24 1878.78,1346.04 \n",
       "  1879.79,1337.18 1880.8,1354.85 1881.81,1354.3 1882.81,1351.73 1883.82,1347.75 1884.83,1358.5 1885.84,1350.73 1886.85,1356.92 1887.86,1362.43 1888.87,1345.33 \n",
       "  1889.87,1340.98 1890.88,1356.7 1891.89,1364.84 1892.9,1347.94 1893.91,1331.04 1894.92,1341.76 1895.92,1366.68 1896.93,1370.52 1897.94,1359.76 1898.95,1371.68 \n",
       "  1899.96,1347.07 1900.97,1370.51 1901.98,1355.82 1902.98,1362.64 1903.99,1360.02 1905,1347.69 1906.01,1345.07 1907.02,1355.05 1908.03,1351.74 1909.04,1343.08 \n",
       "  1910.04,1352.49 1911.05,1376.36 1912.06,1363.7 1913.07,1344.03 1914.08,1343.16 1915.09,1368.26 1916.1,1360.88 1917.1,1365.07 1918.11,1359.55 1919.12,1328.59 \n",
       "  1920.13,1351.17 1921.14,1352.21 1922.15,1361.01 1923.15,1358.4 1924.16,1359.18 1925.17,1340.28 1926.18,1343.14 1927.19,1360.96 1928.2,1356.7 1929.21,1366 \n",
       "  1930.21,1344 1931.22,1360.7 1932.23,1366.52 1933.24,1338.88 1934.25,1357.48 1935.26,1342.3 1936.27,1349.65 1937.27,1337.61 1938.28,1356.12 1939.29,1339.11 \n",
       "  1940.3,1357.9 1941.31,1357.29 1942.32,1354.55 1943.33,1373.33 1944.33,1366.43 1945.34,1372.79 1946.35,1359.14 1947.36,1348.63 1948.37,1364.52 1949.38,1365.61 \n",
       "  1950.39,1356.8 1951.39,1352.25 1952.4,1343.67 1953.41,1358.87 1954.42,1360.75 1955.43,1375.19 1956.44,1341.63 1957.44,1354.79 1958.45,1354.59 1959.46,1343.83 \n",
       "  1960.47,1363.02 1961.48,1348.75 1962.49,1353.43 1963.5,1353.04 1964.5,1350.66 1965.51,1367.48 1966.52,1350.55 1967.53,1348.71 1968.54,1345.56 1969.55,1373.6 \n",
       "  1970.56,1350.39 1971.56,1352.48 1972.57,1362.63 1973.58,1351.86 1974.59,1372.79 1975.6,1382.36 1976.61,1362.61 1977.62,1334.23 1978.62,1356.76 1979.63,1358.02 \n",
       "  1980.64,1368.81 1981.65,1360.31 1982.66,1364.09 1983.67,1355.1 1984.67,1354.11 1985.68,1349.51 1986.69,1350.6 1987.7,1359.08 1988.71,1356.17 1989.72,1350.67 \n",
       "  1990.73,1364.26 1991.73,1331.92 1992.74,1369.36 1993.75,1347.29 1994.76,1354.79 1995.77,1344.69 1996.78,1343.45 1997.79,1359.69 1998.79,1327.58 1999.8,1348.68 \n",
       "  2000.81,1355.42 2001.82,1349.84 2002.83,1368.17 2003.84,1343.29 2004.85,1354.33 2005.85,1352.12 2006.86,1364.03 2007.87,1336.25 2008.88,1342.69 2009.89,1368.94 \n",
       "  2010.9,1371.52 2011.91,1350.84 2012.91,1353.44 2013.92,1359.84 2014.93,1344.81 2015.94,1346.93 2016.95,1362.46 2017.96,1338.93 2018.96,1351.24 2019.97,1359.16 \n",
       "  2020.98,1361.59 2021.99,1354.09 2023,1339.44 2024.01,1341.67 2025.02,1354.14 2026.02,1374.52 2027.03,1372.09 2028.04,1342.72 2029.05,1351.83 2030.06,1352.5 \n",
       "  2031.07,1369.61 2032.08,1359.01 2033.08,1357.02 2034.09,1352.72 2035.1,1378.45 2036.11,1360.73 2037.12,1363.61 2038.13,1360.34 2039.14,1367.95 2040.14,1359.41 \n",
       "  2041.15,1367.45 2042.16,1350.3 2043.17,1354.22 2044.18,1343.68 2045.19,1362.51 2046.19,1371.28 2047.2,1355.41 2048.21,1336.48 2049.22,1353.02 2050.23,1358.53 \n",
       "  2051.24,1362.24 2052.25,1362.87 2053.25,1358.51 2054.26,1355.81 2055.27,1339.08 2056.28,1345.68 2057.29,1336.37 2058.3,1325.87 2059.31,1350.38 2060.31,1329.64 \n",
       "  2061.32,1367.86 2062.33,1349.43 2063.34,1360.22 2064.35,1364 2065.36,1346.56 2066.37,1350.72 2067.37,1339.15 2068.38,1348.31 2069.39,1331.05 2070.4,1344.76 \n",
       "  2071.41,1357.33 2072.42,1374.37 2073.43,1359.36 2074.43,1345.8 2075.44,1339.82 2076.45,1355.97 2077.46,1349.26 2078.47,1362.06 2079.48,1343.52 2080.48,1361.67 \n",
       "  2081.49,1358.2 2082.5,1355.81 2083.51,1347.83 2084.52,1346.17 2085.53,1333.2 2086.54,1358.51 2087.54,1354.9 2088.55,1347.79 2089.56,1343.19 2090.57,1348.6 \n",
       "  2091.58,1360.6 2092.59,1355.21 2093.6,1363.67 2094.6,1350.28 2095.61,1354.85 2096.62,1363.12 2097.63,1352.81 2098.64,1353.36 2099.65,1358.31 2100.66,1375.26 \n",
       "  2101.66,1343.67 2102.67,1351.67 2103.68,1368.69 2104.69,1350.87 2105.7,1344.49 2106.71,1354.27 2107.71,1339.77 2108.72,1361.15 2109.73,1363.01 2110.74,1329.13 \n",
       "  2111.75,1340.52 2112.76,1355.44 2113.77,1369.58 2114.77,1352.31 2115.78,1385.89 2116.79,1341.29 2117.8,1354.43 2118.81,1356.23 2119.82,1352.96 2120.83,1345.46 \n",
       "  2121.83,1359.52 2122.84,1347.63 2123.85,1358.38 2124.86,1345.92 2125.87,1359.37 2126.88,1379.81 2127.89,1343.92 2128.89,1357.5 2129.9,1348.12 2130.91,1354.85 \n",
       "  2131.92,1358.64 2132.93,1354.66 2133.94,1368.21 2134.94,1360.3 2135.95,1357.98 2136.96,1359.33 2137.97,1335.95 2138.98,1359.71 2139.99,1340.01 2141,1368.45 \n",
       "  2142,1331.34 2143.01,1354.91 2144.02,1362.69 2145.03,1370.13 2146.04,1352.97 2147.05,1352.46 2148.06,1354.43 2149.06,1342.54 2150.07,1363.79 2151.08,1346.45 \n",
       "  2152.09,1334.9 2153.1,1350.22 2154.11,1353.55 2155.12,1343.65 2156.12,1376.94 2157.13,1357.08 2158.14,1346.7 2159.15,1362.65 2160.16,1349.31 2161.17,1347.3 \n",
       "  2162.18,1350.5 2163.18,1356.07 2164.19,1357.6 2165.2,1360.1 2166.21,1335.71 2167.22,1367.29 2168.23,1368.27 2169.23,1351.31 2170.24,1342.18 2171.25,1362.61 \n",
       "  2172.26,1328.9 2173.27,1364.11 2174.28,1346.85 2175.29,1355.31 2176.29,1355.95 2177.3,1332.3 2178.31,1348.08 2179.32,1354.64 2180.33,1358.76 2181.34,1357.15 \n",
       "  2182.35,1340.36 2183.35,1357.83 2184.36,1360.96 2185.37,1357.51 2186.38,1363.03 2187.39,1368.94 2188.4,1355.91 2189.41,1359.36 2190.41,1359.6 2191.42,1353.88 \n",
       "  2192.43,1363.69 2193.44,1387.01 2194.45,1371.66 2195.46,1355.09 2196.46,1362.5 2197.47,1354.92 2198.48,1347.9 2199.49,1352.79 2200.5,1353.31 2201.51,1358.86 \n",
       "  2202.52,1356.59 2203.52,1354.67 2204.53,1370.94 2205.54,1341.71 2206.55,1333.44 2207.56,1341.06 2208.57,1345.61 2209.58,1356.31 2210.58,1346.74 2211.59,1337.18 \n",
       "  2212.6,1384.96 2213.61,1368.81 2214.62,1378.09 2215.63,1357.48 2216.64,1332.36 2217.64,1360.33 2218.65,1354.82 2219.66,1364.91 2220.67,1361.71 2221.68,1350.23 \n",
       "  2222.69,1340.1 2223.7,1367.92 2224.7,1339.37 2225.71,1341.07 2226.72,1354.98 2227.73,1348.68 2228.74,1352.03 2229.75,1354.4 2230.75,1327.02 2231.76,1348.19 \n",
       "  2232.77,1336.28 2233.78,1361.43 2234.79,1322.47 2235.8,1339.77 2236.81,1360.31 2237.81,1331.74 2238.82,1365.37 2239.83,1365.44 2240.84,1369.94 2241.85,1359.66 \n",
       "  2242.86,1351.92 2243.87,1364.12 2244.87,1384.28 2245.88,1370.97 2246.89,1359.84 2247.9,1352.95 2248.91,1364.72 2249.92,1353.38 2250.93,1341.43 2251.93,1371.34 \n",
       "  2252.94,1352.53 2253.95,1371.5 2254.96,1343.43 2255.97,1370.4 2256.98,1367.11 2257.98,1342.34 2258.99,1353.99 2260,1362.35 2261.01,1359.96 2262.02,1370.26 \n",
       "  2263.03,1373.19 2264.04,1356.64 2265.04,1368.32 2266.05,1355.52 2267.06,1362.95 2268.07,1363.46 2269.08,1331.83 2270.09,1353.84 2271.1,1335.93 2272.1,1369.58 \n",
       "  2273.11,1372.93 2274.12,1352.5 2275.13,1365.07 2276.14,1343.2 2277.15,1350.92 2278.16,1355.33 2279.16,1361.73 2280.17,1358.67 2281.18,1353.33 2282.19,1351.91 \n",
       "  2283.2,1351.92 2284.21,1348.95 2285.22,1326.73 2286.22,1350.32 2287.23,1370.12 2288.24,1354.86 2289.25,1367.4 2290.26,1363.61 2291.27,1362.29 2292.27,1347.66 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip0100)\" d=\"\n",
       "M1989.93 326.155 L2280.76 326.155 L2280.76 205.195 L1989.93 205.195  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,326.155 2280.76,326.155 2280.76,205.195 1989.93,205.195 1989.93,326.155 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,265.675 2157.93,265.675 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 283.175)\" x=\"2181.93\" y=\"283.175\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_error_plot(3, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.263035, 1.0598], Float32[0.591907, 0.670959])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo: should we use those learned weights as initializers? they approximate the normal distribution.\n",
    "\n",
    "function normal_approximators(N)\n",
    "    vs = gpu(even_us(N))\n",
    "    βs = gpu(ones(Float32, N))\n",
    "\n",
    "    opt = ADAM()\n",
    "    ps = Params([vs, βs])\n",
    "\n",
    "    for _ in 1:1000\n",
    "        # note: in loop!\n",
    "        A = gpu(randn(100, 100))\n",
    "        _, adjoint = pullback(ps) do\n",
    "            Ã = binarize_activations(A, vs, βs)\n",
    "            mean((A .- Ã).^2)\n",
    "        end\n",
    "        gs = adjoint(1.0)\n",
    "\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "    end\n",
    "    \n",
    "    vs, βs\n",
    "end\n",
    "\n",
    "normal_approximators(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.263317, 1.05975], Float32[0.592174, 0.670152])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_approximators(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.535667, 0.420052, 1.40779], Float32[0.513562, 0.496141, 0.562595])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_approximators(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinWeights"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct BinWeights{U <: AbstractVector, A <: AbstractVector}\n",
    "    us :: U\n",
    "    αs :: A\n",
    "    active :: Bool\n",
    "end\n",
    "\n",
    "function BinWeights(W, us :: U; active=false) where U\n",
    "    # note: W is not stored! it's just used to initialize alphas.\n",
    "    _, αs = binarize_weights(W, us)\n",
    "    BinWeights(us, αs, active)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: no args, because the parameters aren't trainable, and shouldn't be moved to GPU.\n",
    "Flux.@functor BinWeights ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (bw :: BinWeights{U, A})(W) where {U, A}\n",
    "    if bw.active\n",
    "        if Flux.istraining()\n",
    "            W̃, αs = binarize_weights(W, bw.us)\n",
    "            bw.αs = αs\n",
    "            W̃\n",
    "        else\n",
    "            # todo: cache W̃?\n",
    "            W̃ = binarize_weights(W, bw.us, bw.αs)\n",
    "            W̃\n",
    "        end\n",
    "    else\n",
    "        W\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinWeights{Array{Float32,1},Array{Float32,1}}(Float32[-1.0, -0.5, 0.0, 0.5, 1.0], Float32[0.36989, 0.257701, 0.2352, 0.22804, 0.387453], true)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = BinWeights(W, even_us(5), active=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test q(W) == binarize_weights(W, even_us(5))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test gpu(q) === q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct BinActs{V, B}\n",
    "    vs :: V\n",
    "    βs :: B\n",
    "    active :: Bool\n",
    "end\n",
    "\n",
    "function BinActs(vs :: V, βs :: B; active=false) where {V, B}\n",
    "    # note: W is not stored! it's just used to initialize alphas.\n",
    "    BinActs(vs, βs, active)\n",
    "end\n",
    "\n",
    "function (ba :: BinActs{V, B})(A) where {V, B}\n",
    "    if ba.active\n",
    "        binarize_activations(A, ba.vs, ba.βs)\n",
    "    else\n",
    "        A\n",
    "    end\n",
    "end\n",
    "\n",
    "Flux.@functor BinActs (vs, βs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABCConv"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://github.com/FluxML/Flux.jl/blob/fb4a48f970ba40d0022a7488b48d19cd563867c4/src/layers/conv.jl\n",
    "# note: does not include activation, that should go before (?)\n",
    "\n",
    "\"\"\"\n",
    "Standard convolutional layer with ABC-based quantization.\n",
    "\"\"\"\n",
    "struct ABCConv{W,Z, S,P, U,A, V,B} # that's a lotta parameters!!\n",
    "    weight::W\n",
    "    bias::Z\n",
    "    \n",
    "    stride::NTuple{S,Int}\n",
    "    pad::NTuple{P,Int}\n",
    "    dilation::NTuple{S,Int}\n",
    "    \n",
    "    bin_weights :: BinWeights{U, A}\n",
    "    bin_acts :: BinActs{V, B}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ABCConv(weight::AbstractArray{T,K}, bias::AbstractVector{T},\n",
    "                 us::AbstractVector{T}, vs::AbstractVector{T}, βs::AbstractVector{T};\n",
    "              stride = 1, pad = 0, dilation = 1, bin_active=false) where {T, K}\n",
    "    @assert size(vs) == size(βs)\n",
    "    \n",
    "    stride = expand(Val(K-2), stride)\n",
    "    pad = expand(Val(2*(K-2)), pad)\n",
    "    dilation = expand(Val(K-2), dilation)\n",
    "    \n",
    "    bin_weights = BinWeights(weight, us, active=bin_active) # note: weights is used to initialize αs, not stored\n",
    "    bin_acts = BinActs(vs, βs, active=bin_active)\n",
    "    \n",
    "    ABCConv(weight, bias, stride, pad, dilation, bin_weights, bin_acts)\n",
    "end\n",
    "\n",
    "expand(N, i::Tuple) = i\n",
    "expand(N, i::Integer) = ntuple(_ -> i, N)\n",
    "\n",
    "function ABCConv(k::NTuple{D,Integer}, ch::Pair{<:Integer,<:Integer}, N::Integer, M::Integer;\n",
    "    weight_init = Flux.glorot_uniform, bias_init=k -> zeros(Float32, k), \n",
    "    us_init=even_us, vs_init=even_us, βs_init=k -> ones(Float32, k), \n",
    "    stride = 1, pad = 0, dilation = 1,\n",
    "    bin_active=false) where D\n",
    "        \n",
    "    ABCConv(weight_init(k..., ch...), bias_init(ch[2]),\n",
    "           us_init(M), vs_init(N), βs_init(N),\n",
    "           stride = stride, pad = pad, dilation = dilation, bin_active=bin_active)\n",
    "end\n",
    "\n",
    "Flux.@functor ABCConv\n",
    "\n",
    "function (c::ABCConv)(A::AbstractArray)\n",
    "    b = reshape(c.bias, map(_->1, c.stride)..., :, 1)\n",
    "    \n",
    "    W = c.weight\n",
    "    W̃ = c.bin_weights(W)\n",
    "    \n",
    "    Ã = c.bin_acts(A)\n",
    "    \n",
    "    cdims = DenseConvDims(Ã, W̃; stride=c.stride, padding=c.pad, dilation=c.dilation)\n",
    "    #println(\"W̃: \", typeof(W̃), \" \", size(W̃), \" Ã: \", typeof(Ã), \" \", size(Ã), \" b: \", typeof(b), \" \", size(b), \" dims: \", cdims)\n",
    "    \n",
    "    conv(Ã, W̃, cdims) .+ b\n",
    "end\n",
    "\n",
    "# Base.show(io :: IO, ::Type{ABCConv}) = print(io, \"ABCConv\")\n",
    "function Base.show(io::IO, l::ABCConv)\n",
    "    print(io, \"ABCConv(\", size(l.weight)[1:ndims(l.weight)-2])\n",
    "    print(io, \", \", size(l.weight, ndims(l.weight)-1), \"=>\", size(l.weight, ndims(l.weight)))\n",
    "    print(io, \", \", size(l.bin_acts.vs), \", \", size(l.bin_weights.us))\n",
    "    print(io, \", active=\", l.bin_acts.active)\n",
    "\n",
    "    print(io, \")\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (c::Conv)(A::AbstractArray)\n",
    "\n",
    "  σ, b = c.σ, reshape(c.bias, map(_->1, c.stride)..., :, 1)\n",
    "  W = c.weight\n",
    "  cdims = DenseConvDims(A, W; stride=c.stride, padding=c.pad, dilation=c.dilation)\n",
    "\n",
    "  r = σ.(conv(A, W, cdims) .+ b)\n",
    "\n",
    "  r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv((3, 3), 10=>10)(randn(Float32, 5, 5, 10, 1))\n",
    "ABCConv((3, 3), 10=>10, 1, 1, bin_active=true)(randn(Float32, 5, 5, 10, 1))\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu(Conv((3, 3), 10=>10))(gpu(randn(Float32, 5, 5, 10, 1)))\n",
    "gpu(ABCConv((3, 3), 10=>10, 1, 1, bin_active=true))(gpu(randn(Float32, 5, 5, 10, 1)))\n",
    "\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABCConv((3, 3), 10=>10, (1,), (1,), active=false)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = ABCConv((3,3), 10 => 10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarize (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function binarize(c :: ABCConv; active=true)\n",
    "    c.bin_acts.active = active\n",
    "    c.bin_weights.active = active\n",
    "    ()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CuArrays, CUDAnative, BenchmarkTools, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n",
      "└ @ GPUArrays /data/scratch/jhgilles/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UInt32[0xfffffffe]\n",
      "UInt32[0x0000000e]\n"
     ]
    }
   ],
   "source": [
    "# which order does ballot go in?\n",
    "# indexed from LSB to MSB\n",
    "# inactive threads get 0\n",
    "\n",
    "function gpu_test_ballot!(o, b)\n",
    "    index = threadIdx().x\n",
    "    ballot = vote_ballot(b[index])\n",
    "    if (index - 1) % 32 == 0\n",
    "        o[1] = ballot\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "b = CuArrays.fill(true, 32)\n",
    "CuArrays.allowscalar(true)\n",
    "b[1] = false\n",
    "CuArrays.allowscalar(false)\n",
    "o = CuArrays.fill(UInt32(0), 1)\n",
    "@cuda threads=32 gpu_test_ballot!(o, b)\n",
    "println(o)\n",
    "@cuda threads=4 gpu_test_ballot!(o, b)\n",
    "println(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n",
      "└ @ GPUArrays /data/scratch/jhgilles/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UInt64[0xfffffffffffffffe]\n",
      "UInt64[0x0000000f0000000e]\n"
     ]
    }
   ],
   "source": [
    "# which order does ballot go in?\n",
    "# indexed from LSB to MSB\n",
    "# inactive threads get 0\n",
    "\n",
    "function gpu_test_ballot_64!(o, b)\n",
    "    #@inbounds begin\n",
    "        index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n",
    "        ballot_low = vote_ballot(b[index])\n",
    "        ballot_high = vote_ballot(b[index + 32])\n",
    "        if (index - 1) % 32 == 0\n",
    "            o[1] = UInt64(ballot_low) | (UInt64(ballot_high) << 32)\n",
    "        end\n",
    "    #end\n",
    "    return\n",
    "end\n",
    "\n",
    "b = CuArrays.fill(true, 64)\n",
    "CuArrays.allowscalar(true)\n",
    "b[1] = false\n",
    "CuArrays.allowscalar(false)\n",
    "o = CuArrays.fill(UInt64(0), 1)\n",
    "CuArrays.@sync @cuda threads=32 gpu_test_ballot_64!(o, b)\n",
    "println(o)\n",
    "CuArrays.@sync @cuda threads=4 gpu_test_ballot_64!(o, b)\n",
    "println(o)\n",
    "\n",
    "# hm, uneven things on the end there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: how does julia's multi-dimensional indexing computation work? / array-walking helpers\n",
    "# todo: helper functions for computing sizes of output arrays (ceil to multiples of 64)\n",
    "# todo: 32 and 64 bit versions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simplequant (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplequant(x) = x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layout:\n",
    "\n",
    "# todo: optimize this\n",
    "# todo: support more than 1024 channels\n",
    "\n",
    "# thread x: C index\n",
    "\n",
    "# block x: W\n",
    "# block y: H\n",
    "# block z: N\n",
    "\n",
    "# note: P is packed C\n",
    "\n",
    "function quant_pack_WHCN_PWHN_32!(quantized, input, f)\n",
    "    tidx = threadIdx()\n",
    "    bidx = blockIdx()\n",
    "    \n",
    "    C = tidx.x\n",
    "    \n",
    "    W = bidx.x\n",
    "    H = bidx.y\n",
    "    N = bidx.z\n",
    "    \n",
    "    P = div(C - 1, 32) + 1\n",
    "    @inbounds begin\n",
    "        ballot = vote_ballot(f(input[W,H,C,N]))\n",
    "        if (C - 1) % 32 == 0\n",
    "            #@cuprintf(\"WHCN %ld %ld %ld %ld; P=%ld\\n\", W, H, C, N, P)\n",
    "            quantized[P,W,H,N] = ballot\n",
    "            #@cuprintf(\"DONE: WHCN %ld %ld %ld %ld; P=%ld\\n\", W, H, C, N, P)\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "(W,H,C,N) = (10, 10, 128, 128)\n",
    "P = ceil(Int64, C/32)\n",
    "\n",
    "input_WHCN = CuArray(randn(Float32, W, H, C, N))\n",
    "quantized_PWHN = CuArray(zeros(UInt32, P, W, H, N))\n",
    "\n",
    "CuArrays.@sync @cuda threads=C blocks=(W, H, N) quant_pack_WHCN_PWHN_32!(quantized_PWHN, input_WHCN, simplequant)\n",
    "\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  115.657 μs (57 allocations: 2.63 KiB)\n"
     ]
    }
   ],
   "source": [
    "## (W,H,C,N) = (10, 10, 128, 128)\n",
    "# @btime CuArrays.@sync @cuda threads=C blocks=(W, H, N) quant_pack_WHCN_PWHN_32!(quantized_PWHN, input_WHCN, (x) -> x > 0)\n",
    "## pre-inbounds: 1ms\n",
    "## inbounds: 115.657 μs (57 allocations: 2.63 KiB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function unpack_PWHN_WHCN_32!(unquantized, quantized)\n",
    "    tidx = threadIdx()\n",
    "    bidx = blockIdx()\n",
    "    \n",
    "    C = tidx.x\n",
    "    \n",
    "    W = bidx.x\n",
    "    H = bidx.y\n",
    "    N = bidx.z\n",
    "    \n",
    "    P = div(C - 1, 32) + 1\n",
    "    \n",
    "    #@inbounds begin\n",
    "        q = quantized[P,W,H,N]\n",
    "        unquantized[W,H,C,N] = ((q >> rem(C - 1, 32)) & 1) == 1\n",
    "    #end\n",
    "    return\n",
    "end\n",
    "unquantized_WHCN = CuArray(zeros(Bool, W, H, C, N))\n",
    "CuArrays.@sync @cuda threads=C blocks=(W, H, N) unpack_PWHN_WHCN_32!(unquantized_WHCN, quantized_PWHN)\n",
    "\n",
    "@test simplequant.(input_WHCN) == unquantized_WHCN\n",
    "\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function quant_pack_CWHN_PWHN_32!(quantized, input, f)\n",
    "    tidx = threadIdx()\n",
    "    bidx = blockIdx()\n",
    "    \n",
    "    C = tidx.x\n",
    "    \n",
    "    W = bidx.x\n",
    "    H = bidx.y\n",
    "    N = bidx.z\n",
    "    \n",
    "    P = div(C - 1, 32) + 1\n",
    "    \n",
    "    @inbounds begin\n",
    "        ballot = vote_ballot(f(input[C,W,H,N]))\n",
    "        if (C - 1) % 32 == 0\n",
    "            quantized[P,W,H,N] = ballot\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end\n",
    "(W,H,C,N) = (10, 10, 128, 128)\n",
    "\n",
    "P = ceil(Int64, C/32)\n",
    "\n",
    "input_CWHN = CuArray(randn(Float32, C, W, H, N))\n",
    "quantized_PWHN = CuArray(zeros(UInt32, P, W, H, N))\n",
    "\n",
    "CuArrays.@sync @cuda threads=C blocks=(W, H, N) quant_pack_CWHN_PWHN_32!(quantized_PWHN, input_CWHN, simplequant)\n",
    "\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  110.349 μs (57 allocations: 2.63 KiB)\n"
     ]
    }
   ],
   "source": [
    "# @btime CuArrays.@sync @cuda threads=C blocks=(W, H, N) quant_pack_CWHN_PWHN_32!(quantized_PWHN, input_CWHN, simplequant)\n",
    "## inbounds: 110.657 μs (57 allocations: 2.63 KiB)\n",
    "## not any faster than WHCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data set\n",
      "└ @ Main In[48]:10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on Flux MNIST example\n",
    "\n",
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "\n",
    "dataset = Flux.Data.FashionMNIST\n",
    "\n",
    "@info(\"Loading data set\")\n",
    "train_labels = dataset.labels()\n",
    "train_imgs = dataset.images()\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAPASURBVGje7dm9ix1VGMfxz71792aTlexGRTERFHwNRMWXwhQiWIiNhVhYaGchFloF/AtSiZWVjaCthZYKlhIkhSkUhYiNkRVfgom7mn25d67Fcw5z7t0ht3LnMuyBYWbOnDnPPPPl9zzPmelpaD1MZvpO4318gkvYxR7O4CX8hHdxzc1b3wG37hvszZ6U7B7HK3gZY9yCFdw2M8llVHgIv+ELvIdvF8HD7hvsNXUex8d4ND3RFm4I3VUYYA3/pPOS+wqOYoiv8FrbHnbfYCPDL3EPrqqZjYrBfcGzX5zPTjrBXXgBP7TpYfcNDmY7nhT8/kwXl4SuTuKYmt9AxNcelgXjTfySjqXrr+Ncmx523+A+HZ7D24JhJRiO8QE2BKOT+FXNcyhy5RN4S83/eNrf26aH3Te4j+HXuENoalewuY6n8Tzuxod4A98JjS7hd1Gv/pjuXRF6fFjUrpfb8rD7BvfF0sdwRXA5kvqOp/3noo45LfT6KV5Mk3wj4vAIq0K7FX7GWYcM/8c2xfAR/CE49NW58Gq6fgY7olY5L0S8l/Zn05gNnBL8KmzjGXzUlofdNzjF8B3BbEvo6KhgMMJTYl14q6hh7hT8tkU+XBdryRNiHbIm2A7Tva152H2DUwwvCDb3i/i5KvLbWOTJrK2x0OhAcBqnJ98UMXM1Xe8LXX7WpofdN9i4PjyBB/AmnhX5cU18R1sWfGYn6QtNronvM68uiofdNzho6vwLF0Xue06s14dqfVVpXC9tlahDd9P+wiJ52H2D+xjmNfuuYLep5jYpxkwaJsv6vFacz36L6/4rbZ/hRNQqxL+I62nQTnG9ZJiD8Y5gD38X3ozb9rD7BhtjaX73N4Qej4jaNNcwmWPOgxPB8Ji6xlkYD7tvsJFh1liuQSdp6xf9OW6WHKt0XM3M06qH3Tc4mDfglKhxltQcm4rZ/O20Z3/d2qqH3Td4Ux1Sx8Wh+j9F3nJMrUQu3El9yw3ztOZh9w3O1eG20NXIdJ25pK5dB2n/b7pnfZE87L7BuQyr4risR8uYWtaqI/GNjkMdHlCby7B8otl1Xskw1zUjUZ8ujIfdNzg3HzJdo2TNlf25L69JDmuaA22NDMuYuWtaV+W//dl1RF4blmxb97D7BufG0vxUuSbtq+NoqckmfS6Eh903ODeWbuBBkefyv6fl4ngi+A6Kew91eKBtrg7Xxf+KAW5X63BYjMnrjisi7t5XeFPNzNf9V7oYDMt8eAnfq/8d5qfcUsfPrNE9wfxiGlc1zN39V3rgBv8DhyffYhCW4SwAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                                  \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(train_imgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bundle images together with labels and group into minibatchess\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "\n",
    "batch_size = 128\n",
    "mb_idxs = partition(1:length(train_imgs), batch_size)\n",
    "train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]\n",
    "\n",
    "test_imgs = dataset.images(:test)\n",
    "test_labels = dataset.labels(:test)\n",
    "mb_idxs = partition(1:length(test_imgs), batch_size)\n",
    "test_set = [make_minibatch(test_imgs, test_labels, i) for i in mb_idxs]\n",
    "\n",
    "train_set = gpu.(train_set)\n",
    "test_set = gpu.(test_set)\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAPASURBVGje7dm9ix1VGMfxz71792aTlexGRTERFHwNRMWXwhQiWIiNhVhYaGchFloF/AtSiZWVjaCthZYKlhIkhSkUhYiNkRVfgom7mn25d67Fcw5z7t0ht3LnMuyBYWbOnDnPPPPl9zzPmelpaD1MZvpO4318gkvYxR7O4CX8hHdxzc1b3wG37hvszZ6U7B7HK3gZY9yCFdw2M8llVHgIv+ELvIdvF8HD7hvsNXUex8d4ND3RFm4I3VUYYA3/pPOS+wqOYoiv8FrbHnbfYCPDL3EPrqqZjYrBfcGzX5zPTjrBXXgBP7TpYfcNDmY7nhT8/kwXl4SuTuKYmt9AxNcelgXjTfySjqXrr+Ncmx523+A+HZ7D24JhJRiO8QE2BKOT+FXNcyhy5RN4S83/eNrf26aH3Te4j+HXuENoalewuY6n8Tzuxod4A98JjS7hd1Gv/pjuXRF6fFjUrpfb8rD7BvfF0sdwRXA5kvqOp/3noo45LfT6KV5Mk3wj4vAIq0K7FX7GWYcM/8c2xfAR/CE49NW58Gq6fgY7olY5L0S8l/Zn05gNnBL8KmzjGXzUlofdNzjF8B3BbEvo6KhgMMJTYl14q6hh7hT8tkU+XBdryRNiHbIm2A7Tva152H2DUwwvCDb3i/i5KvLbWOTJrK2x0OhAcBqnJ98UMXM1Xe8LXX7WpofdN9i4PjyBB/AmnhX5cU18R1sWfGYn6QtNronvM68uiofdNzho6vwLF0Xue06s14dqfVVpXC9tlahDd9P+wiJ52H2D+xjmNfuuYLep5jYpxkwaJsv6vFacz36L6/4rbZ/hRNQqxL+I62nQTnG9ZJiD8Y5gD38X3ozb9rD7BhtjaX73N4Qej4jaNNcwmWPOgxPB8Ji6xlkYD7tvsJFh1liuQSdp6xf9OW6WHKt0XM3M06qH3Tc4mDfglKhxltQcm4rZ/O20Z3/d2qqH3Td4Ux1Sx8Wh+j9F3nJMrUQu3El9yw3ztOZh9w3O1eG20NXIdJ25pK5dB2n/b7pnfZE87L7BuQyr4risR8uYWtaqI/GNjkMdHlCby7B8otl1Xskw1zUjUZ8ujIfdNzg3HzJdo2TNlf25L69JDmuaA22NDMuYuWtaV+W//dl1RF4blmxb97D7BufG0vxUuSbtq+NoqckmfS6Eh903ODeWbuBBkefyv6fl4ngi+A6Kew91eKBtrg7Xxf+KAW5X63BYjMnrjisi7t5XeFPNzNf9V7oYDMt8eAnfq/8d5qfcUsfPrNE9wfxiGlc1zN39V3rgBv8DhyffYhCW4SwAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{Float32},2} with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(cpu(train_set[1])[1][:,:,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAAeAQAAAAANp/SVAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QAAd2KE6QAAADYSURBVDjLvZNLDoQgDEBrWLDDC5B4DXZeibmAoxfwTO68BslcwOUsSGZoixUTt9AgwdLX9AcMK6gZLOCeVpKQvm77HHBJt13n+gBLyCpXGO1qdoE0wzyiL98K6PZs4SlWXN/0Z4U0heP31gLwWWtB00kPByisqbOM+ZwYF70FkDEX+ns7U+jTWfSHftcFYn+22UhK40PcBmIrQC4iaAqXd5lIzWlEHETdBHDlM7CcicjrHE3DpGsCgAQqr9bCXSaw9EhKqQdgBb2WK7XevRj4LVRcssHqVwf+0XiPcf1Ksa8AAAAASUVORK5CYII=",
      "text/plain": [
       "10×128 Array{Gray{Bool},2} with eltype Gray{Bool}:\n",
       " Gray{Bool}(false)  Gray{Bool}(true)   …  Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)  …  Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(true) \n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(false)     Gray{Bool}(false)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(cpu(train_set[1])[2][:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/FluxML/Flux.jl/blob/bdeb9c6d584668c7cef1ce71caf659d611c86d65/src/optimise/train.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train!"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    train!(loss, params, data, opt; cb)\n",
    "For each datapoint `d` in `data` computes the gradient of `loss(d...)` through\n",
    "backpropagation and calls the optimizer `opt`.\n",
    "Takes a callback as keyword argument `cb`. For example, this will print \"training\"\n",
    "every 10 seconds:\n",
    "```julia\n",
    "Flux.train!(loss, params, data, opt,\n",
    "            cb = throttle(() -> println(\"training\"), 10))\n",
    "```\n",
    "The callback can call `Flux.stop()` to interrupt the training loop.\n",
    "Multiple optimisers and callbacks can be passed to `opt` and `cb` as arrays.\n",
    "\"\"\"\n",
    "function train!(loss, ps, data, opt; cb = () -> ())\n",
    "  ps = Params(ps)\n",
    "  cb = Flux.Optimise.runall(cb)\n",
    "  @showprogress for d in data\n",
    "    try\n",
    "      gs = gradient(ps) do\n",
    "        loss(d...)\n",
    "      end\n",
    "      Flux.Optimise.update!(opt, ps, gs)\n",
    "      cb()\n",
    "    catch ex\n",
    "      if ex isa Flux.Optimise.StopException\n",
    "        break\n",
    "      else\n",
    "        rethrow(ex)\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold_(y::AbstractMatrix, labels...) =\n",
    "  dropdims(mapslices(y -> Base.argmax(y, labels...), y, dims=1), dims=1)\n",
    "\n",
    "function evaluate(model, data)\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for d in data\n",
    "        ŷ = cpu(model(d[1])) # reduction on GPU recompiles every iteration ???\n",
    "        ŷ_cold = onecold_(ŷ)\n",
    "        y = cpu(d[2])\n",
    "        y_cold = onecold_(y)\n",
    "        diff = y_cold .== ŷ_cold\n",
    "        correct += sum(diff)\n",
    "        total += size(d[2])[2]\n",
    "    end\n",
    "    return correct / total\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Relu\n",
    "end\n",
    "\n",
    "(r :: Relu)(x) = relu.(x)\n",
    "\n",
    "Flux.@functor Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Constructing model...\n",
      "└ @ Main In[57]:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(Conv((3, 3), 1=>16), BatchNorm(16), Relu(), ABCConv((3, 3), 16=>32, (3,), (3,), active=false), BatchNorm(32), Relu(), ABCConv((3, 3), 32=>32, (3,), (3,), active=false), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), BatchNorm(32), Relu(), ABCConv((3, 3), 32=>32, (3,), (3,), active=false), BatchNorm(32), Relu(), ABCConv((3, 3), 32=>64, (3,), (3,), active=false), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), getfield(Main, Symbol(\"##79#80\"))(), Relu(), Dense(576, 10), NNlib.softmax)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "M = 3\n",
    "\n",
    "@info(\"Constructing model...\")\n",
    "model = Chain(\n",
    "    Conv((3, 3), 1=>16, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(16),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 16=>32, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(32),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 32=>32, N, M, pad=(1,1)),\n",
    "    \n",
    "    MaxPool((2,2)),\n",
    "    \n",
    "    BatchNorm(32),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 32=>32, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(32),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 32=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    \n",
    "    Relu(),\n",
    "    Dense(64 * 3 * 3, 10),\n",
    "\n",
    "    softmax,\n",
    ")\n",
    "model = gpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(x, y)\n",
    "    ŷ = model(x)\n",
    "    return crossentropy(ŷ, y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure our model + gradients are nicely precompiled before starting our training loop\n",
    "for layer in model.layers\n",
    "    if isa(layer, ABCConv)\n",
    "        binarize(layer, active=true)\n",
    "    end\n",
    "end\n",
    "model(train_set[1][1])\n",
    "gradient(Flux.params(model)) do\n",
    "    loss(train_set[1]...)\n",
    "end\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now switch back to non-binary for pre-training\n",
    "for layer in model.layers\n",
    "    if isa(layer, ABCConv)\n",
    "        binarize(layer, active=false)\n",
    "    end\n",
    "end\n",
    "model(train_set[1][1])\n",
    "gradient(Flux.params(model)) do\n",
    "    loss(train_set[1]...)\n",
    "end\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = ADAM(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[63]:1\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:37\u001b[39m\n",
      "┌ Info: [1]: Test accuracy: 0.8873\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [2]: Test accuracy: 0.9045\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:19\u001b[39m\n",
      "┌ Info: [3]: Test accuracy: 0.9099\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [4]: Test accuracy: 0.9097\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [5]: Test accuracy: 0.9053\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 0.0005!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:19\u001b[39m\n",
      "┌ Info: [6]: Test accuracy: 0.9113\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [7]: Test accuracy: 0.9039\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [8]: Test accuracy: 0.9086\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 0.00025!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [9]: Test accuracy: 0.9273\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [10]: Test accuracy: 0.9289\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [11]: Test accuracy: 0.9252\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [12]: Test accuracy: 0.9271\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 0.000125!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:20\u001b[39m\n",
      "┌ Info: [13]: Test accuracy: 0.9293\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [14]: Test accuracy: 0.9300\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [15]: Test accuracy: 0.9304\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [16]: Test accuracy: 0.9304\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [17]: Test accuracy: 0.9303\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [18]: Test accuracy: 0.9305\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "┌ Info: [19]: Test accuracy: 0.9300\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:15\u001b[39m\n",
      "┌ Info: [20]: Test accuracy: 0.9296\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 6.25e-5!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [21]: Test accuracy: 0.9307\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [22]: Test accuracy: 0.9307\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [23]: Test accuracy: 0.9306\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:19\u001b[39m\n",
      "┌ Info: [24]: Test accuracy: 0.9304\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 3.125e-5!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [25]: Test accuracy: 0.9311\n",
      "└ @ Main In[63]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[63]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [26]: Test accuracy: 0.9310\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [27]: Test accuracy: 0.9307\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 1.5625e-5!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [28]: Test accuracy: 0.9303\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [29]: Test accuracy: 0.9304\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 7.8125e-6!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [30]: Test accuracy: 0.9301\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [31]: Test accuracy: 0.9302\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 3.90625e-6!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [32]: Test accuracy: 0.9297\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [33]: Test accuracy: 0.9300\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 1.953125e-6!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:19\u001b[39m\n",
      "┌ Info: [34]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [35]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 9.765625e-7!\n",
      "└ @ Main In[63]:32\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [36]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "┌ Info: [37]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [38]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [39]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [40]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:17\u001b[39m\n",
      "┌ Info: [41]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:19\u001b[39m\n",
      "┌ Info: [42]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [43]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:19\u001b[39m\n",
      "┌ Info: [44]: Test accuracy: 0.9295\n",
      "└ @ Main In[63]:13\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "┌ Info: [45]: Test accuracy: 0.9294\n",
      "└ @ Main In[63]:13\n",
      "┌ Warning:  -> We're calling this converged.\n",
      "└ @ Main In[63]:39\n"
     ]
    }
   ],
   "source": [
    "@info(\"Beginning training loop...\")\n",
    "\n",
    "best_acc = 0.0\n",
    "last_improvement = 0\n",
    "\n",
    "for epoch_idx in 1:50\n",
    "    global best_acc, last_improvement\n",
    "    # Train for a single epoch\n",
    "    train!(loss, Flux.params(model), train_set, opt)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = evaluate(model, test_set)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "\n",
    "    # If our accuracy is good enough, quit out.\n",
    "    if acc >= 0.999\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # If this is the best accuracy we've seen so far, save the model out\n",
    "    if acc >= best_acc\n",
    "        @info(\" -> New best accuracy! Saving model out to data/mnist_conv.bson\")\n",
    "        BSON.@save joinpath(dirname(@__FILE__), \"data/mnist_conv.bson\") model epoch_idx acc\n",
    "        best_acc = acc\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    # If we haven't seen improvement in 5 epochs, drop our learning rate:\n",
    "    if epoch_idx - last_improvement >= 2 && opt.eta > 1e-6\n",
    "        opt.eta /= 2.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "\n",
    "        # After dropping learning rate, give it a few epochs to improve\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Post-quantization test accuracy: 0.4123\n",
      "└ @ Main In[64]:7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in model.layers\n",
    "    if isa(layer, ABCConv)\n",
    "        binarize(layer)\n",
    "    end\n",
    "end\n",
    "acc = evaluate(model, test_set)\n",
    "@info(@sprintf(\"Post-quantization test accuracy: %.4f\", acc))\n",
    "opt.eta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: quantizing...\n",
      "└ @ Main In[65]:1\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:07\u001b[39m\n",
      "┌ Info: [1]: Test accuracy: 0.8851\n",
      "└ @ Main In[65]:10\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:59\u001b[39m\n",
      "┌ Info: [2]: Test accuracy: 0.8955\n",
      "└ @ Main In[65]:10\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:58\u001b[39m\n",
      "┌ Info: [3]: Test accuracy: 0.8953\n",
      "└ @ Main In[65]:10\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:58\u001b[39m\n",
      "┌ Info: [4]: Test accuracy: 0.9081\n",
      "└ @ Main In[65]:10\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:59\u001b[39m\n",
      "┌ Info: [5]: Test accuracy: 0.9060\n",
      "└ @ Main In[65]:10\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:57\u001b[39m\n",
      "┌ Info: [6]: Test accuracy: 0.9063\n",
      "└ @ Main In[65]:10\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:58\u001b[39m\n",
      "┌ Info: [7]: Test accuracy: 0.9098\n",
      "└ @ Main In[65]:10\n",
      "\u001b[32mProgress:   7%|███                                      |  ETA: 0:00:47\u001b[39m"
     ]
    }
   ],
   "source": [
    "@info(\"quantizing...\")\n",
    "\n",
    "for epoch_idx in 1:50\n",
    "    global best_acc, last_improvement\n",
    "    # Train for a single epoch\n",
    "    train!(loss, Flux.params(model), train_set, opt)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = evaluate(model, test_set)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "\n",
    "    # If our accuracy is good enough, quit out.\n",
    "    if acc >= 0.999\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # If this is the best accuracy we've seen so far, save the model out\n",
    "    if acc >= best_acc\n",
    "        @info(\" -> New best accuracy! Saving model out to data/mnist_conv.bson\")\n",
    "        BSON.@save joinpath(dirname(@__FILE__), \"data/mnist_conv.bson\") model epoch_idx acc\n",
    "        best_acc = acc\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    # If we haven't seen improvement in 5 epochs, drop our learning rate:\n",
    "    if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "\n",
    "        # After dropping learning rate, give it a few epochs to improve\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fieldmeta (generic function with 1 method)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fieldmeta(q)\n",
    "    println(typeof(q))\n",
    "    for name in fieldnames(typeof(q))\n",
    "        field = getfield(q, name)\n",
    "        shape = if typeof(field) <: AbstractArray\n",
    "            repr(size(field))\n",
    "        else\n",
    "            \"\"\n",
    "        end\n",
    "        \n",
    "        println('\\t', String(name), '\\t', typeof(field), '\\t', shape)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuda stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576-element Array{Float32,1}:\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " ⋮  \n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2^20\n",
    "x = fill(1.0f0, N)  # a vector filled with 1.0 (Float32)\n",
    "y = fill(2.0f0, N)  # a vector filled with 2.0\n",
    "\n",
    "y .+= x             # increment each element of y with the corresponding element of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test all(y .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sequential_add!(y, x)\n",
    "    for i in eachindex(y, x)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y, 2)\n",
    "sequential_add!(y, x)\n",
    "@test all(y .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function parallel_add!(y, x)\n",
    "    Threads.@threads for i in eachindex(y, x)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y, 2)\n",
    "parallel_add!(y, x)\n",
    "@test all(y .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  321.687 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "#@btime sequential_add!($y, $x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  326.044 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "#@btime sequential_add!($y, $x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576-element CuArray{Float32,1,Nothing}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " ⋮  \n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_d = CuArrays.fill(1.0f0, N)  # a vector stored on the GPU filled with 1.0 (Float32)\n",
    "y_d = CuArrays.fill(2.0f0, N)  # a vector stored on the GPU filled with 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_d .+= x_d\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  58.964 μs (63 allocations: 2.67 KiB)\n"
     ]
    }
   ],
   "source": [
    "function add_broadcast!(y, x)\n",
    "    CuArrays.@  y .+= x\n",
    "    return\n",
    "end\n",
    "\n",
    "#@btime add_broadcast!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDAnative\n",
    "\n",
    "function gpu_add1!(y, x)\n",
    "    for i = 1:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y_d, 2)\n",
    "@cuda gpu_add1!(y_d, x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  149.128 ms (25 allocations: 736 bytes)\n"
     ]
    }
   ],
   "source": [
    "function bench_gpu1!(y, x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda gpu_add1!(y, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "#@btime bench_gpu1!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Calling CUDAdrv.@profile only informs an external profiler to start.\n",
      "│ The user is responsible for launching Julia under a CUDA profiler like `nvprof`.\n",
      "│ \n",
      "│ For improved usability, launch Julia under the Nsight Systems profiler:\n",
      "│ $ nsys launch -t cuda,cublas,cudnn,nvtx julia\n",
      "└ @ CUDAdrv.Profile /data/scratch/jhgilles/.julia/packages/CUDAdrv/3EzC1/src/profile.jl:42\n"
     ]
    }
   ],
   "source": [
    "using CUDAdrv\n",
    "bench_gpu1!(y_d, x_d)  # run it once to force compilation\n",
    "#CUDAdrv.@profile bench_gpu1!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gpu_add2!(y, x)\n",
    "    index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n",
    "    stride = blockDim().x\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y_d, 2)\n",
    "@cuda threads=256 gpu_add2!(y_d, x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.308 ms (25 allocations: 736 bytes)\n"
     ]
    }
   ],
   "source": [
    "function bench_gpu2!(y, x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=256 gpu_add2!(y, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "#@btime bench_gpu2!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gpu_add3!(y, x)\n",
    "    index = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "numblocks = ceil(Int, N/256)\n",
    "\n",
    "fill!(y_d, 2)\n",
    "@cuda threads=256 blocks=numblocks gpu_add3!(y_d, x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  57.996 μs (28 allocations: 784 bytes)\n"
     ]
    }
   ],
   "source": [
    "function bench_gpu3!(y, x)\n",
    "    numblocks = ceil(Int, length(y)/256)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=256 blocks=numblocks gpu_add3!(y, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "#@btime bench_gpu3!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threadIdx 1, blockDim 16\n",
      "threadIdx 2, blockDim 16\n",
      "threadIdx 3, blockDim 16\n",
      "threadIdx 4, blockDim 16\n",
      "threadIdx 5, blockDim 16\n",
      "threadIdx 6, blockDim 16\n",
      "threadIdx 7, blockDim 16\n",
      "threadIdx 8, blockDim 16\n",
      "threadIdx 9, blockDim 16\n",
      "threadIdx 10, blockDim 16\n",
      "threadIdx 11, blockDim 16\n",
      "threadIdx 12, blockDim 16\n",
      "threadIdx 13, blockDim 16\n",
      "threadIdx 14, blockDim 16\n",
      "threadIdx 15, blockDim 16\n",
      "threadIdx 16, blockDim 16\n"
     ]
    }
   ],
   "source": [
    "function gpu_add2_print!(y, x)\n",
    "    index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n",
    "    stride = blockDim().x\n",
    "    @cuprintf(\"threadIdx %ld, blockDim %ld\\n\", index, stride)\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "@cuda threads=16 gpu_add2_print!(y_d, x_d)\n",
    "synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
