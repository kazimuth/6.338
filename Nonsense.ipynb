{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote\n",
    "using CuArrays\n",
    "using CUDAnative\n",
    "using CUDAdrv\n",
    "using NNlib\n",
    "using Test\n",
    "using BenchmarkTools\n",
    "using Flux\n",
    "using Plots\n",
    "using Images\n",
    "using Colors\n",
    "using ProgressMeter\n",
    "using StaticArrays\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zygote.@adjoint gpu(a :: Array) = gpu(a), a̅ -> (cpu(a̅),)\n",
    "Zygote.@adjoint function Base.convert(::Type{T}, xs::Array{K,N}) where {T<:CuArray, K, N}\n",
    "  Base.convert(T, xs), Δ -> (nothing, Base.convert(Array, Δ),)\n",
    "end\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CuArrays.allowscalar(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight_masks (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function weight_masks(W, us)\n",
    "    dims = size(W)\n",
    "\n",
    "    W̅ = W .- mean(W)\n",
    "\n",
    "    std_W = std(W)\n",
    "\n",
    "    W̃s = cat((sign.(W̅ .+ (u * std_W)) for u in us)...,\n",
    "            dims=length(size(W)) + 1)\n",
    "\n",
    "    W̃s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarize_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function binarize_weights(W, us)\n",
    "    W̃s = weight_masks(W, us)\n",
    "\n",
    "    dims = size(W)\n",
    "\n",
    "    Wv = reshape(W, :)\n",
    "    W̃vs = reshape(W̃s, :, length(us))\n",
    "\n",
    "    αs = W̃vs \\ Wv\n",
    "\n",
    "    W̃v = W̃vs * αs\n",
    "    \n",
    "    W̃ = reshape(W̃v, dims...)\n",
    "\n",
    "    W̃, αs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zygote.@adjoint function binarize_weights(W, us) \n",
    "    W̃ = binarize_weights(W, us)\n",
    "    function adjoint((∇_W̃, ∇_αs)) \n",
    "        # the \"straight-through estimator\"\n",
    "        (∇_W̃, nothing)\n",
    "    end\n",
    "    W̃, adjoint\n",
    "end\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "even_err (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_us(M) = if M == 1\n",
    "        [0.0f0]\n",
    "    else\n",
    "        Array(range(-1.0f0, stop=1.0f0, length=M))\n",
    "end\n",
    "function even_err(M, W)\n",
    "    us = even_us(M)\n",
    "    W̃, _ = binarize_weights(W, us)\n",
    "    mean((W .- W̃).^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = randn(Float32, 3,3,10,10)\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize_weights(W, even_us(3))\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo write gradient test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip0800\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0800)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0801\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0800)\" d=\"\n",
       "M242.516 1425.62 L2352.76 1425.62 L2352.76 47.2441 L242.516 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0802\">\n",
       "    <rect x=\"242\" y=\"47\" width=\"2111\" height=\"1379\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  523.439,1425.62 523.439,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  965.838,1425.62 965.838,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1408.24,1425.62 1408.24,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1850.63,1425.62 1850.63,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2293.03,1425.62 2293.03,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,1245.3 2352.76,1245.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,1026.49 2352.76,1026.49 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,807.667 2352.76,807.667 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,588.848 2352.76,588.848 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,370.03 2352.76,370.03 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  242.516,151.212 2352.76,151.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,1425.62 242.516,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  523.439,1425.62 523.439,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  965.838,1425.62 965.838,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1408.24,1425.62 1408.24,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1850.63,1425.62 1850.63,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2293.03,1425.62 2293.03,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,1245.3 274.17,1245.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,1026.49 274.17,1026.49 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,807.667 274.17,807.667 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,588.848 274.17,588.848 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,370.03 274.17,370.03 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  242.516,151.212 274.17,151.212 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 523.439, 1479.62)\" x=\"523.439\" y=\"1479.62\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 965.838, 1479.62)\" x=\"965.838\" y=\"1479.62\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1408.24, 1479.62)\" x=\"1408.24\" y=\"1479.62\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1850.63, 1479.62)\" x=\"1850.63\" y=\"1479.62\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2293.03, 1479.62)\" x=\"2293.03\" y=\"1479.62\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 1262.8)\" x=\"218.516\" y=\"1262.8\">0.10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 1043.99)\" x=\"218.516\" y=\"1043.99\">0.15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 825.167)\" x=\"218.516\" y=\"825.167\">0.20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 606.348)\" x=\"218.516\" y=\"606.348\">0.25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 387.53)\" x=\"218.516\" y=\"387.53\">0.30</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 218.516, 168.712)\" x=\"218.516\" y=\"168.712\">0.35</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1297.64, 1559.48)\" x=\"1297.64\" y=\"1559.48\">M</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 736.431)\" x=\"89.2861\" y=\"736.431\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0802)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  302.24,86.2547 523.439,532.361 744.638,1156.49 965.838,1292.54 1187.04,1336.13 1408.24,1358.39 1629.43,1370.45 1850.63,1379.72 2071.83,1383.16 2293.03,1386.61 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip0800)\" d=\"\n",
       "M1989.93 251.724 L2280.76 251.724 L2280.76 130.764 L1989.93 130.764  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,251.724 2280.76,251.724 2280.76,130.764 1989.93,130.764 1989.93,251.724 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0800)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,191.244 2157.93,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0800)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 208.744)\" x=\"2181.93\" y=\"208.744\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ms = 1:10\n",
    "errs = [even_err(M, W) for M in Ms]\n",
    "plot(Ms, errs, xlabel=\"M\", ylabel=\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't solve for alphas\n",
    "function binarize_weights(W, us, αs)\n",
    "    W̃s = weight_masks(W, us)\n",
    "\n",
    "    dims = size(W)\n",
    "\n",
    "    Wv = reshape(W, :)\n",
    "    W̃vs = reshape(W̃s, :, length(us))\n",
    "\n",
    "    W̃v = W̃vs * αs\n",
    "    \n",
    "    W̃ = reshape(W̃v, dims...)\n",
    "\n",
    "    W̃\n",
    "end\n",
    "\n",
    "Zygote.@adjoint function binarize_weights(W, us, αs)\n",
    "    # not sure why you'd need this, but might as well...\n",
    "    W̃ = binarize_weights(W, us, αs)\n",
    "    function adjoint(∇_W̃) \n",
    "        # the \"straight-through estimator\"\n",
    "        (∇_W̃, nothing, nothing)\n",
    "    end\n",
    "    W̃, adjoint\n",
    "end\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us = even_us(5)\n",
    "W̃, αs = binarize_weights(W, us)\n",
    "W̃1 = binarize_weights(W, us, αs)\n",
    "\n",
    "@test W̃ == W̃1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇_W, ∇_us, ∇_αs = Zygote.gradient((W, us, αs) -> sum(binarize_weights(W, us, αs)), W, us, αs)\n",
    "@test ∇_W == ones(Float32, size(W))\n",
    "@test ∇_us == nothing\n",
    "@test ∇_αs == nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarize_weights (generic function with 4 methods)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function binarize_weights(W :: CuArray, us)\n",
    "    W̃, αs = binarize_weights(cpu(W), cpu(us))\n",
    "    gpu(W̃), gpu(αs)\n",
    "end\n",
    "function binarize_weights(W :: CuArray, us, αs)\n",
    "    W̃ = binarize_weights(cpu(W), cpu(us), cpu(αs))\n",
    "    gpu(W̃)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  87.720 μs (201 allocations: 109.77 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime binarize_weights(W, even_us(5))\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  166.781 μs (241 allocations: 119.70 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wg = gpu(W)\n",
    "@btime binarize_weights(Wg, even_us(5))\n",
    "()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "function z(q)\n",
    "    if q > 0.5f0\n",
    "        1.0f0\n",
    "    else\n",
    "        -1.0f0\n",
    "    end\n",
    "end\n",
    "\n",
    "function _zrev(q, ∇_q)\n",
    "    if 0.0f0 < q < 1.0f0\n",
    "        ∇_q\n",
    "    else\n",
    "        0.0f0\n",
    "    end\n",
    "end\n",
    "\n",
    "Zygote.@adjoint z(q) = z(q), (∇_q) -> _zrev(q, ∇_q)\n",
    "\n",
    "function zb(Q)\n",
    "    z.(Q)\n",
    "end\n",
    "\n",
    "Zygote.@adjoint zb(Q) = zb(Q), (∇_Q) -> (_zrev.(Q, ∇_Q),)\n",
    "\n",
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "function binarize_activations(A, vs, βs)\n",
    "    shape = size(A)\n",
    "    \n",
    "    Av_x = reshape( (A), :, 1)\n",
    "    \n",
    "    vs_x = reshape( (vs), 1, :)\n",
    "    βs_x = reshape( (βs), 1, :)\n",
    "    \n",
    "    Av1 =  (Av_x) .+  (vs_x)\n",
    "    #Av2 = z.( (Av1))\n",
    "    Av2 = zb( (Av1))\n",
    "    Av3 =  (Av2) .*  (βs_x)\n",
    "    \n",
    "    Ãv = sum( (Av3), dims=2)\n",
    "    \n",
    "    result = reshape( (Ãv), shape)\n",
    "        \n",
    "    result\n",
    "end\n",
    "\n",
    "Zygote.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float32,2}:\n",
       " -0.87212    0.161947   -0.952302  -0.892614\n",
       "  0.614873  -0.446069    0.704746  -0.592104\n",
       "  1.30157    0.0858498  -0.155583   1.98845 \n",
       "  0.334237  -0.444257   -1.54135   -0.413687"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = randn(Float32, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float32,2}:\n",
       " -2.0  0.0  -2.0  -2.0\n",
       "  0.0  0.0   0.0  -2.0\n",
       "  0.0  0.0   0.0   2.0\n",
       "  0.0  0.0  -2.0   0.0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = even_us(2)\n",
    "bs = βs = ones(Float32, 2)\n",
    "\n",
    "binarize_activations(A, vs, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[1.0 0.0 1.0 1.0; 0.0 1.0 0.0 1.0; 1.0 0.0 1.0 1.0; 0.0 1.0 0.0 1.0], Float32[2.0, 8.0], Float32[-14.0, 6.0])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient((A, vs, βs) -> sum(binarize_activations(A, vs, βs)), A, vs, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 CuArray{Float32,2,CuArray{Float32,2,Nothing}}:\n",
       " 0.0  -2.0   0.0  -2.0\n",
       " 0.0  -2.0   0.0  -2.0\n",
       " 0.0   0.0  -2.0   0.0\n",
       " 0.0  -2.0   0.0   0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize_activations(gpu(A), gpu(even_us(2)), gpu(ones(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.0f0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((A, vs, βs) -> sum(binarize_activations(A, vs, βs)))(gpu(A), gpu(even_us(2)), gpu(ones(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0 1.0 0.0 1.0; 0.0 1.0 1.0 0.0; 0.0 0.0 1.0 1.0; 0.0 0.0 0.0 1.0], Float32[0.0, 7.0], Float32[-16.0, 4.0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient((A, vs, βs) -> sum(binarize_activations(A, vs, βs)), gpu(A), gpu(even_us(2)), gpu(ones(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal_error_plot (generic function with 2 methods)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normal_error_plot(N, steps=1000)\n",
    "    vs = gpu(even_us(2))\n",
    "    bs = βs = gpu(ones(Float32, 2))\n",
    "\n",
    "    opt = ADAM()\n",
    "    ps = Params([vs, βs])\n",
    "    errors = Float32[]\n",
    "    @showprogress for _ in 1:steps\n",
    "        # note: in loop!\n",
    "        A = gpu(randn(100, 100))\n",
    "        err, adjoint = pullback(ps) do\n",
    "            Ã = binarize_activations(A, vs, βs)\n",
    "            mean((A .- Ã).^2)\n",
    "        end\n",
    "        gs = adjoint(1.0)\n",
    "\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "        push!(errors, err)\n",
    "    end\n",
    "    plot(errors, xlabel=\"gradient descent step\", ylabel=\"MSE\", title=\"error with N=$N bits\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:31\u001b[39m\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip1200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip1200)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip1201\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip1200)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 121.675 L215.754 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip1202\">\n",
       "    <rect x=\"215\" y=\"121\" width=\"2138\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1347.67 2352.76,1347.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1037.91 2352.76,1037.91 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,728.156 2352.76,728.156 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,418.397 2352.76,418.397 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1347.67 247.809,1347.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1037.91 247.809,1037.91 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,728.156 247.809,728.156 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,418.397 247.809,418.397 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 274.217, 1479.62)\" x=\"274.217\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 778.732, 1479.62)\" x=\"778.732\" y=\"1479.62\">250</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1283.25, 1479.62)\" x=\"1283.25\" y=\"1479.62\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1787.76, 1479.62)\" x=\"1787.76\" y=\"1479.62\">750</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2292.27, 1479.62)\" x=\"2292.27\" y=\"1479.62\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1365.17)\" x=\"191.754\" y=\"1365.17\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1055.41)\" x=\"191.754\" y=\"1055.41\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 745.656)\" x=\"191.754\" y=\"745.656\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 435.897)\" x=\"191.754\" y=\"435.897\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 73.2)\" x=\"1284.25\" y=\"73.2\">error with N=2 bits</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">gradient descent step</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,158.579 278.253,183.778 280.271,184.494 282.289,242.134 284.307,243.422 286.326,226.282 288.344,177.269 290.362,247.128 292.38,234.825 294.398,214.535 \n",
       "  296.416,274.334 298.434,257.091 300.452,273.696 302.47,253.982 304.488,309.318 306.506,288.6 308.524,322.382 310.542,293.582 312.56,343.962 314.578,271.32 \n",
       "  316.596,286.062 318.614,316.151 320.633,316.732 322.651,327.085 324.669,327.852 326.687,352.202 328.705,343.828 330.723,336.9 332.741,363.877 334.759,371.089 \n",
       "  336.777,382.572 338.795,354.686 340.813,363.831 342.831,387.852 344.849,383.839 346.867,387.282 348.885,370.864 350.903,403.995 352.921,401.727 354.94,398.246 \n",
       "  356.958,423.568 358.976,401.927 360.994,439.296 363.012,440.212 365.03,397.987 367.048,473.869 369.066,465.088 371.084,424.972 373.102,463.458 375.12,438.338 \n",
       "  377.138,465.734 379.156,483.223 381.174,474.701 383.192,467.7 385.21,477.152 387.228,506.056 389.246,485.035 391.265,473.541 393.283,530.645 395.301,492.856 \n",
       "  397.319,493.884 399.337,523.016 401.355,548.393 403.373,516.842 405.391,517.292 407.409,517.286 409.427,542.483 411.445,541.072 413.463,527.82 415.481,542.669 \n",
       "  417.499,558.245 419.517,564.208 421.535,559.582 423.553,532.47 425.572,581.433 427.59,595.918 429.608,547.025 431.626,560.261 433.644,586.163 435.662,601.418 \n",
       "  437.68,608.319 439.698,579.896 441.716,629.305 443.734,608.617 445.752,607.106 447.77,623.576 449.788,612.464 451.806,611.497 453.824,616.774 455.842,613.845 \n",
       "  457.86,626.934 459.879,612.102 461.897,620.91 463.915,658.374 465.933,653.456 467.951,633.668 469.969,652.525 471.987,641.343 474.005,685.61 476.023,651.83 \n",
       "  478.041,678.508 480.059,631.704 482.077,696.081 484.095,675.824 486.113,671.163 488.131,685.525 490.149,679.716 492.167,701.479 494.185,695.544 496.204,710.985 \n",
       "  498.222,685.572 500.24,705.856 502.258,718.17 504.276,718.471 506.294,725.117 508.312,726.722 510.33,743.484 512.348,723.993 514.366,715.559 516.384,731.07 \n",
       "  518.402,731.84 520.42,735.662 522.438,735.169 524.456,748.921 526.474,717.218 528.492,745.419 530.511,752.329 532.529,776.888 534.547,749.771 536.565,766.811 \n",
       "  538.583,755.538 540.601,754.666 542.619,783.237 544.637,739.643 546.655,760.555 548.673,795.141 550.691,781.322 552.709,783.839 554.727,770.699 556.745,764.62 \n",
       "  558.763,793.804 560.781,795.711 562.799,794.342 564.817,778.967 566.836,782.333 568.854,782.501 570.872,825.282 572.89,807.507 574.908,815.708 576.926,811.228 \n",
       "  578.944,832.715 580.962,795.997 582.98,821.053 584.998,829.836 587.016,843.839 589.034,833.834 591.052,832.391 593.07,830.131 595.088,819.955 597.106,817.75 \n",
       "  599.124,865.285 601.143,843.873 603.161,834.503 605.179,850.812 607.197,869.245 609.215,861.408 611.233,841.157 613.251,832.421 615.269,855.611 617.287,863.686 \n",
       "  619.305,871.341 621.323,877.082 623.341,855.182 625.359,880.548 627.377,861.761 629.395,861.462 631.413,879.344 633.431,869.106 635.45,906.318 637.468,891.631 \n",
       "  639.486,878.738 641.504,886.181 643.522,899.433 645.54,882.741 647.558,870.44 649.576,916.893 651.594,912.372 653.612,892.959 655.63,905.921 657.648,905.461 \n",
       "  659.666,918.298 661.684,913.897 663.702,914.826 665.72,899.029 667.738,928.519 669.756,934.495 671.775,930.185 673.793,910.865 675.811,922.745 677.829,930.232 \n",
       "  679.847,922.493 681.865,908.789 683.883,927.077 685.901,961.004 687.919,944.932 689.937,955.874 691.955,960.294 693.973,948.18 695.991,949.153 698.009,941.448 \n",
       "  700.027,962.098 702.045,944.535 704.063,951.039 706.082,962.68 708.1,974.141 710.118,966.17 712.136,953.468 714.154,958.192 716.172,973.722 718.19,958.754 \n",
       "  720.208,959.876 722.226,966.879 724.244,949.489 726.262,982.212 728.28,966.744 730.298,972.964 732.316,995.23 734.334,974.868 736.352,967.726 738.37,982.552 \n",
       "  740.388,978.245 742.407,980.157 744.425,968.746 746.443,962.948 748.461,1002.39 750.479,970.555 752.497,981.589 754.515,990.346 756.533,983.675 758.551,997.978 \n",
       "  760.569,996.223 762.587,1002 764.605,995.211 766.623,1004.01 768.641,1030.83 770.659,993.873 772.677,1018.54 774.695,1004.03 776.714,998.777 778.732,1018.23 \n",
       "  780.75,986.134 782.768,1013.89 784.786,1000.17 786.804,1017.16 788.822,1013.46 790.84,1019.28 792.858,1027.58 794.876,1008.3 796.894,1032.32 798.912,1007.84 \n",
       "  800.93,1025.81 802.948,1033.32 804.966,1017.3 806.984,1035.93 809.002,1015.43 811.021,1033.5 813.039,1037.19 815.057,1042.68 817.075,1031.94 819.093,1051.45 \n",
       "  821.111,1049.4 823.129,1038.32 825.147,1037.42 827.165,1033.04 829.183,1019.83 831.201,1060.12 833.219,1054.15 835.237,1068.09 837.255,1047.83 839.273,1064.52 \n",
       "  841.291,1049.03 843.309,1061.42 845.327,1037.82 847.346,1048.83 849.364,1056.08 851.382,1068.67 853.4,1042.02 855.418,1059.8 857.436,1076.23 859.454,1056.55 \n",
       "  861.472,1046.62 863.49,1050.86 865.508,1048.22 867.526,1074.25 869.544,1063.46 871.562,1088.05 873.58,1068.84 875.598,1083.61 877.616,1059.12 879.634,1072.59 \n",
       "  881.653,1082.8 883.671,1078.92 885.689,1083.79 887.707,1079.32 889.725,1073.38 891.743,1065.64 893.761,1091.91 895.779,1090.13 897.797,1080.01 899.815,1098.52 \n",
       "  901.833,1094.72 903.851,1087.39 905.869,1084.58 907.887,1087.21 909.905,1073.13 911.923,1080.43 913.941,1098.21 915.959,1088.97 917.978,1105.35 919.996,1086.84 \n",
       "  922.014,1086.92 924.032,1100.46 926.05,1090.27 928.068,1088.01 930.086,1109.69 932.104,1098.88 934.122,1116.94 936.14,1109 938.158,1107.76 940.176,1093.08 \n",
       "  942.194,1111.15 944.212,1118.74 946.23,1100.14 948.248,1103.88 950.266,1107.77 952.285,1139.18 954.303,1134.2 956.321,1124.94 958.339,1102.08 960.357,1119.52 \n",
       "  962.375,1105.24 964.393,1103.13 966.411,1124.49 968.429,1116.48 970.447,1139.18 972.465,1134.18 974.483,1141.96 976.501,1127.88 978.519,1110.95 980.537,1119.28 \n",
       "  982.555,1118.23 984.573,1135.19 986.592,1123.27 988.61,1134.31 990.628,1126.94 992.646,1133.53 994.664,1119.24 996.682,1145.6 998.7,1119.08 1000.72,1139.09 \n",
       "  1002.74,1134.89 1004.75,1119.93 1006.77,1133.37 1008.79,1136.45 1010.81,1137.3 1012.83,1142.55 1014.84,1131.92 1016.86,1139.38 1018.88,1146.67 1020.9,1142.74 \n",
       "  1022.92,1134.88 1024.93,1143.85 1026.95,1146 1028.97,1127.37 1030.99,1155.85 1033.01,1150.77 1035.02,1136.83 1037.04,1149.9 1039.06,1151.47 1041.08,1153.07 \n",
       "  1043.1,1154.77 1045.12,1158.76 1047.13,1153.59 1049.15,1170.83 1051.17,1140.59 1053.19,1156.14 1055.21,1152.47 1057.22,1147.76 1059.24,1167.24 1061.26,1160.51 \n",
       "  1063.28,1161.3 1065.3,1156.02 1067.31,1158.75 1069.33,1170.34 1071.35,1171.95 1073.37,1156.48 1075.39,1154.06 1077.4,1174.65 1079.42,1153.56 1081.44,1159.77 \n",
       "  1083.46,1161.82 1085.48,1172.82 1087.49,1169.18 1089.51,1165.07 1091.53,1162.75 1093.55,1180.9 1095.57,1183.82 1097.58,1166.58 1099.6,1142.58 1101.62,1161.96 \n",
       "  1103.64,1186.47 1105.66,1164.63 1107.67,1169.22 1109.69,1166.97 1111.71,1179.62 1113.73,1166.56 1115.75,1173.5 1117.77,1179.04 1119.78,1186.34 1121.8,1168.39 \n",
       "  1123.82,1179.7 1125.84,1178.03 1127.86,1176.92 1129.87,1167.39 1131.89,1204.42 1133.91,1179.99 1135.93,1199.34 1137.95,1192.44 1139.96,1172.32 1141.98,1204 \n",
       "  1144,1173.45 1146.02,1205.92 1148.04,1195.9 1150.05,1204.75 1152.07,1202.04 1154.09,1173.31 1156.11,1181.11 1158.13,1174 1160.14,1177.02 1162.16,1216.68 \n",
       "  1164.18,1194.54 1166.2,1186.59 1168.22,1191.14 1170.23,1199.98 1172.25,1196.44 1174.27,1198.84 1176.29,1204.22 1178.31,1201.65 1180.33,1181.3 1182.34,1191.5 \n",
       "  1184.36,1200.37 1186.38,1199.65 1188.4,1195.2 1190.42,1206.09 1192.43,1197.96 1194.45,1202.97 1196.47,1218.09 1198.49,1202.97 1200.51,1213.45 1202.52,1198.22 \n",
       "  1204.54,1196.31 1206.56,1204.63 1208.58,1209.43 1210.6,1205.4 1212.61,1208.71 1214.63,1207.66 1216.65,1212.88 1218.67,1207.52 1220.69,1227.26 1222.7,1225.78 \n",
       "  1224.72,1197.84 1226.74,1201.83 1228.76,1222.48 1230.78,1222.46 1232.79,1219.01 1234.81,1214.2 1236.83,1226.84 1238.85,1219.65 1240.87,1212.62 1242.88,1232.33 \n",
       "  1244.9,1221.58 1246.92,1229.26 1248.94,1209.38 1250.96,1234.7 1252.98,1214.69 1254.99,1234.47 1257.01,1208.99 1259.03,1205.32 1261.05,1229.91 1263.07,1210.66 \n",
       "  1265.08,1211.12 1267.1,1228.17 1269.12,1220.4 1271.14,1235.57 1273.16,1246.66 1275.17,1240.65 1277.19,1232.87 1279.21,1218.33 1281.23,1228.01 1283.25,1250.17 \n",
       "  1285.26,1233.61 1287.28,1223.67 1289.3,1241.1 1291.32,1240.16 1293.34,1244.22 1295.35,1220.74 1297.37,1234.51 1299.39,1239.32 1301.41,1237.6 1303.43,1252.88 \n",
       "  1305.44,1221.45 1307.46,1232.81 1309.48,1248.09 1311.5,1247.3 1313.52,1227.75 1315.53,1247.34 1317.55,1239.6 1319.57,1245.76 1321.59,1238.6 1323.61,1237.2 \n",
       "  1325.63,1256.27 1327.64,1250.16 1329.66,1240.2 1331.68,1247.46 1333.7,1240.29 1335.72,1245.85 1337.73,1233.33 1339.75,1247.9 1341.77,1250.58 1343.79,1223.8 \n",
       "  1345.81,1256.14 1347.82,1254.35 1349.84,1251.88 1351.86,1239.45 1353.88,1244.66 1355.9,1273.18 1357.91,1232.36 1359.93,1240.29 1361.95,1251.57 1363.97,1232.14 \n",
       "  1365.99,1242.06 1368,1249.21 1370.02,1259.5 1372.04,1258.96 1374.06,1266.61 1376.08,1239.85 1378.09,1229.69 1380.11,1254.97 1382.13,1258.49 1384.15,1266.14 \n",
       "  1386.17,1264.4 1388.18,1264.41 1390.2,1249.73 1392.22,1261.02 1394.24,1263.6 1396.26,1245.42 1398.28,1260.92 1400.29,1257.76 1402.31,1259.94 1404.33,1261.68 \n",
       "  1406.35,1276.99 1408.37,1251.28 1410.38,1258.74 1412.4,1263.97 1414.42,1266.22 1416.44,1249.46 1418.46,1268.15 1420.47,1249.6 1422.49,1257.32 1424.51,1259.51 \n",
       "  1426.53,1275.33 1428.55,1258.1 1430.56,1270.2 1432.58,1248.52 1434.6,1251.46 1436.62,1261.2 1438.64,1263.94 1440.65,1254.95 1442.67,1276.52 1444.69,1260.31 \n",
       "  1446.71,1275.13 1448.73,1277.2 1450.74,1281.96 1452.76,1277.91 1454.78,1288.22 1456.8,1263.24 1458.82,1275.69 1460.84,1278.17 1462.85,1277.19 1464.87,1257.41 \n",
       "  1466.89,1257.09 1468.91,1283.81 1470.93,1299.67 1472.94,1272.16 1474.96,1286.2 1476.98,1268.84 1479,1275.25 1481.02,1269.34 1483.03,1270.01 1485.05,1263.27 \n",
       "  1487.07,1270.99 1489.09,1262.83 1491.11,1274.65 1493.12,1281.89 1495.14,1279.75 1497.16,1272.39 1499.18,1276.31 1501.2,1271.95 1503.21,1276.92 1505.23,1286.72 \n",
       "  1507.25,1279.15 1509.27,1284.67 1511.29,1279.21 1513.3,1283.49 1515.32,1297.85 1517.34,1287.04 1519.36,1283.17 1521.38,1280.36 1523.39,1293.09 1525.41,1290.66 \n",
       "  1527.43,1289.61 1529.45,1277.74 1531.47,1280.23 1533.49,1284.1 1535.5,1303.91 1537.52,1287.49 1539.54,1282.96 1541.56,1282.13 1543.58,1269.36 1545.59,1291.28 \n",
       "  1547.61,1281.44 1549.63,1282.07 1551.65,1292.66 1553.67,1282.91 1555.68,1303.17 1557.7,1293.01 1559.72,1295 1561.74,1278.1 1563.76,1299.99 1565.77,1286.69 \n",
       "  1567.79,1288.73 1569.81,1303.17 1571.83,1289.9 1573.85,1285.44 1575.86,1304.81 1577.88,1288.32 1579.9,1280.11 1581.92,1298.03 1583.94,1290.64 1585.95,1286.67 \n",
       "  1587.97,1296.79 1589.99,1293.6 1592.01,1316.4 1594.03,1299.68 1596.04,1297.58 1598.06,1308.21 1600.08,1277.22 1602.1,1290.67 1604.12,1311.47 1606.14,1278.4 \n",
       "  1608.15,1297.46 1610.17,1298.37 1612.19,1309.29 1614.21,1301.03 1616.23,1286.73 1618.24,1315.39 1620.26,1298.36 1622.28,1303.65 1624.3,1288.2 1626.32,1299.19 \n",
       "  1628.33,1292.83 1630.35,1310.93 1632.37,1295.03 1634.39,1312.11 1636.41,1324.26 1638.42,1312.95 1640.44,1291.6 1642.46,1312.13 1644.48,1294.36 1646.5,1283.57 \n",
       "  1648.51,1317.24 1650.53,1305.94 1652.55,1311.46 1654.57,1309.12 1656.59,1303.99 1658.6,1316.28 1660.62,1306.11 1662.64,1300.18 1664.66,1315.15 1666.68,1307.86 \n",
       "  1668.69,1315.24 1670.71,1309.19 1672.73,1315.66 1674.75,1313.62 1676.77,1303.96 1678.79,1309.95 1680.8,1298.4 1682.82,1321.38 1684.84,1309.92 1686.86,1304.95 \n",
       "  1688.88,1293.73 1690.89,1303.65 1692.91,1318.36 1694.93,1308.72 1696.95,1291.63 1698.97,1310.15 1700.98,1321.89 1703,1300.89 1705.02,1297.85 1707.04,1323.76 \n",
       "  1709.06,1306.45 1711.07,1305.25 1713.09,1309.32 1715.11,1318.63 1717.13,1335.72 1719.15,1327.5 1721.16,1316.51 1723.18,1301.03 1725.2,1306.36 1727.22,1307.19 \n",
       "  1729.24,1316.65 1731.25,1317.36 1733.27,1309.22 1735.29,1312.89 1737.31,1327.54 1739.33,1331.69 1741.35,1318.86 1743.36,1323.82 1745.38,1309.42 1747.4,1320.07 \n",
       "  1749.42,1315.48 1751.44,1317 1753.45,1312.9 1755.47,1320.12 1757.49,1330.76 1759.51,1332.28 1761.53,1311.67 1763.54,1303.67 1765.56,1324.18 1767.58,1323.79 \n",
       "  1769.6,1310.02 1771.62,1321.44 1773.63,1345.65 1775.65,1325.23 1777.67,1318.14 1779.69,1331.76 1781.71,1314.67 1783.72,1326.05 1785.74,1328.39 1787.76,1329.07 \n",
       "  1789.78,1331.72 1791.8,1329.08 1793.81,1335.19 1795.83,1323.98 1797.85,1336.08 1799.87,1329.42 1801.89,1319.84 1803.9,1315.63 1805.92,1335.31 1807.94,1310.84 \n",
       "  1809.96,1323.94 1811.98,1329.69 1814,1337.89 1816.01,1311.16 1818.03,1327.05 1820.05,1317.16 1822.07,1322.8 1824.09,1333.89 1826.1,1317.47 1828.12,1332.34 \n",
       "  1830.14,1341.34 1832.16,1325.39 1834.18,1333.73 1836.19,1345.97 1838.21,1317.45 1840.23,1335.99 1842.25,1318.69 1844.27,1345.32 1846.28,1337.76 1848.3,1318.8 \n",
       "  1850.32,1338.19 1852.34,1340.43 1854.36,1348.25 1856.37,1336.46 1858.39,1328.11 1860.41,1323.14 1862.43,1336.87 1864.45,1334.94 1866.46,1340.27 1868.48,1332.3 \n",
       "  1870.5,1338.9 1872.52,1326.04 1874.54,1331.92 1876.55,1331.67 1878.57,1322.92 1880.59,1341.92 1882.61,1326.18 1884.63,1313.21 1886.65,1319.45 1888.66,1337.05 \n",
       "  1890.68,1342.05 1892.7,1329.59 1894.72,1342.38 1896.74,1332.09 1898.75,1331.16 1900.77,1310.43 1902.79,1331.59 1904.81,1339.49 1906.83,1354.42 1908.84,1338.71 \n",
       "  1910.86,1341.62 1912.88,1335.54 1914.9,1359.23 1916.92,1336.31 1918.93,1344.4 1920.95,1342.52 1922.97,1344.93 1924.99,1330.43 1927.01,1346.58 1929.02,1331.42 \n",
       "  1931.04,1347.59 1933.06,1330.31 1935.08,1330.65 1937.1,1321.37 1939.11,1350.02 1941.13,1324.92 1943.15,1344.76 1945.17,1340.61 1947.19,1334.83 1949.2,1318.95 \n",
       "  1951.22,1327.13 1953.24,1344.1 1955.26,1332.39 1957.28,1350.17 1959.3,1327.37 1961.31,1338.38 1963.33,1343.57 1965.35,1322.46 1967.37,1337.48 1969.39,1346.72 \n",
       "  1971.4,1342.22 1973.42,1352.91 1975.44,1327.81 1977.46,1338.15 1979.48,1357.2 1981.49,1337.14 1983.51,1345.8 1985.53,1351.05 1987.55,1339.97 1989.57,1323.63 \n",
       "  1991.58,1326.31 1993.6,1334.28 1995.62,1344.58 1997.64,1331.09 1999.66,1343.95 2001.67,1347.8 2003.69,1328.03 2005.71,1359.61 2007.73,1345.64 2009.75,1347.69 \n",
       "  2011.76,1335.04 2013.78,1344.89 2015.8,1340.35 2017.82,1347.19 2019.84,1324.94 2021.86,1334.16 2023.87,1353.57 2025.89,1339.87 2027.91,1353.5 2029.93,1349.05 \n",
       "  2031.95,1363.93 2033.96,1336.95 2035.98,1345.51 2038,1340.5 2040.02,1343.51 2042.04,1354.74 2044.05,1336.18 2046.07,1354.34 2048.09,1349.84 2050.11,1334.92 \n",
       "  2052.13,1356.29 2054.14,1343.31 2056.16,1360.88 2058.18,1345.6 2060.2,1345.65 2062.22,1349.92 2064.23,1346.14 2066.25,1323.01 2068.27,1338.14 2070.29,1368.26 \n",
       "  2072.31,1330.43 2074.32,1341.14 2076.34,1349.22 2078.36,1358.77 2080.38,1346.41 2082.4,1332.94 2084.41,1372.4 2086.43,1358.39 2088.45,1335.13 2090.47,1351.03 \n",
       "  2092.49,1353.22 2094.51,1357.33 2096.52,1355.69 2098.54,1337.52 2100.56,1345.63 2102.58,1352.5 2104.6,1352.42 2106.61,1359.07 2108.63,1343.14 2110.65,1362.3 \n",
       "  2112.67,1354.64 2114.69,1337.55 2116.7,1356.58 2118.72,1343.82 2120.74,1362.3 2122.76,1376.78 2124.78,1368.6 2126.79,1350.73 2128.81,1350.35 2130.83,1358.18 \n",
       "  2132.85,1360.55 2134.87,1347 2136.88,1344.89 2138.9,1339.23 2140.92,1353.62 2142.94,1355.28 2144.96,1370.92 2146.97,1332.54 2148.99,1375.3 2151.01,1359.8 \n",
       "  2153.03,1351.23 2155.05,1366.64 2157.06,1363.38 2159.08,1373.94 2161.1,1359.74 2163.12,1370.63 2165.14,1348.03 2167.16,1342.91 2169.17,1340.94 2171.19,1346.54 \n",
       "  2173.21,1376.24 2175.23,1382.62 2177.25,1328.36 2179.26,1362.29 2181.28,1346.09 2183.3,1367.06 2185.32,1341.4 2187.34,1358.36 2189.35,1349.8 2191.37,1347.77 \n",
       "  2193.39,1364.44 2195.41,1349.15 2197.43,1356.31 2199.44,1342.49 2201.46,1344.57 2203.48,1362.18 2205.5,1345.68 2207.52,1351.45 2209.53,1356.56 2211.55,1335.66 \n",
       "  2213.57,1355.8 2215.59,1357.39 2217.61,1356.37 2219.62,1358.55 2221.64,1361.83 2223.66,1353.45 2225.68,1364.43 2227.7,1348.16 2229.71,1367.01 2231.73,1345.45 \n",
       "  2233.75,1356.63 2235.77,1360.76 2237.79,1367.63 2239.81,1375.32 2241.82,1352.26 2243.84,1365.05 2245.86,1352.79 2247.88,1362.47 2249.9,1374.98 2251.91,1361.79 \n",
       "  2253.93,1361.7 2255.95,1346.68 2257.97,1350.09 2259.99,1358.24 2262,1362.24 2264.02,1363.92 2266.04,1367.18 2268.06,1373.73 2270.08,1377.39 2272.09,1344.24 \n",
       "  2274.11,1354.35 2276.13,1345.72 2278.15,1353.32 2280.17,1357.95 2282.18,1367.93 2284.2,1335.74 2286.22,1382.66 2288.24,1378.2 2290.26,1362.33 2292.27,1388.71 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip1200)\" d=\"\n",
       "M1989.93 326.155 L2280.76 326.155 L2280.76 205.195 L1989.93 205.195  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,326.155 2280.76,326.155 2280.76,205.195 1989.93,205.195 1989.93,326.155 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,265.675 2157.93,265.675 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 283.175)\" x=\"2181.93\" y=\"283.175\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_error_plot(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:06\u001b[39m\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip1600\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip1600)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip1601\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip1600)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 121.675 L215.754 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip1602\">\n",
       "    <rect x=\"215\" y=\"121\" width=\"2138\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1354.1 2352.76,1354.1 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1041.09 2352.76,1041.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,728.086 2352.76,728.086 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,415.078 2352.76,415.078 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  274.217,1425.62 274.217,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  778.732,1425.62 778.732,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1283.25,1425.62 1283.25,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1787.76,1425.62 1787.76,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1354.1 247.809,1354.1 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1041.09 247.809,1041.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,728.086 247.809,728.086 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,415.078 247.809,415.078 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 274.217, 1479.62)\" x=\"274.217\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 778.732, 1479.62)\" x=\"778.732\" y=\"1479.62\">250</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1283.25, 1479.62)\" x=\"1283.25\" y=\"1479.62\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1787.76, 1479.62)\" x=\"1787.76\" y=\"1479.62\">750</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2292.27, 1479.62)\" x=\"2292.27\" y=\"1479.62\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1371.6)\" x=\"191.754\" y=\"1371.6\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1058.59)\" x=\"191.754\" y=\"1058.59\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 745.586)\" x=\"191.754\" y=\"745.586\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 432.578)\" x=\"191.754\" y=\"432.578\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 73.2)\" x=\"1284.25\" y=\"73.2\">error with N=3 bits</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">gradient descent step</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip1602)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,158.579 278.253,160.706 280.271,196.143 282.289,207.197 284.307,208.757 286.326,214.338 288.344,194.841 290.362,239.438 292.38,218.211 294.398,200.938 \n",
       "  296.416,237.531 298.434,261.776 300.452,265.296 302.47,271.165 304.488,289.381 306.506,250.62 308.524,263.766 310.542,324.445 312.56,274.561 314.578,278.447 \n",
       "  316.596,318.385 318.614,296.139 320.633,345.412 322.651,268.927 324.669,304.484 326.687,312.196 328.705,339.337 330.723,351.953 332.741,344.833 334.759,345.493 \n",
       "  336.777,343.668 338.795,373.246 340.813,369.21 342.831,407.675 344.849,378.23 346.867,400.583 348.885,397.461 350.903,374.215 352.921,376.136 354.94,410.388 \n",
       "  356.958,424.998 358.976,382.261 360.994,413.877 363.012,419.241 365.03,440.491 367.048,439.841 369.066,438.569 371.084,461.086 373.102,432.7 375.12,473.461 \n",
       "  377.138,465.815 379.156,461.511 381.174,473.61 383.192,470.674 385.21,475.604 387.228,480.52 389.246,467.96 391.265,461.466 393.283,490.227 395.301,461.669 \n",
       "  397.319,495.952 399.337,487.635 401.355,515.939 403.373,511.213 405.391,532.21 407.409,518.437 409.427,548.574 411.445,540.482 413.463,548.25 415.481,568.496 \n",
       "  417.499,558.798 419.517,583.094 421.535,565.82 423.553,574.362 425.572,555.577 427.59,571.396 429.608,530.529 431.626,568.42 433.644,556.392 435.662,590.392 \n",
       "  437.68,599.904 439.698,581.024 441.716,605.936 443.734,618.887 445.752,607.458 447.77,611.446 449.788,616.896 451.806,620.721 453.824,621.281 455.842,604.711 \n",
       "  457.86,617.806 459.879,632.078 461.897,624.209 463.915,661.444 465.933,669.12 467.951,653.199 469.969,648.674 471.987,669.039 474.005,673.959 476.023,651.865 \n",
       "  478.041,667.458 480.059,666.586 482.077,697.899 484.095,680.171 486.113,697.897 488.131,681.306 490.149,700.915 492.167,680.145 494.185,714.58 496.204,705.838 \n",
       "  498.222,724.283 500.24,705.057 502.258,718.284 504.276,700.085 506.294,720.701 508.312,715.58 510.33,732.044 512.348,721.719 514.366,726.148 516.384,740.519 \n",
       "  518.402,742.003 520.42,756.49 522.438,723.664 524.456,739.211 526.474,735.217 528.492,762.044 530.511,738.242 532.529,751.078 534.547,747.604 536.565,772.433 \n",
       "  538.583,771.407 540.601,773.16 542.619,762.971 544.637,781.171 546.655,781.874 548.673,785.012 550.691,784.136 552.709,790.425 554.727,770.414 556.745,784.682 \n",
       "  558.763,782.882 560.781,789.531 562.799,771.707 564.817,807.453 566.836,785.355 568.854,794.3 570.872,786.625 572.89,823.986 574.908,808.199 576.926,823.028 \n",
       "  578.944,813.903 580.962,807.986 582.98,799.79 584.998,849.695 587.016,852.097 589.034,797.322 591.052,840.021 593.07,843.599 595.088,843.687 597.106,851.87 \n",
       "  599.124,848.884 601.143,842.116 603.161,846.617 605.179,862.675 607.197,849.964 609.215,869.546 611.233,841.063 613.251,862.736 615.269,857.017 617.287,819.265 \n",
       "  619.305,853.551 621.323,872.759 623.341,854.886 625.359,865.615 627.377,874.963 629.395,883.601 631.413,860.548 633.431,865.682 635.45,894.811 637.468,891.825 \n",
       "  639.486,872.523 641.504,901.139 643.522,884.452 645.54,920.813 647.558,896.144 649.576,893.639 651.594,871.803 653.612,902.487 655.63,874.482 657.648,901.323 \n",
       "  659.666,898.793 661.684,903.284 663.702,906.764 665.72,910.061 667.738,893.621 669.756,913.882 671.775,902.538 673.793,924.257 675.811,902.006 677.829,929.061 \n",
       "  679.847,930.218 681.865,934.311 683.883,950.063 685.901,918.213 687.919,938.522 689.937,938.141 691.955,945.607 693.973,956.726 695.991,917.545 698.009,913.937 \n",
       "  700.027,939.56 702.045,947.096 704.063,928.676 706.082,954.718 708.1,948.757 710.118,975.657 712.136,953.041 714.154,943.603 716.172,965.946 718.19,953.407 \n",
       "  720.208,942.293 722.226,971.264 724.244,981.616 726.262,976.113 728.28,995.498 730.298,983.064 732.316,980.335 734.334,966.177 736.352,970.669 738.37,982.331 \n",
       "  740.388,987.267 742.407,982.754 744.425,979.562 746.443,989.714 748.461,978.699 750.479,993.478 752.497,991.584 754.515,998.729 756.533,1012.32 758.551,994.947 \n",
       "  760.569,1021.16 762.587,1020.61 764.605,1006.31 766.623,1012.38 768.641,1014.83 770.659,1016.53 772.677,1007.35 774.695,1010.55 776.714,1002.59 778.732,1018.1 \n",
       "  780.75,1006.39 782.768,1014.81 784.786,1006.02 786.804,1008.64 788.822,1010.21 790.84,1026.14 792.858,1024.9 794.876,1012.07 796.894,1034.04 798.912,1024.25 \n",
       "  800.93,1027.57 802.948,1049.59 804.966,1032.34 806.984,1023.52 809.002,1019.57 811.021,1043.72 813.039,1015.19 815.057,1042.27 817.075,1033.77 819.093,1042.39 \n",
       "  821.111,1040.45 823.129,1052.28 825.147,1044.76 827.165,1061.96 829.183,1047.73 831.201,1044.86 833.219,1016.2 835.237,1038.7 837.255,1041.24 839.273,1072.19 \n",
       "  841.291,1045.64 843.309,1050.71 845.327,1059.7 847.346,1074.61 849.364,1054.53 851.382,1057.82 853.4,1063.34 855.418,1068.74 857.436,1064.82 859.454,1072.87 \n",
       "  861.472,1068.72 863.49,1068.32 865.508,1077.49 867.526,1048.23 869.544,1075.04 871.562,1051.03 873.58,1070.97 875.598,1064.75 877.616,1085.66 879.634,1064.42 \n",
       "  881.653,1086.86 883.671,1085.97 885.689,1091.24 887.707,1078.25 889.725,1070.9 891.743,1089.04 893.761,1076.63 895.779,1081.15 897.797,1091.69 899.815,1080.64 \n",
       "  901.833,1096.56 903.851,1072.72 905.869,1091.7 907.887,1088.11 909.905,1101.92 911.923,1117.47 913.941,1092.53 915.959,1089.85 917.978,1115.89 919.996,1096.61 \n",
       "  922.014,1101.66 924.032,1102.03 926.05,1106.63 928.068,1109.46 930.086,1117.77 932.104,1102.21 934.122,1108.12 936.14,1087.93 938.158,1112.75 940.176,1115.62 \n",
       "  942.194,1122 944.212,1105.43 946.23,1111.46 948.248,1118.68 950.266,1108.36 952.285,1120.58 954.303,1121.14 956.321,1115.53 958.339,1107.82 960.357,1099.61 \n",
       "  962.375,1117.73 964.393,1101.57 966.411,1104.19 968.429,1116.36 970.447,1125.5 972.465,1123.11 974.483,1110.87 976.501,1122.44 978.519,1136.27 980.537,1116.24 \n",
       "  982.555,1123.2 984.573,1127.42 986.592,1134.03 988.61,1126.87 990.628,1155.5 992.646,1124.82 994.664,1131.36 996.682,1121.87 998.7,1122.04 1000.72,1157.96 \n",
       "  1002.74,1146.64 1004.75,1152.01 1006.77,1128.52 1008.79,1152.81 1010.81,1156.49 1012.83,1136.29 1014.84,1125.47 1016.86,1124.89 1018.88,1133.65 1020.9,1151.17 \n",
       "  1022.92,1151.2 1024.93,1149.9 1026.95,1143.95 1028.97,1162.91 1030.99,1135.49 1033.01,1153.76 1035.02,1146.99 1037.04,1159.19 1039.06,1153.94 1041.08,1160.51 \n",
       "  1043.1,1170.24 1045.12,1153.56 1047.13,1145.13 1049.15,1162.66 1051.17,1150.39 1053.19,1167.32 1055.21,1157.27 1057.22,1158.49 1059.24,1163.54 1061.26,1155.38 \n",
       "  1063.28,1161.4 1065.3,1165.88 1067.31,1182.05 1069.33,1172.6 1071.35,1157.18 1073.37,1147.21 1075.39,1156.08 1077.4,1195.56 1079.42,1170.68 1081.44,1167.83 \n",
       "  1083.46,1177.49 1085.48,1167.4 1087.49,1161.5 1089.51,1175.48 1091.53,1157.67 1093.55,1174.54 1095.57,1176.67 1097.58,1178.11 1099.6,1184.37 1101.62,1166.19 \n",
       "  1103.64,1171.22 1105.66,1174.35 1107.67,1188.49 1109.69,1157.32 1111.71,1185.13 1113.73,1170.6 1115.75,1186.77 1117.77,1173.61 1119.78,1175.96 1121.8,1187.71 \n",
       "  1123.82,1187.33 1125.84,1178.25 1127.86,1182.53 1129.87,1190.82 1131.89,1171.49 1133.91,1202.8 1135.93,1185.97 1137.95,1177.46 1139.96,1184.66 1141.98,1203.87 \n",
       "  1144,1182.68 1146.02,1194.31 1148.04,1194.86 1150.05,1204.01 1152.07,1192.98 1154.09,1191.86 1156.11,1182.85 1158.13,1208.11 1160.14,1201.72 1162.16,1217.39 \n",
       "  1164.18,1201.82 1166.2,1199.5 1168.22,1195.7 1170.23,1200.85 1172.25,1196.68 1174.27,1204.33 1176.29,1216.62 1178.31,1210.45 1180.33,1193.02 1182.34,1203.51 \n",
       "  1184.36,1213.44 1186.38,1204.55 1188.4,1209.05 1190.42,1195.65 1192.43,1204.68 1194.45,1191.62 1196.47,1202.81 1198.49,1201.17 1200.51,1222.98 1202.52,1207.74 \n",
       "  1204.54,1206.72 1206.56,1219.01 1208.58,1218.13 1210.6,1214.43 1212.61,1214.01 1214.63,1200.31 1216.65,1209.48 1218.67,1222.05 1220.69,1218.96 1222.7,1224.06 \n",
       "  1224.72,1211.01 1226.74,1225.35 1228.76,1219.78 1230.78,1217.07 1232.79,1230.99 1234.81,1219.1 1236.83,1206.84 1238.85,1209.36 1240.87,1216.06 1242.88,1231.12 \n",
       "  1244.9,1237.05 1246.92,1200.16 1248.94,1225.49 1250.96,1208.35 1252.98,1225.24 1254.99,1232.15 1257.01,1241.14 1259.03,1210.91 1261.05,1231.56 1263.07,1244.81 \n",
       "  1265.08,1237.76 1267.1,1253.35 1269.12,1228.09 1271.14,1221.32 1273.16,1233.24 1275.17,1240.28 1277.19,1229.3 1279.21,1239.54 1281.23,1241.4 1283.25,1237.34 \n",
       "  1285.26,1230.19 1287.28,1245.02 1289.3,1219.99 1291.32,1246.25 1293.34,1241.27 1295.35,1236.83 1297.37,1242.84 1299.39,1243.99 1301.41,1241.87 1303.43,1252.12 \n",
       "  1305.44,1252.1 1307.46,1229.9 1309.48,1240.83 1311.5,1226.48 1313.52,1256.09 1315.53,1242.75 1317.55,1250.36 1319.57,1240.41 1321.59,1255.11 1323.61,1244.68 \n",
       "  1325.63,1249.41 1327.64,1252.71 1329.66,1238.39 1331.68,1249.99 1333.7,1245.94 1335.72,1242.63 1337.73,1225.3 1339.75,1247.06 1341.77,1254.59 1343.79,1257.89 \n",
       "  1345.81,1238.37 1347.82,1260.36 1349.84,1275.83 1351.86,1261.23 1353.88,1231.41 1355.9,1253.76 1357.91,1235.08 1359.93,1254.42 1361.95,1239.27 1363.97,1258.48 \n",
       "  1365.99,1241.47 1368,1264.86 1370.02,1266.12 1372.04,1269.17 1374.06,1266.29 1376.08,1257.27 1378.09,1257.17 1380.11,1243.04 1382.13,1266.9 1384.15,1260.92 \n",
       "  1386.17,1263.9 1388.18,1260.47 1390.2,1254.29 1392.22,1267.08 1394.24,1260.3 1396.26,1244.41 1398.28,1248.24 1400.29,1267.11 1402.31,1268.87 1404.33,1266.6 \n",
       "  1406.35,1247.21 1408.37,1274.83 1410.38,1274.21 1412.4,1258.77 1414.42,1265.32 1416.44,1267.54 1418.46,1272.86 1420.47,1272.03 1422.49,1287.75 1424.51,1275.68 \n",
       "  1426.53,1269.32 1428.55,1267.37 1430.56,1276.12 1432.58,1268.35 1434.6,1275.68 1436.62,1269.1 1438.64,1279.81 1440.65,1300.45 1442.67,1272.85 1444.69,1290.86 \n",
       "  1446.71,1282.99 1448.73,1278.08 1450.74,1273.42 1452.76,1268.38 1454.78,1271.6 1456.8,1282.22 1458.82,1290.99 1460.84,1287.76 1462.85,1264.82 1464.87,1272.93 \n",
       "  1466.89,1282.82 1468.91,1274.7 1470.93,1275.95 1472.94,1304.37 1474.96,1285.71 1476.98,1270.21 1479,1284.85 1481.02,1280.7 1483.03,1269.95 1485.05,1265.2 \n",
       "  1487.07,1292.9 1489.09,1280.52 1491.11,1277.32 1493.12,1291.05 1495.14,1283.21 1497.16,1288.26 1499.18,1290.19 1501.2,1296.37 1503.21,1285.62 1505.23,1277.42 \n",
       "  1507.25,1290.82 1509.27,1285.35 1511.29,1290.61 1513.3,1294.73 1515.32,1289.69 1517.34,1278.14 1519.36,1286.19 1521.38,1287.5 1523.39,1286.11 1525.41,1288.14 \n",
       "  1527.43,1304.6 1529.45,1303.5 1531.47,1287.81 1533.49,1289.95 1535.5,1277.22 1537.52,1289.27 1539.54,1306.24 1541.56,1295.75 1543.58,1305.76 1545.59,1290.19 \n",
       "  1547.61,1286.8 1549.63,1286.29 1551.65,1295.39 1553.67,1288.6 1555.68,1281.63 1557.7,1291.73 1559.72,1303.23 1561.74,1281.85 1563.76,1271.42 1565.77,1297.53 \n",
       "  1567.79,1286.12 1569.81,1311.11 1571.83,1304.09 1573.85,1317.38 1575.86,1289.42 1577.88,1300.18 1579.9,1298.51 1581.92,1290.99 1583.94,1293.85 1585.95,1305.79 \n",
       "  1587.97,1308.18 1589.99,1301.28 1592.01,1307.39 1594.03,1293.82 1596.04,1304.32 1598.06,1304.51 1600.08,1300.59 1602.1,1282.27 1604.12,1293.08 1606.14,1313.85 \n",
       "  1608.15,1318.07 1610.17,1313.77 1612.19,1302.01 1614.21,1302.39 1616.23,1307.1 1618.24,1327.85 1620.26,1297.6 1622.28,1291.57 1624.3,1299.29 1626.32,1297.36 \n",
       "  1628.33,1306.38 1630.35,1321.43 1632.37,1299.01 1634.39,1326.46 1636.41,1316.35 1638.42,1323.01 1640.44,1307.37 1642.46,1320.25 1644.48,1304.4 1646.5,1304.85 \n",
       "  1648.51,1304.45 1650.53,1312.83 1652.55,1318.07 1654.57,1299.1 1656.59,1313.67 1658.6,1317.46 1660.62,1291.52 1662.64,1316.27 1664.66,1313.54 1666.68,1293.02 \n",
       "  1668.69,1320.38 1670.71,1314.89 1672.73,1321.83 1674.75,1295.07 1676.77,1295.87 1678.79,1325.95 1680.8,1298.17 1682.82,1326.28 1684.84,1319.9 1686.86,1299.6 \n",
       "  1688.88,1326.97 1690.89,1336.37 1692.91,1311.88 1694.93,1330.59 1696.95,1322.81 1698.97,1337.3 1700.98,1317.94 1703,1328.28 1705.02,1319.88 1707.04,1304.04 \n",
       "  1709.06,1325.65 1711.07,1294.69 1713.09,1306 1715.11,1321.12 1717.13,1307.88 1719.15,1323.27 1721.16,1321.52 1723.18,1320.06 1725.2,1335.94 1727.22,1325.86 \n",
       "  1729.24,1330.08 1731.25,1332.81 1733.27,1310.34 1735.29,1322.63 1737.31,1317.5 1739.33,1323.78 1741.35,1332.35 1743.36,1337.37 1745.38,1330.19 1747.4,1313.33 \n",
       "  1749.42,1313.99 1751.44,1338.71 1753.45,1313.6 1755.47,1326.58 1757.49,1313.82 1759.51,1309.87 1761.53,1343.51 1763.54,1331.88 1765.56,1334.06 1767.58,1340.59 \n",
       "  1769.6,1306.55 1771.62,1326.3 1773.63,1304.03 1775.65,1324.6 1777.67,1336.43 1779.69,1327.02 1781.71,1316.69 1783.72,1318.27 1785.74,1342.99 1787.76,1323.64 \n",
       "  1789.78,1304.05 1791.8,1329.16 1793.81,1334.87 1795.83,1340.18 1797.85,1325.96 1799.87,1347.02 1801.89,1345.26 1803.9,1330.67 1805.92,1345.15 1807.94,1340.15 \n",
       "  1809.96,1333.69 1811.98,1336.11 1814,1340.21 1816.01,1325.19 1818.03,1341.03 1820.05,1330.65 1822.07,1340.02 1824.09,1329.29 1826.1,1304.27 1828.12,1331.61 \n",
       "  1830.14,1329.46 1832.16,1329.65 1834.18,1344.84 1836.19,1322.74 1838.21,1334.06 1840.23,1348.75 1842.25,1329.76 1844.27,1344.38 1846.28,1323.39 1848.3,1344.72 \n",
       "  1850.32,1341.66 1852.34,1349.72 1854.36,1322.5 1856.37,1325.86 1858.39,1344.56 1860.41,1336.6 1862.43,1339.45 1864.45,1335.72 1866.46,1337.97 1868.48,1336.86 \n",
       "  1870.5,1318.77 1872.52,1334.07 1874.54,1344.03 1876.55,1340.8 1878.57,1336.62 1880.59,1333.81 1882.61,1338 1884.63,1345.58 1886.65,1347.56 1888.66,1341.07 \n",
       "  1890.68,1337.43 1892.7,1323.51 1894.72,1370.43 1896.74,1322.59 1898.75,1348.76 1900.77,1324.62 1902.79,1337.57 1904.81,1331.41 1906.83,1329.37 1908.84,1343.71 \n",
       "  1910.86,1357.49 1912.88,1318.24 1914.9,1357.87 1916.92,1346.67 1918.93,1326.02 1920.95,1334.66 1922.97,1359.45 1924.99,1350.85 1927.01,1348.07 1929.02,1347.03 \n",
       "  1931.04,1351.06 1933.06,1343.3 1935.08,1325.96 1937.1,1358.29 1939.11,1340.08 1941.13,1334.02 1943.15,1343.66 1945.17,1349.58 1947.19,1353.21 1949.2,1348.53 \n",
       "  1951.22,1310.19 1953.24,1333.21 1955.26,1347.07 1957.28,1368.98 1959.3,1339.89 1961.31,1338.57 1963.33,1341.59 1965.35,1352.33 1967.37,1360.32 1969.39,1352.07 \n",
       "  1971.4,1357.82 1973.42,1359.6 1975.44,1352.19 1977.46,1355.29 1979.48,1342.32 1981.49,1350.72 1983.51,1348.54 1985.53,1355.42 1987.55,1348.31 1989.57,1328.5 \n",
       "  1991.58,1353.12 1993.6,1361.75 1995.62,1345.38 1997.64,1354.68 1999.66,1353.78 2001.67,1323.73 2003.69,1361.48 2005.71,1351.49 2007.73,1334.27 2009.75,1372.98 \n",
       "  2011.76,1369.48 2013.78,1362 2015.8,1365.02 2017.82,1351.45 2019.84,1344.85 2021.86,1361.4 2023.87,1370.29 2025.89,1361.51 2027.91,1342.16 2029.93,1364.48 \n",
       "  2031.95,1352.94 2033.96,1357.3 2035.98,1369.87 2038,1350.39 2040.02,1357.75 2042.04,1346.7 2044.05,1333.16 2046.07,1355.6 2048.09,1352.98 2050.11,1342.98 \n",
       "  2052.13,1364.9 2054.14,1347.42 2056.16,1368.45 2058.18,1356.69 2060.2,1375.69 2062.22,1361.43 2064.23,1354.95 2066.25,1352.91 2068.27,1355.89 2070.29,1359 \n",
       "  2072.31,1342.42 2074.32,1360.83 2076.34,1362.21 2078.36,1365.69 2080.38,1357.66 2082.4,1364.21 2084.41,1374.12 2086.43,1374.47 2088.45,1370.16 2090.47,1364.18 \n",
       "  2092.49,1351.65 2094.51,1358.49 2096.52,1344 2098.54,1348.59 2100.56,1354.39 2102.58,1352.43 2104.6,1349.99 2106.61,1344.57 2108.63,1348.69 2110.65,1355.85 \n",
       "  2112.67,1346.24 2114.69,1348.67 2116.7,1377.09 2118.72,1356.38 2120.74,1351.04 2122.76,1349.19 2124.78,1356.63 2126.79,1364.23 2128.81,1365.26 2130.83,1344.57 \n",
       "  2132.85,1350.9 2134.87,1355.01 2136.88,1365.11 2138.9,1364.74 2140.92,1339.24 2142.94,1343.53 2144.96,1351.28 2146.97,1358.46 2148.99,1360.53 2151.01,1354.79 \n",
       "  2153.03,1362.74 2155.05,1358.51 2157.06,1344.84 2159.08,1363.94 2161.1,1343.4 2163.12,1374.12 2165.14,1337.43 2167.16,1358.51 2169.17,1336.42 2171.19,1349.51 \n",
       "  2173.21,1364.05 2175.23,1358.07 2177.25,1357.43 2179.26,1352.75 2181.28,1369.13 2183.3,1355.28 2185.32,1349.73 2187.34,1352.76 2189.35,1335.33 2191.37,1358.48 \n",
       "  2193.39,1345.07 2195.41,1353.98 2197.43,1363.35 2199.44,1382.06 2201.46,1348.68 2203.48,1365 2205.5,1366.45 2207.52,1357.81 2209.53,1376.44 2211.55,1359.87 \n",
       "  2213.57,1367.61 2215.59,1364.58 2217.61,1367.47 2219.62,1363.15 2221.64,1357.91 2223.66,1354.54 2225.68,1362.23 2227.7,1360.07 2229.71,1357.75 2231.73,1359.51 \n",
       "  2233.75,1368.29 2235.77,1370.85 2237.79,1388.71 2239.81,1375.43 2241.82,1358.51 2243.84,1368.64 2245.86,1356.83 2247.88,1367.95 2249.9,1365.81 2251.91,1362 \n",
       "  2253.93,1377.83 2255.95,1374.52 2257.97,1364.17 2259.99,1368.03 2262,1354.85 2264.02,1353.35 2266.04,1337.86 2268.06,1358.85 2270.08,1377.25 2272.09,1380.15 \n",
       "  2274.11,1360.51 2276.13,1346.01 2278.15,1384.73 2280.17,1368.46 2282.18,1366.14 2284.2,1369.18 2286.22,1363.4 2288.24,1387.17 2290.26,1385.66 2292.27,1381.77 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip1600)\" d=\"\n",
       "M1989.93 326.155 L2280.76 326.155 L2280.76 205.195 L1989.93 205.195  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,326.155 2280.76,326.155 2280.76,205.195 1989.93,205.195 1989.93,326.155 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1600)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,265.675 2157.93,265.675 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip1600)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 283.175)\" x=\"2181.93\" y=\"283.175\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_error_plot(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip2000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2001\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 121.675 L215.754 121.675  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2002\">\n",
       "    <rect x=\"215\" y=\"121\" width=\"2138\" height=\"1305\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  275.227,1425.62 275.227,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  779.489,1425.62 779.489,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1283.75,1425.62 1283.75,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1788.01,1425.62 1788.01,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1331.02 2352.76,1331.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1022.27 2352.76,1022.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,713.515 2352.76,713.515 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,404.762 2352.76,404.762 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,121.675 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  275.227,1425.62 275.227,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  779.489,1425.62 779.489,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1283.75,1425.62 1283.75,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1788.01,1425.62 1788.01,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,1406.06 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1331.02 247.809,1331.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1022.27 247.809,1022.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,713.515 247.809,713.515 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,404.762 247.809,404.762 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 275.227, 1479.62)\" x=\"275.227\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 779.489, 1479.62)\" x=\"779.489\" y=\"1479.62\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1283.75, 1479.62)\" x=\"1283.75\" y=\"1479.62\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1788.01, 1479.62)\" x=\"1788.01\" y=\"1479.62\">1500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2292.27, 1479.62)\" x=\"2292.27\" y=\"1479.62\">2000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1348.52)\" x=\"191.754\" y=\"1348.52\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1039.77)\" x=\"191.754\" y=\"1039.77\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 731.015)\" x=\"191.754\" y=\"731.015\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 422.262)\" x=\"191.754\" y=\"422.262\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 73.2)\" x=\"1284.25\" y=\"73.2\">error with N=3 bits</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">gradient descent step</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 773.647)\" x=\"89.2861\" y=\"773.647\">MSE</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,191.307 277.244,158.579 278.252,217.009 279.261,170.817 280.269,202.658 281.278,196.355 282.286,201.517 283.295,229.186 284.303,215.109 285.312,214.758 \n",
       "  286.321,265.342 287.329,256.936 288.338,256.307 289.346,245.293 290.355,291.272 291.363,264.632 292.372,263.171 293.38,262.862 294.389,280.765 295.397,301.854 \n",
       "  296.406,275.01 297.414,306.856 298.423,278.025 299.431,319.823 300.44,318.854 301.448,301.989 302.457,351.56 303.465,338.474 304.474,336.106 305.482,341.812 \n",
       "  306.491,340.109 307.5,348.885 308.508,352.498 309.517,356.238 310.525,378.688 311.534,377.063 312.542,392.911 313.551,367.374 314.559,366.097 315.568,386.286 \n",
       "  316.576,406.149 317.585,407.473 318.593,427.899 319.602,402.759 320.61,425.309 321.619,429.253 322.627,433.121 323.636,423.9 324.644,409.807 325.653,428.316 \n",
       "  326.661,442.161 327.67,453.435 328.679,487.362 329.687,452.986 330.696,478.382 331.704,457.478 332.713,484.375 333.721,510.608 334.73,501.596 335.738,513.183 \n",
       "  336.747,498.492 337.755,481.633 338.764,529.407 339.772,488.024 340.781,518.104 341.789,489.411 342.798,534.05 343.806,499.873 344.815,526.479 345.823,540.216 \n",
       "  346.832,573.437 347.84,558.124 348.849,558.82 349.858,548.801 350.866,542.397 351.875,543.681 352.883,533.62 353.892,561.876 354.9,593.057 355.909,580.783 \n",
       "  356.917,592.901 357.926,566.513 358.934,594.982 359.943,570.61 360.951,583.748 361.96,587.174 362.968,623.389 363.977,589.597 364.985,613.637 365.994,620.066 \n",
       "  367.002,613.607 368.011,630.917 369.019,590.268 370.028,607.017 371.037,616.294 372.045,611.925 373.054,644.386 374.062,641.786 375.071,667.046 376.079,676.459 \n",
       "  377.088,667.57 378.096,639.638 379.105,673.063 380.113,673.408 381.122,644.217 382.13,661.814 383.139,651.082 384.147,659.807 385.156,685.809 386.164,684.047 \n",
       "  387.173,700.716 388.181,665.966 389.19,689.104 390.198,689.788 391.207,707.152 392.216,680.716 393.224,718.826 394.233,714.52 395.241,720.744 396.25,682.111 \n",
       "  397.258,714.319 398.267,712.507 399.275,737.747 400.284,761.613 401.292,718.466 402.301,751.057 403.309,733.611 404.318,737.88 405.326,744.148 406.335,741.821 \n",
       "  407.343,750.074 408.352,746.218 409.36,754.623 410.369,769.252 411.377,766.848 412.386,785.136 413.395,744.699 414.403,750.047 415.412,781.7 416.42,780.948 \n",
       "  417.429,790.036 418.437,779.269 419.446,782.459 420.454,786.99 421.463,767.122 422.471,785.479 423.48,802.04 424.488,796.995 425.497,804.721 426.505,777.717 \n",
       "  427.514,793.547 428.522,822.541 429.531,805.145 430.539,839.664 431.548,814.844 432.556,843.174 433.565,813.987 434.574,835.576 435.582,824.323 436.591,824.539 \n",
       "  437.599,841.49 438.608,828 439.616,834.462 440.625,839.051 441.633,845.806 442.642,824.613 443.65,830.595 444.659,815.028 445.667,844.095 446.676,824.054 \n",
       "  447.684,836.5 448.693,845.649 449.701,866.045 450.71,838.193 451.718,861.188 452.727,861.186 453.735,872.616 454.744,875.227 455.753,854.334 456.761,852.044 \n",
       "  457.77,886.61 458.778,863.207 459.787,855.017 460.795,871.694 461.804,903.796 462.812,859.461 463.821,871.937 464.829,902.836 465.838,877.17 466.846,890.255 \n",
       "  467.855,895.965 468.863,887.367 469.872,896.009 470.88,884.172 471.889,917.192 472.897,889.311 473.906,887.99 474.914,908.13 475.923,910.806 476.932,912.895 \n",
       "  477.94,924.436 478.949,920.13 479.957,907.563 480.966,924.166 481.974,913.845 482.983,928.447 483.991,915.902 485,913.824 486.008,940.265 487.017,934.531 \n",
       "  488.025,911.292 489.034,933.55 490.042,935.585 491.051,945.86 492.059,939.917 493.068,926.354 494.076,947.665 495.085,941.802 496.093,949.418 497.102,953.268 \n",
       "  498.111,949.613 499.119,942.972 500.128,932.995 501.136,931.359 502.145,954.153 503.153,950.958 504.162,964.617 505.17,954.272 506.179,966.519 507.187,936.28 \n",
       "  508.196,969.632 509.204,963.109 510.213,975.594 511.221,963.42 512.23,997.201 513.238,969.404 514.247,984.814 515.255,958.646 516.264,985.822 517.273,973.085 \n",
       "  518.281,986.051 519.29,993.104 520.298,985.772 521.307,993.511 522.315,986.403 523.324,983.299 524.332,985.909 525.341,1005.74 526.349,984.749 527.358,997.087 \n",
       "  528.366,979.731 529.375,985.559 530.383,992.499 531.392,1003.43 532.4,1009.36 533.409,996.414 534.417,1005.37 535.426,1013.2 536.434,1000.92 537.443,1000.74 \n",
       "  538.452,1011.51 539.46,1013.04 540.469,1007.79 541.477,993.973 542.486,1011.95 543.494,1017.74 544.503,1017.43 545.511,1011.67 546.52,999.033 547.528,1030.8 \n",
       "  548.537,1013.12 549.545,1034.4 550.554,1030.84 551.562,1023.72 552.571,1014.13 553.579,1027.77 554.588,1040.58 555.596,1021.14 556.605,1038.89 557.613,1032.93 \n",
       "  558.622,1021.28 559.631,1032.16 560.639,1038.01 561.648,1034.22 562.656,1041.39 563.665,1030.86 564.673,1051.3 565.682,1054.19 566.69,1058.63 567.699,1058.13 \n",
       "  568.707,1055.18 569.716,1054.79 570.724,1060.57 571.733,1040.06 572.741,1049.24 573.75,1053.58 574.758,1076.54 575.767,1067.68 576.775,1044.43 577.784,1075.12 \n",
       "  578.792,1053.5 579.801,1058.2 580.81,1044.29 581.818,1061.47 582.827,1075.27 583.835,1072.26 584.844,1067.16 585.852,1065.71 586.861,1084.76 587.869,1071 \n",
       "  588.878,1072.35 589.886,1076.95 590.895,1064.78 591.903,1086.41 592.912,1062.35 593.92,1067.17 594.929,1086.44 595.937,1077.35 596.946,1083.28 597.954,1059.9 \n",
       "  598.963,1082.44 599.971,1082.78 600.98,1081.64 601.989,1081.38 602.997,1083.03 604.006,1073.18 605.014,1082.25 606.023,1091.13 607.031,1100.16 608.04,1084.09 \n",
       "  609.048,1106.88 610.057,1097.76 611.065,1091.43 612.074,1080.86 613.082,1087.18 614.091,1115.76 615.099,1102.24 616.108,1096.61 617.116,1104.6 618.125,1094.52 \n",
       "  619.133,1100.29 620.142,1096.4 621.15,1095.64 622.159,1112.35 623.168,1123.86 624.176,1087.29 625.185,1100.87 626.193,1099.2 627.202,1105.97 628.21,1101.24 \n",
       "  629.219,1111.63 630.227,1108.67 631.236,1116.96 632.244,1110.08 633.253,1124.6 634.261,1111.63 635.27,1121.84 636.278,1132.89 637.287,1111.01 638.295,1125.31 \n",
       "  639.304,1134.6 640.312,1106.89 641.321,1115.32 642.329,1114.71 643.338,1132.06 644.347,1098.17 645.355,1133.31 646.364,1115.29 647.372,1121.68 648.381,1127.05 \n",
       "  649.389,1139.79 650.398,1136.22 651.406,1124.1 652.415,1134.27 653.423,1120.64 654.432,1131.23 655.44,1135.8 656.449,1136.1 657.457,1140.25 658.466,1109.01 \n",
       "  659.474,1135.57 660.483,1132.65 661.491,1148.38 662.5,1128.06 663.508,1150.31 664.517,1138.01 665.526,1130.03 666.534,1141.8 667.543,1133.38 668.551,1146.79 \n",
       "  669.56,1130.45 670.568,1137.45 671.577,1149.19 672.585,1139.57 673.594,1157.71 674.602,1145.8 675.611,1142.75 676.619,1155.54 677.628,1140.23 678.636,1157.56 \n",
       "  679.645,1139.85 680.653,1142.68 681.662,1151.4 682.67,1150.59 683.679,1172.75 684.687,1137.74 685.696,1151.59 686.705,1139.22 687.713,1156.79 688.722,1170.54 \n",
       "  689.73,1162.5 690.739,1157.43 691.747,1152.54 692.756,1151.59 693.764,1152.21 694.773,1158.87 695.781,1168.16 696.79,1167.96 697.798,1150.68 698.807,1154.59 \n",
       "  699.815,1180.27 700.824,1152.85 701.832,1164.2 702.841,1173.78 703.849,1168.8 704.858,1174.57 705.866,1162.46 706.875,1174.67 707.884,1183.12 708.892,1153.48 \n",
       "  709.901,1169.01 710.909,1167.7 711.918,1171.86 712.926,1177.56 713.935,1170.31 714.943,1167.87 715.952,1194.88 716.96,1173.78 717.969,1177.22 718.977,1190.35 \n",
       "  719.986,1186.53 720.994,1178.14 722.003,1170.78 723.011,1168.99 724.02,1184.26 725.028,1168.99 726.037,1193.56 727.045,1168.71 728.054,1197.68 729.063,1190.59 \n",
       "  730.071,1185.94 731.08,1174.97 732.088,1187.51 733.097,1178.98 734.105,1189.06 735.114,1201.78 736.122,1197.09 737.131,1182.25 738.139,1188.36 739.148,1201.36 \n",
       "  740.156,1201.1 741.165,1195.04 742.173,1203.24 743.182,1199.35 744.19,1203.4 745.199,1203.73 746.207,1191.92 747.216,1197.09 748.224,1195.64 749.233,1206.99 \n",
       "  750.242,1207.95 751.25,1194.9 752.259,1191.96 753.267,1204.51 754.276,1193.42 755.284,1185.24 756.293,1206.73 757.301,1181.89 758.31,1195.52 759.318,1183.45 \n",
       "  760.327,1203.22 761.335,1211 762.344,1207.03 763.352,1200.91 764.361,1192.79 765.369,1203.31 766.378,1204.81 767.386,1203.21 768.395,1202.86 769.403,1211.23 \n",
       "  770.412,1208.33 771.421,1216.1 772.429,1212.25 773.438,1218.08 774.446,1218.43 775.455,1192.41 776.463,1207.08 777.472,1220.14 778.48,1212.13 779.489,1226 \n",
       "  780.497,1224.29 781.506,1221.43 782.514,1210.98 783.523,1214.12 784.531,1226.3 785.54,1210.49 786.548,1215.54 787.557,1223.14 788.565,1212.99 789.574,1236.96 \n",
       "  790.583,1225.86 791.591,1205.3 792.6,1214.05 793.608,1217.87 794.617,1215.97 795.625,1230.01 796.634,1210.46 797.642,1231.4 798.651,1214.61 799.659,1241.05 \n",
       "  800.668,1229.7 801.676,1230.06 802.685,1230.9 803.693,1223.31 804.702,1214.85 805.71,1219.02 806.719,1237.14 807.727,1232.22 808.736,1234.11 809.744,1229.79 \n",
       "  810.753,1235.78 811.762,1232.26 812.77,1222.91 813.779,1249.45 814.787,1237.64 815.796,1239.6 816.804,1227.67 817.813,1216.35 818.821,1219.37 819.83,1234.29 \n",
       "  820.838,1227.94 821.847,1227.27 822.855,1227.06 823.864,1241.53 824.872,1236.8 825.881,1221.07 826.889,1244.81 827.898,1246.33 828.906,1230.28 829.915,1235.14 \n",
       "  830.923,1240.72 831.932,1237.12 832.941,1243.6 833.949,1235.26 834.958,1233.16 835.966,1249.25 836.975,1226.86 837.983,1241.04 838.992,1250.08 840,1244.12 \n",
       "  841.009,1228.98 842.017,1237.04 843.026,1247.77 844.034,1234.01 845.043,1247.5 846.051,1237.88 847.06,1264.4 848.068,1257.35 849.077,1255.35 850.085,1239.99 \n",
       "  851.094,1257.45 852.102,1255.88 853.111,1244.65 854.12,1257.86 855.128,1259.25 856.137,1256.15 857.145,1254.52 858.154,1248.03 859.162,1247.12 860.171,1227.3 \n",
       "  861.179,1252.63 862.188,1258.5 863.196,1253.13 864.205,1240.04 865.213,1248.18 866.222,1236.21 867.23,1256.98 868.239,1266.05 869.247,1253.05 870.256,1268.08 \n",
       "  871.264,1261 872.273,1255.36 873.281,1250.84 874.29,1257.09 875.299,1269.92 876.307,1272.69 877.316,1260.92 878.324,1267.24 879.333,1269.15 880.341,1261.57 \n",
       "  881.35,1255.77 882.358,1233.23 883.367,1248.52 884.375,1256.99 885.384,1247.27 886.392,1264.45 887.401,1265.85 888.409,1256.48 889.418,1273.48 890.426,1259.56 \n",
       "  891.435,1256.16 892.443,1262.45 893.452,1263.64 894.46,1267.67 895.469,1267.98 896.478,1264.08 897.486,1254.68 898.495,1269.36 899.503,1275.18 900.512,1271.96 \n",
       "  901.52,1248.46 902.529,1268.44 903.537,1266.71 904.546,1249.91 905.554,1272.15 906.563,1265.82 907.571,1257.99 908.58,1281.99 909.588,1261.14 910.597,1273.78 \n",
       "  911.605,1279.54 912.614,1284.34 913.622,1261.44 914.631,1286.67 915.639,1266.41 916.648,1260.81 917.657,1285.09 918.665,1272.57 919.674,1268.43 920.682,1270.44 \n",
       "  921.691,1268.5 922.699,1278.08 923.708,1284.89 924.716,1273.49 925.725,1279.26 926.733,1281.96 927.742,1286.14 928.75,1271.17 929.759,1282.56 930.767,1284.82 \n",
       "  931.776,1285.61 932.784,1275.62 933.793,1276.39 934.801,1290.6 935.81,1263.27 936.818,1274.21 937.827,1291.42 938.836,1287.43 939.844,1291.59 940.853,1285.71 \n",
       "  941.861,1289.45 942.87,1285.16 943.878,1261.61 944.887,1288.38 945.895,1291.72 946.904,1283.1 947.912,1287.02 948.921,1291.27 949.929,1271.52 950.938,1300.67 \n",
       "  951.946,1276.21 952.955,1294.53 953.963,1259.15 954.972,1285.65 955.98,1296.54 956.989,1270.32 957.997,1295.73 959.006,1292.94 960.015,1290.04 961.023,1295.73 \n",
       "  962.032,1286.57 963.04,1282.26 964.049,1296.55 965.057,1297.63 966.066,1293.73 967.074,1276.01 968.083,1286.44 969.091,1290.22 970.1,1296.12 971.108,1295.66 \n",
       "  972.117,1297.99 973.125,1282.08 974.134,1297.21 975.142,1299.11 976.151,1300.75 977.159,1301.48 978.168,1286.29 979.176,1283.42 980.185,1306.68 981.194,1286.85 \n",
       "  982.202,1294.69 983.211,1284.38 984.219,1292.55 985.228,1290.72 986.236,1303.74 987.245,1262.61 988.253,1292.96 989.262,1289.85 990.27,1283.73 991.279,1311.03 \n",
       "  992.287,1276.57 993.296,1296.71 994.304,1304.35 995.313,1290.73 996.321,1289.14 997.33,1295.83 998.338,1315.89 999.347,1299.65 1000.36,1295.54 1001.36,1313.63 \n",
       "  1002.37,1296 1003.38,1295.84 1004.39,1302.99 1005.4,1300.07 1006.41,1297.32 1007.42,1287.29 1008.42,1298.5 1009.43,1299.55 1010.44,1296.66 1011.45,1306.4 \n",
       "  1012.46,1306.73 1013.47,1301.56 1014.47,1290.59 1015.48,1329.05 1016.49,1306.52 1017.5,1303.73 1018.51,1305.42 1019.52,1307.74 1020.53,1308.85 1021.53,1307.03 \n",
       "  1022.54,1305.65 1023.55,1284.98 1024.56,1280.84 1025.57,1312.25 1026.58,1285.27 1027.59,1302.86 1028.59,1290.67 1029.6,1307.81 1030.61,1304.21 1031.62,1301.77 \n",
       "  1032.63,1309.01 1033.64,1307.25 1034.65,1322.13 1035.65,1311.73 1036.66,1323.16 1037.67,1310.1 1038.68,1310.82 1039.69,1298.11 1040.7,1308.32 1041.7,1300.65 \n",
       "  1042.71,1322.44 1043.72,1314.92 1044.73,1308.6 1045.74,1311.45 1046.75,1318.42 1047.76,1309.51 1048.76,1319.45 1049.77,1326.48 1050.78,1300.89 1051.79,1323.21 \n",
       "  1052.8,1293.47 1053.81,1326.63 1054.82,1308.92 1055.82,1300.59 1056.83,1316.4 1057.84,1287.23 1058.85,1300.56 1059.86,1307.17 1060.87,1319.14 1061.88,1317.64 \n",
       "  1062.88,1294.07 1063.89,1303.48 1064.9,1318.73 1065.91,1331.76 1066.92,1322 1067.93,1318.81 1068.94,1327.51 1069.94,1290.11 1070.95,1309.04 1071.96,1315.17 \n",
       "  1072.97,1303.56 1073.98,1336.83 1074.99,1319 1075.99,1322.79 1077,1327.87 1078.01,1313.99 1079.02,1322.45 1080.03,1312.18 1081.04,1319.35 1082.05,1302.46 \n",
       "  1083.05,1313.95 1084.06,1331.38 1085.07,1293.95 1086.08,1335.99 1087.09,1322.25 1088.1,1320.97 1089.11,1314.95 1090.11,1315.9 1091.12,1321.46 1092.13,1306.06 \n",
       "  1093.14,1324.91 1094.15,1314.12 1095.16,1327.78 1096.17,1321.63 1097.17,1326.9 1098.18,1315.29 1099.19,1306.69 1100.2,1314.97 1101.21,1333.13 1102.22,1320.1 \n",
       "  1103.22,1313.9 1104.23,1336.85 1105.24,1329.99 1106.25,1331.43 1107.26,1328.14 1108.27,1326.71 1109.28,1327.21 1110.28,1358.78 1111.29,1312.6 1112.3,1322.69 \n",
       "  1113.31,1326.26 1114.32,1347.19 1115.33,1308.85 1116.34,1338.11 1117.34,1329.03 1118.35,1342.79 1119.36,1318.18 1120.37,1333.57 1121.38,1332.3 1122.39,1334.47 \n",
       "  1123.4,1308.57 1124.4,1318.99 1125.41,1313.64 1126.42,1331.59 1127.43,1335.85 1128.44,1338 1129.45,1324.79 1130.46,1345.69 1131.46,1317.85 1132.47,1313.02 \n",
       "  1133.48,1338.97 1134.49,1320.37 1135.5,1305.36 1136.51,1317.63 1137.51,1310.99 1138.52,1336.51 1139.53,1328.9 1140.54,1332.15 1141.55,1338.22 1142.56,1325.34 \n",
       "  1143.57,1303.83 1144.57,1325.22 1145.58,1317.19 1146.59,1318.61 1147.6,1331.34 1148.61,1327.31 1149.62,1323.38 1150.63,1324.31 1151.63,1330.33 1152.64,1330.14 \n",
       "  1153.65,1350.45 1154.66,1316.51 1155.67,1355.84 1156.68,1329.51 1157.69,1322.86 1158.69,1340.94 1159.7,1311.52 1160.71,1311.69 1161.72,1330.46 1162.73,1318.32 \n",
       "  1163.74,1336.25 1164.74,1346.49 1165.75,1325.91 1166.76,1342.84 1167.77,1350.89 1168.78,1314.25 1169.79,1347.03 1170.8,1334.33 1171.8,1330.47 1172.81,1337.41 \n",
       "  1173.82,1333.19 1174.83,1328.54 1175.84,1338.87 1176.85,1355.51 1177.86,1347.18 1178.86,1319.81 1179.87,1321.78 1180.88,1330.62 1181.89,1323.93 1182.9,1333.66 \n",
       "  1183.91,1331.05 1184.92,1329.11 1185.92,1311.3 1186.93,1348.87 1187.94,1351.51 1188.95,1347.37 1189.96,1344.97 1190.97,1318.66 1191.98,1355.13 1192.98,1340.9 \n",
       "  1193.99,1331.81 1195,1334.95 1196.01,1344.82 1197.02,1341.67 1198.03,1334.64 1199.03,1324.16 1200.04,1342.45 1201.05,1350.17 1202.06,1334.88 1203.07,1334.29 \n",
       "  1204.08,1332.9 1205.09,1353.7 1206.09,1322.62 1207.1,1350.06 1208.11,1337.38 1209.12,1334.13 1210.13,1339.73 1211.14,1352.16 1212.15,1319.96 1213.15,1347.55 \n",
       "  1214.16,1347.71 1215.17,1333.14 1216.18,1332.1 1217.19,1332.39 1218.2,1351.83 1219.21,1352.18 1220.21,1357.8 1221.22,1346.05 1222.23,1332.06 1223.24,1342.67 \n",
       "  1224.25,1314.59 1225.26,1337.96 1226.26,1335.95 1227.27,1340.51 1228.28,1331.86 1229.29,1338.99 1230.3,1335.2 1231.31,1333.22 1232.32,1335.11 1233.32,1347.63 \n",
       "  1234.33,1350.45 1235.34,1337.36 1236.35,1330.91 1237.36,1352.27 1238.37,1309.04 1239.38,1335.56 1240.38,1336.15 1241.39,1343.73 1242.4,1329.28 1243.41,1324.01 \n",
       "  1244.42,1346.83 1245.43,1341.63 1246.44,1342.63 1247.44,1338.4 1248.45,1346.56 1249.46,1336.26 1250.47,1344.05 1251.48,1343.06 1252.49,1315.3 1253.5,1333.68 \n",
       "  1254.5,1353.69 1255.51,1342.65 1256.52,1335.59 1257.53,1359.65 1258.54,1338.8 1259.55,1343.19 1260.55,1352.74 1261.56,1331.87 1262.57,1354.31 1263.58,1332.05 \n",
       "  1264.59,1360.33 1265.6,1361.52 1266.61,1351.06 1267.61,1327.67 1268.62,1339.99 1269.63,1327.29 1270.64,1336.9 1271.65,1356.06 1272.66,1353.05 1273.67,1318.8 \n",
       "  1274.67,1352.98 1275.68,1360.94 1276.69,1322.07 1277.7,1340.11 1278.71,1327.42 1279.72,1345.44 1280.73,1346.33 1281.73,1339.79 1282.74,1358.41 1283.75,1350 \n",
       "  1284.76,1344.92 1285.77,1342.12 1286.78,1346.92 1287.78,1365.98 1288.79,1334.92 1289.8,1341.89 1290.81,1350.8 1291.82,1330.43 1292.83,1350.6 1293.84,1348.8 \n",
       "  1294.84,1343.61 1295.85,1325.6 1296.86,1341.06 1297.87,1349.04 1298.88,1353.46 1299.89,1354.36 1300.9,1353.17 1301.9,1339.53 1302.91,1344.15 1303.92,1339 \n",
       "  1304.93,1342.02 1305.94,1333.32 1306.95,1366.81 1307.96,1328.35 1308.96,1330.06 1309.97,1349.54 1310.98,1347.97 1311.99,1334.67 1313,1355.17 1314.01,1334.91 \n",
       "  1315.01,1332.26 1316.02,1340.14 1317.03,1345.3 1318.04,1358.33 1319.05,1349.08 1320.06,1342.43 1321.07,1368.72 1322.07,1341.99 1323.08,1346.41 1324.09,1331.78 \n",
       "  1325.1,1353.84 1326.11,1350.96 1327.12,1372.57 1328.13,1337.23 1329.13,1340.81 1330.14,1339.5 1331.15,1345.43 1332.16,1354.53 1333.17,1342.54 1334.18,1366.97 \n",
       "  1335.19,1351.07 1336.19,1358.73 1337.2,1349.99 1338.21,1335.98 1339.22,1360.11 1340.23,1335.43 1341.24,1350.11 1342.25,1349.59 1343.25,1358.62 1344.26,1336.23 \n",
       "  1345.27,1353.91 1346.28,1357.39 1347.29,1357.76 1348.3,1341.65 1349.3,1343.85 1350.31,1360.48 1351.32,1344.27 1352.33,1360.62 1353.34,1369.59 1354.35,1353.53 \n",
       "  1355.36,1346.7 1356.36,1346.66 1357.37,1349.96 1358.38,1336.73 1359.39,1336.71 1360.4,1349.93 1361.41,1351.03 1362.42,1345.68 1363.42,1337.21 1364.43,1335.37 \n",
       "  1365.44,1365.94 1366.45,1358.49 1367.46,1354.53 1368.47,1349.49 1369.48,1355.9 1370.48,1354.73 1371.49,1348.47 1372.5,1352.78 1373.51,1349.48 1374.52,1360.7 \n",
       "  1375.53,1357.06 1376.53,1356.06 1377.54,1340.98 1378.55,1331.97 1379.56,1352.95 1380.57,1356.56 1381.58,1353.6 1382.59,1334.95 1383.59,1340.1 1384.6,1340.08 \n",
       "  1385.61,1347.66 1386.62,1344.73 1387.63,1355.3 1388.64,1355.35 1389.65,1348.78 1390.65,1369.53 1391.66,1354.61 1392.67,1344.91 1393.68,1340.37 1394.69,1353.4 \n",
       "  1395.7,1361.06 1396.71,1361.78 1397.71,1354.84 1398.72,1333.86 1399.73,1359.53 1400.74,1357.36 1401.75,1356.79 1402.76,1360.68 1403.77,1354.56 1404.77,1362.27 \n",
       "  1405.78,1363.41 1406.79,1333.12 1407.8,1373.53 1408.81,1343.95 1409.82,1348.05 1410.82,1366.03 1411.83,1367.59 1412.84,1334.5 1413.85,1352.03 1414.86,1376.73 \n",
       "  1415.87,1353.93 1416.88,1353.39 1417.88,1374.42 1418.89,1370.5 1419.9,1336.64 1420.91,1366.29 1421.92,1359.16 1422.93,1359.81 1423.94,1351.63 1424.94,1352.51 \n",
       "  1425.95,1360.59 1426.96,1340.25 1427.97,1371.56 1428.98,1373.68 1429.99,1367.31 1431,1350.1 1432,1360.76 1433.01,1378.05 1434.02,1361.4 1435.03,1340.73 \n",
       "  1436.04,1338.57 1437.05,1367.82 1438.05,1345.13 1439.06,1347.01 1440.07,1372.13 1441.08,1375 1442.09,1350.79 1443.1,1354.5 1444.11,1359.37 1445.11,1350.34 \n",
       "  1446.12,1366.34 1447.13,1363.34 1448.14,1352.91 1449.15,1344.47 1450.16,1359.87 1451.17,1352.7 1452.17,1364.58 1453.18,1348.36 1454.19,1354.73 1455.2,1357.91 \n",
       "  1456.21,1360.14 1457.22,1362.66 1458.23,1327.38 1459.23,1363.4 1460.24,1336.96 1461.25,1372.72 1462.26,1347.88 1463.27,1345.52 1464.28,1346.42 1465.29,1377.76 \n",
       "  1466.29,1357.41 1467.3,1355.46 1468.31,1372.04 1469.32,1359.65 1470.33,1350.59 1471.34,1349.47 1472.34,1360.33 1473.35,1342.22 1474.36,1358.43 1475.37,1355.01 \n",
       "  1476.38,1339.3 1477.39,1366.54 1478.4,1367.79 1479.4,1346.97 1480.41,1361.29 1481.42,1358.66 1482.43,1366.64 1483.44,1351.59 1484.45,1337.33 1485.46,1345.88 \n",
       "  1486.46,1363.92 1487.47,1348.86 1488.48,1347.7 1489.49,1358.12 1490.5,1361.29 1491.51,1347.77 1492.52,1368.53 1493.52,1343.86 1494.53,1368.83 1495.54,1330.66 \n",
       "  1496.55,1349.26 1497.56,1360.58 1498.57,1354.53 1499.57,1347.74 1500.58,1350.55 1501.59,1363.5 1502.6,1339.3 1503.61,1352.16 1504.62,1361.88 1505.63,1357.26 \n",
       "  1506.63,1350.79 1507.64,1348.43 1508.65,1372.88 1509.66,1352.79 1510.67,1348.12 1511.68,1378.91 1512.69,1348.21 1513.69,1366.86 1514.7,1324.69 1515.71,1367.54 \n",
       "  1516.72,1359.04 1517.73,1360.97 1518.74,1373.37 1519.75,1356.93 1520.75,1360.8 1521.76,1354.26 1522.77,1349.16 1523.78,1357.57 1524.79,1357.7 1525.8,1348.74 \n",
       "  1526.81,1348.64 1527.81,1352.17 1528.82,1348.18 1529.83,1350.89 1530.84,1354.48 1531.85,1343.52 1532.86,1358.21 1533.86,1361.66 1534.87,1356.22 1535.88,1353.23 \n",
       "  1536.89,1358.88 1537.9,1351.65 1538.91,1362.13 1539.92,1360.7 1540.92,1374.73 1541.93,1348.59 1542.94,1359.88 1543.95,1339.94 1544.96,1353.28 1545.97,1359.56 \n",
       "  1546.98,1353.11 1547.98,1354.22 1548.99,1372.17 1550,1363.58 1551.01,1352.89 1552.02,1362.91 1553.03,1348.1 1554.04,1355.69 1555.04,1342.82 1556.05,1363.23 \n",
       "  1557.06,1354.31 1558.07,1356.67 1559.08,1329.95 1560.09,1372.03 1561.09,1344.48 1562.1,1349.93 1563.11,1368.95 1564.12,1335.2 1565.13,1353.33 1566.14,1364.3 \n",
       "  1567.15,1362.09 1568.15,1355.77 1569.16,1364.19 1570.17,1353.84 1571.18,1361.84 1572.19,1352.34 1573.2,1365.26 1574.21,1351.68 1575.21,1354.54 1576.22,1349.16 \n",
       "  1577.23,1370.72 1578.24,1369.97 1579.25,1368.35 1580.26,1354.12 1581.27,1353.24 1582.27,1361.81 1583.28,1362.56 1584.29,1361.15 1585.3,1346.3 1586.31,1330.09 \n",
       "  1587.32,1371.33 1588.32,1360.68 1589.33,1361.38 1590.34,1369.12 1591.35,1362.02 1592.36,1348.02 1593.37,1359.25 1594.38,1354.86 1595.38,1358.91 1596.39,1340.66 \n",
       "  1597.4,1358.13 1598.41,1352.76 1599.42,1365.16 1600.43,1374.22 1601.44,1325.43 1602.44,1353.3 1603.45,1371.67 1604.46,1364.94 1605.47,1376.04 1606.48,1372.2 \n",
       "  1607.49,1354.36 1608.5,1362.07 1609.5,1362.2 1610.51,1377.25 1611.52,1375.64 1612.53,1367.04 1613.54,1359.83 1614.55,1348.06 1615.56,1357.54 1616.56,1340.44 \n",
       "  1617.57,1355.07 1618.58,1359.56 1619.59,1347.09 1620.6,1346.72 1621.61,1353.95 1622.61,1365.7 1623.62,1353.61 1624.63,1336.64 1625.64,1373.72 1626.65,1356.3 \n",
       "  1627.66,1379.53 1628.67,1355.58 1629.67,1371.2 1630.68,1358.55 1631.69,1353.32 1632.7,1369.46 1633.71,1352.3 1634.72,1351.88 1635.73,1373.93 1636.73,1353.98 \n",
       "  1637.74,1348.57 1638.75,1343.48 1639.76,1361.07 1640.77,1332.37 1641.78,1361.35 1642.79,1346.18 1643.79,1331.84 1644.8,1371.23 1645.81,1372.98 1646.82,1355.39 \n",
       "  1647.83,1349.46 1648.84,1356.49 1649.84,1374.09 1650.85,1350.74 1651.86,1340.86 1652.87,1355.48 1653.88,1339.23 1654.89,1354.91 1655.9,1356.91 1656.9,1346.54 \n",
       "  1657.91,1373.87 1658.92,1359.17 1659.93,1366.4 1660.94,1336.38 1661.95,1349.65 1662.96,1341.25 1663.96,1365.71 1664.97,1365.98 1665.98,1354.21 1666.99,1346.63 \n",
       "  1668,1364.92 1669.01,1354.29 1670.02,1360.35 1671.02,1355.93 1672.03,1371.96 1673.04,1350.77 1674.05,1359.34 1675.06,1344.27 1676.07,1353.3 1677.08,1351.82 \n",
       "  1678.08,1347.09 1679.09,1363.63 1680.1,1362.77 1681.11,1349.79 1682.12,1358.69 1683.13,1360.04 1684.13,1350.76 1685.14,1339.1 1686.15,1372.99 1687.16,1361.27 \n",
       "  1688.17,1352.79 1689.18,1362.4 1690.19,1341.37 1691.19,1359.46 1692.2,1366.95 1693.21,1342.92 1694.22,1362.53 1695.23,1340.23 1696.24,1341.63 1697.25,1366.82 \n",
       "  1698.25,1346.5 1699.26,1363.87 1700.27,1337.47 1701.28,1351.62 1702.29,1346.84 1703.3,1360.12 1704.31,1377.65 1705.31,1364.61 1706.32,1348.72 1707.33,1343.63 \n",
       "  1708.34,1371.55 1709.35,1347.76 1710.36,1383.75 1711.36,1365.43 1712.37,1362.02 1713.38,1376.39 1714.39,1356.05 1715.4,1322.3 1716.41,1357.66 1717.42,1348.97 \n",
       "  1718.42,1361.78 1719.43,1379.5 1720.44,1349.25 1721.45,1359.3 1722.46,1359.41 1723.47,1343.76 1724.48,1370.94 1725.48,1365.28 1726.49,1349.92 1727.5,1340.43 \n",
       "  1728.51,1357.23 1729.52,1355.32 1730.53,1355.98 1731.54,1334.05 1732.54,1356.13 1733.55,1364.82 1734.56,1355.58 1735.57,1361.29 1736.58,1348.21 1737.59,1354.68 \n",
       "  1738.6,1360.56 1739.6,1370.7 1740.61,1344.16 1741.62,1352.89 1742.63,1363.91 1743.64,1360.85 1744.65,1373.58 1745.65,1359.17 1746.66,1340.2 1747.67,1345.62 \n",
       "  1748.68,1369.73 1749.69,1342.08 1750.7,1354.17 1751.71,1326.38 1752.71,1359 1753.72,1349.77 1754.73,1348.05 1755.74,1364.73 1756.75,1367.66 1757.76,1362.94 \n",
       "  1758.77,1368.22 1759.77,1347.43 1760.78,1353.01 1761.79,1365.27 1762.8,1343.67 1763.81,1341.88 1764.82,1362.13 1765.83,1348.53 1766.83,1368.24 1767.84,1366.54 \n",
       "  1768.85,1366.76 1769.86,1350.72 1770.87,1360.3 1771.88,1357.47 1772.88,1362.57 1773.89,1355.07 1774.9,1363.97 1775.91,1342.54 1776.92,1364.33 1777.93,1363.04 \n",
       "  1778.94,1352.83 1779.94,1344.32 1780.95,1362.26 1781.96,1357.6 1782.97,1349.2 1783.98,1352.33 1784.99,1366.29 1786,1350.44 1787,1381.24 1788.01,1352.63 \n",
       "  1789.02,1347.13 1790.03,1361.25 1791.04,1362.34 1792.05,1343.6 1793.06,1363.13 1794.06,1361.27 1795.07,1350.15 1796.08,1346.88 1797.09,1344.43 1798.1,1347.61 \n",
       "  1799.11,1349.69 1800.12,1367.99 1801.12,1373.37 1802.13,1346.86 1803.14,1357.23 1804.15,1333.74 1805.16,1345.58 1806.17,1340.48 1807.17,1379.67 1808.18,1363.89 \n",
       "  1809.19,1370.15 1810.2,1373.06 1811.21,1348.41 1812.22,1334.56 1813.23,1352.65 1814.23,1354.29 1815.24,1362.41 1816.25,1378.37 1817.26,1372.19 1818.27,1347.4 \n",
       "  1819.28,1372.11 1820.29,1357.38 1821.29,1362.37 1822.3,1363.67 1823.31,1342.98 1824.32,1362.8 1825.33,1348.55 1826.34,1355.78 1827.35,1352.61 1828.35,1383.65 \n",
       "  1829.36,1336.12 1830.37,1340.52 1831.38,1360.21 1832.39,1362.65 1833.4,1356.7 1834.4,1376.29 1835.41,1360.06 1836.42,1355.01 1837.43,1369.39 1838.44,1352.82 \n",
       "  1839.45,1349.62 1840.46,1365.32 1841.46,1353.98 1842.47,1357.02 1843.48,1365.45 1844.49,1343.03 1845.5,1370.15 1846.51,1357.14 1847.52,1345.8 1848.52,1357.62 \n",
       "  1849.53,1353.5 1850.54,1346.24 1851.55,1341.72 1852.56,1343.78 1853.57,1345.35 1854.58,1372.77 1855.58,1379.58 1856.59,1367.36 1857.6,1360.92 1858.61,1366.51 \n",
       "  1859.62,1373.46 1860.63,1366.32 1861.63,1357.42 1862.64,1361.77 1863.65,1372.79 1864.66,1362.65 1865.67,1354.51 1866.68,1335.51 1867.69,1333.12 1868.69,1349.05 \n",
       "  1869.7,1351.54 1870.71,1363.15 1871.72,1367.06 1872.73,1358.53 1873.74,1336.75 1874.75,1371.99 1875.75,1346.01 1876.76,1349.73 1877.77,1359.37 1878.78,1362.2 \n",
       "  1879.79,1360.94 1880.8,1349.23 1881.81,1375.02 1882.81,1342.76 1883.82,1355.57 1884.83,1342.46 1885.84,1346.16 1886.85,1366.98 1887.86,1360.66 1888.87,1361.72 \n",
       "  1889.87,1356.18 1890.88,1346 1891.89,1355.28 1892.9,1357.11 1893.91,1353.5 1894.92,1352.27 1895.92,1339.99 1896.93,1362.92 1897.94,1348.14 1898.95,1372.69 \n",
       "  1899.96,1361.74 1900.97,1353.49 1901.98,1339.96 1902.98,1348.18 1903.99,1357.71 1905,1366.98 1906.01,1353.18 1907.02,1362.01 1908.03,1369.3 1909.04,1363 \n",
       "  1910.04,1355.75 1911.05,1359.96 1912.06,1358.91 1913.07,1347.5 1914.08,1361.16 1915.09,1353.2 1916.1,1346.69 1917.1,1371.67 1918.11,1372.76 1919.12,1384.24 \n",
       "  1920.13,1353 1921.14,1351.93 1922.15,1364.13 1923.15,1377.59 1924.16,1349.1 1925.17,1368.73 1926.18,1367.09 1927.19,1355.73 1928.2,1361.07 1929.21,1358.81 \n",
       "  1930.21,1379.92 1931.22,1351.6 1932.23,1365.16 1933.24,1361.46 1934.25,1368.16 1935.26,1344.86 1936.27,1367.25 1937.27,1369.24 1938.28,1360.14 1939.29,1355.27 \n",
       "  1940.3,1381.44 1941.31,1358.86 1942.32,1368.79 1943.33,1336.51 1944.33,1349.3 1945.34,1356.42 1946.35,1358.93 1947.36,1361.06 1948.37,1340.87 1949.38,1360.89 \n",
       "  1950.39,1343.43 1951.39,1330.31 1952.4,1354.38 1953.41,1372.94 1954.42,1353.38 1955.43,1354.5 1956.44,1359.73 1957.44,1364.44 1958.45,1347.43 1959.46,1362.21 \n",
       "  1960.47,1359.91 1961.48,1362.33 1962.49,1357.48 1963.5,1349.16 1964.5,1340.54 1965.51,1346.54 1966.52,1359.97 1967.53,1360.1 1968.54,1350.7 1969.55,1352.08 \n",
       "  1970.56,1350.6 1971.56,1360.62 1972.57,1361.35 1973.58,1377.37 1974.59,1341.18 1975.6,1354.23 1976.61,1347.82 1977.62,1356.79 1978.62,1366.37 1979.63,1354.11 \n",
       "  1980.64,1353.83 1981.65,1361.43 1982.66,1361.47 1983.67,1354.33 1984.67,1345.85 1985.68,1358.8 1986.69,1375.17 1987.7,1358.83 1988.71,1343.45 1989.72,1373.67 \n",
       "  1990.73,1352.61 1991.73,1360.33 1992.74,1354.7 1993.75,1346.41 1994.76,1337.72 1995.77,1358.93 1996.78,1355.99 1997.79,1360.33 1998.79,1358.88 1999.8,1342.42 \n",
       "  2000.81,1347.61 2001.82,1366.28 2002.83,1365.42 2003.84,1351.44 2004.85,1354.57 2005.85,1387.34 2006.86,1346.14 2007.87,1375.45 2008.88,1376.77 2009.89,1355.7 \n",
       "  2010.9,1344.51 2011.91,1338.65 2012.91,1358.84 2013.92,1360.12 2014.93,1343.12 2015.94,1342.02 2016.95,1343.19 2017.96,1370.79 2018.96,1353.93 2019.97,1374.06 \n",
       "  2020.98,1369.78 2021.99,1364.13 2023,1355.27 2024.01,1336.14 2025.02,1329.71 2026.02,1353.04 2027.03,1346.72 2028.04,1357.59 2029.05,1335.94 2030.06,1370.63 \n",
       "  2031.07,1337.42 2032.08,1353.53 2033.08,1351.28 2034.09,1353.02 2035.1,1347.41 2036.11,1342.81 2037.12,1346.92 2038.13,1355.36 2039.14,1356.26 2040.14,1345.83 \n",
       "  2041.15,1356.63 2042.16,1365.09 2043.17,1366.87 2044.18,1351 2045.19,1352.45 2046.19,1358.46 2047.2,1371.05 2048.21,1343.91 2049.22,1359.14 2050.23,1353.72 \n",
       "  2051.24,1346.89 2052.25,1345.46 2053.25,1355.41 2054.26,1367.07 2055.27,1327.13 2056.28,1363.08 2057.29,1352.95 2058.3,1358.13 2059.31,1360.81 2060.31,1357.7 \n",
       "  2061.32,1350.92 2062.33,1359.34 2063.34,1336.48 2064.35,1346.98 2065.36,1348.58 2066.37,1355.2 2067.37,1368.14 2068.38,1344.71 2069.39,1339.25 2070.4,1357.98 \n",
       "  2071.41,1357 2072.42,1348.78 2073.43,1373.41 2074.43,1364.79 2075.44,1346.71 2076.45,1343.7 2077.46,1344.06 2078.47,1352.58 2079.48,1358.01 2080.48,1380.97 \n",
       "  2081.49,1353.29 2082.5,1351.1 2083.51,1349.76 2084.52,1346.54 2085.53,1356.09 2086.54,1361.14 2087.54,1378.19 2088.55,1363.79 2089.56,1347.19 2090.57,1360.96 \n",
       "  2091.58,1358.76 2092.59,1388.08 2093.6,1361.84 2094.6,1353.4 2095.61,1352.5 2096.62,1336.55 2097.63,1330.34 2098.64,1347 2099.65,1367.55 2100.66,1372.99 \n",
       "  2101.66,1354.29 2102.67,1324.63 2103.68,1362.76 2104.69,1355.85 2105.7,1370.78 2106.71,1354.82 2107.71,1360.34 2108.72,1360.58 2109.73,1369.25 2110.74,1354.27 \n",
       "  2111.75,1375.91 2112.76,1354.49 2113.77,1357.72 2114.77,1357.22 2115.78,1361.38 2116.79,1362.35 2117.8,1381.23 2118.81,1367.89 2119.82,1363.23 2120.83,1359.32 \n",
       "  2121.83,1368.08 2122.84,1335.96 2123.85,1358.13 2124.86,1339.32 2125.87,1358.73 2126.88,1354.39 2127.89,1351.28 2128.89,1353.01 2129.9,1382.54 2130.91,1354.68 \n",
       "  2131.92,1345.28 2132.93,1351.16 2133.94,1365.74 2134.94,1355.76 2135.95,1355.28 2136.96,1355.99 2137.97,1336.16 2138.98,1358.75 2139.99,1354.61 2141,1379.73 \n",
       "  2142,1369.14 2143.01,1364.8 2144.02,1359.52 2145.03,1358.19 2146.04,1350.92 2147.05,1366.03 2148.06,1368.47 2149.06,1367.56 2150.07,1370.6 2151.08,1385.73 \n",
       "  2152.09,1359.98 2153.1,1338.51 2154.11,1361.23 2155.12,1339.97 2156.12,1359.5 2157.13,1350.01 2158.14,1359.66 2159.15,1342.15 2160.16,1357.03 2161.17,1345.15 \n",
       "  2162.18,1363.46 2163.18,1369.54 2164.19,1326.45 2165.2,1364.02 2166.21,1357.22 2167.22,1347.59 2168.23,1347.55 2169.23,1360.85 2170.24,1364.05 2171.25,1353.74 \n",
       "  2172.26,1332.68 2173.27,1352.7 2174.28,1352.56 2175.29,1375.46 2176.29,1344.67 2177.3,1353.97 2178.31,1353.67 2179.32,1351.67 2180.33,1363.28 2181.34,1349.35 \n",
       "  2182.35,1352.46 2183.35,1343.85 2184.36,1349.47 2185.37,1332.65 2186.38,1350.37 2187.39,1354.28 2188.4,1382.55 2189.41,1369.58 2190.41,1360.27 2191.42,1349.25 \n",
       "  2192.43,1364.37 2193.44,1361.33 2194.45,1358.84 2195.46,1356.92 2196.46,1376.44 2197.47,1355.9 2198.48,1380.43 2199.49,1356.37 2200.5,1356.41 2201.51,1335.73 \n",
       "  2202.52,1373.92 2203.52,1340.23 2204.53,1350.22 2205.54,1344.78 2206.55,1369.69 2207.56,1361.59 2208.57,1345.49 2209.58,1356.95 2210.58,1371.77 2211.59,1375.9 \n",
       "  2212.6,1373.47 2213.61,1345.47 2214.62,1344.73 2215.63,1368.96 2216.64,1351.53 2217.64,1353.79 2218.65,1360.85 2219.66,1379.29 2220.67,1345.1 2221.68,1355.92 \n",
       "  2222.69,1354.84 2223.7,1344.01 2224.7,1363.94 2225.71,1350.19 2226.72,1369.67 2227.73,1345.79 2228.74,1349.25 2229.75,1388.71 2230.75,1363.9 2231.76,1365.27 \n",
       "  2232.77,1364.03 2233.78,1370.76 2234.79,1360.62 2235.8,1357.24 2236.81,1353.96 2237.81,1345.12 2238.82,1341.96 2239.83,1371.83 2240.84,1344.52 2241.85,1361.66 \n",
       "  2242.86,1383.15 2243.87,1366.85 2244.87,1347 2245.88,1343.97 2246.89,1359.85 2247.9,1358.08 2248.91,1361.54 2249.92,1342.61 2250.93,1356.94 2251.93,1361.89 \n",
       "  2252.94,1342.03 2253.95,1367.77 2254.96,1365.33 2255.97,1349.1 2256.98,1361.79 2257.98,1354.86 2258.99,1374.97 2260,1350.38 2261.01,1361.09 2262.02,1366.87 \n",
       "  2263.03,1349.42 2264.04,1365.32 2265.04,1343.25 2266.05,1359.68 2267.06,1372.01 2268.07,1355.53 2269.08,1352.08 2270.09,1366.24 2271.1,1357.99 2272.1,1367.64 \n",
       "  2273.11,1358.72 2274.12,1378.91 2275.13,1363.51 2276.14,1334 2277.15,1355.69 2278.16,1356.38 2279.16,1382.04 2280.17,1346.28 2281.18,1348.41 2282.19,1348.68 \n",
       "  2283.2,1360.07 2284.21,1358.76 2285.22,1359.39 2286.22,1365.92 2287.23,1364.67 2288.24,1351.32 2289.25,1355.69 2290.26,1359.37 2291.27,1360.75 2292.27,1357.48 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"\n",
       "M1989.93 326.155 L2280.76 326.155 L2280.76 205.195 L1989.93 205.195  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.93,326.155 2280.76,326.155 2280.76,205.195 1989.93,205.195 1989.93,326.155 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.93,265.675 2157.93,265.675 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip2000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2181.93, 283.175)\" x=\"2181.93\" y=\"283.175\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_error_plot(3, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.263087, 1.05853], Float32[0.591713, 0.669902])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo: should we use those learned weights as initializers? they approximate the normal distribution.\n",
    "\n",
    "function normal_approximators(N)\n",
    "    vs = gpu(even_us(N))\n",
    "    βs = gpu(ones(Float32, N))\n",
    "\n",
    "    opt = ADAM()\n",
    "    ps = Params([vs, βs])\n",
    "\n",
    "    for _ in 1:1000\n",
    "        # note: in loop!\n",
    "        A = gpu(randn(100, 100))\n",
    "        _, adjoint = pullback(ps) do\n",
    "            Ã = binarize_activations(A, vs, βs)\n",
    "            mean((A .- Ã).^2)\n",
    "        end\n",
    "        gs = adjoint(1.0)\n",
    "\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "    end\n",
    "    \n",
    "    vs, βs\n",
    "end\n",
    "\n",
    "normal_approximators(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.262703, 1.05827], Float32[0.591643, 0.672335])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_approximators(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.535493, 0.420531, 1.40799], Float32[0.514053, 0.496134, 0.562909])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_approximators(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinWeights"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct BinWeights{U <: AbstractVector, A <: AbstractVector}\n",
    "    us :: U\n",
    "    αs :: A\n",
    "    active :: Bool\n",
    "end\n",
    "\n",
    "function BinWeights(W, us :: U; active=false) where U\n",
    "    # note: W is not stored! it's just used to initialize alphas.\n",
    "    _, αs = binarize_weights(W, us)\n",
    "    BinWeights(us, αs, active)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: no args, because the parameters aren't trainable, and shouldn't be moved to GPU.\n",
    "Flux.@functor BinWeights ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (bw :: BinWeights{U, A})(W) where {U, A}\n",
    "    if bw.active\n",
    "        if Flux.istraining()\n",
    "            W̃, αs = binarize_weights(W, bw.us)\n",
    "            bw.αs = αs\n",
    "            W̃\n",
    "        else\n",
    "            # todo: cache W̃?\n",
    "            W̃ = binarize_weights(W, bw.us, bw.αs)\n",
    "            W̃\n",
    "        end\n",
    "    else\n",
    "        W\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinWeights{Array{Float32,1},Array{Float32,1}}(Float32[-1.0, -0.5, 0.0, 0.5, 1.0], Float32[0.365286, 0.241805, 0.234198, 0.242631, 0.379702], true)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = BinWeights(W, even_us(5), active=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test q(W) == binarize_weights(W, even_us(5))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test gpu(q) === q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct BinActs{V, B}\n",
    "    vs :: V\n",
    "    βs :: B\n",
    "    active :: Bool\n",
    "end\n",
    "\n",
    "function BinActs(vs :: V, βs :: B; active=false) where {V, B}\n",
    "    # note: W is not stored! it's just used to initialize alphas.\n",
    "    BinActs(vs, βs, active)\n",
    "end\n",
    "\n",
    "function (ba :: BinActs{V, B})(A) where {V, B}\n",
    "    if ba.active\n",
    "        binarize_activations(A, ba.vs, ba.βs)\n",
    "    else\n",
    "        A\n",
    "    end\n",
    "end\n",
    "\n",
    "Flux.@functor BinActs (vs, βs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABCConv"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://github.com/FluxML/Flux.jl/blob/fb4a48f970ba40d0022a7488b48d19cd563867c4/src/layers/conv.jl\n",
    "# note: does not include activation, that should go before (?)\n",
    "\n",
    "\"\"\"\n",
    "Standard convolutional layer with ABC-based quantization.\n",
    "\"\"\"\n",
    "struct ABCConv{W,Z, S,P, U,A, V,B} # that's a lotta parameters!!\n",
    "    weight::W\n",
    "    bias::Z\n",
    "    \n",
    "    stride::NTuple{S,Int}\n",
    "    pad::NTuple{P,Int}\n",
    "    dilation::NTuple{S,Int}\n",
    "    \n",
    "    bin_weights :: BinWeights{U, A}\n",
    "    bin_acts :: BinActs{V, B}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ABCConv(weight::AbstractArray{T,K}, bias::AbstractVector{T},\n",
    "                 us::AbstractVector{T}, vs::AbstractVector{T}, βs::AbstractVector{T};\n",
    "              stride = 1, pad = 0, dilation = 1, bin_active=false) where {T, K}\n",
    "    @assert size(vs) == size(βs)\n",
    "    \n",
    "    stride = expand(Val(K-2), stride)\n",
    "    pad = expand(Val(2*(K-2)), pad)\n",
    "    dilation = expand(Val(K-2), dilation)\n",
    "    \n",
    "    bin_weights = BinWeights(weight, us, active=bin_active) # note: weights is used to initialize αs, not stored\n",
    "    bin_acts = BinActs(vs, βs, active=bin_active)\n",
    "    \n",
    "    ABCConv(weight, bias, stride, pad, dilation, bin_weights, bin_acts)\n",
    "end\n",
    "\n",
    "expand(N, i::Tuple) = i\n",
    "expand(N, i::Integer) = ntuple(_ -> i, N)\n",
    "\n",
    "function ABCConv(k::NTuple{D,Integer}, ch::Pair{<:Integer,<:Integer}, N::Integer, M::Integer;\n",
    "    weight_init = Flux.glorot_uniform, bias_init=k -> zeros(Float32, k), \n",
    "    us_init=even_us, vs_init=even_us, βs_init=k -> ones(Float32, k), \n",
    "    stride = 1, pad = 0, dilation = 1,\n",
    "    bin_active=false) where D\n",
    "        \n",
    "    ABCConv(weight_init(k..., ch...), bias_init(ch[2]),\n",
    "           us_init(M), vs_init(N), βs_init(N),\n",
    "           stride = stride, pad = pad, dilation = dilation, bin_active=bin_active)\n",
    "end\n",
    "\n",
    "Flux.@functor ABCConv\n",
    "\n",
    "function (c::ABCConv)(A::AbstractArray)\n",
    "    b = reshape(c.bias, map(_->1, c.stride)..., :, 1)\n",
    "    \n",
    "    W = c.weight\n",
    "    W̃ = c.bin_weights(W)\n",
    "    \n",
    "    Ã = c.bin_acts(A)\n",
    "    \n",
    "    cdims = DenseConvDims(Ã, W̃; stride=c.stride, padding=c.pad, dilation=c.dilation)\n",
    "    #println(\"W̃: \", typeof(W̃), \" \", size(W̃), \" Ã: \", typeof(Ã), \" \", size(Ã), \" b: \", typeof(b), \" \", size(b), \" dims: \", cdims)\n",
    "    \n",
    "    conv(Ã, W̃, cdims) .+ b\n",
    "end\n",
    "\n",
    "# Base.show(io :: IO, ::Type{ABCConv}) = print(io, \"ABCConv\")\n",
    "function Base.show(io::IO, l::ABCConv)\n",
    "    print(io, \"ABCConv(\", size(l.weight)[1:ndims(l.weight)-2])\n",
    "    print(io, \", \", size(l.weight, ndims(l.weight)-1), \"=>\", size(l.weight, ndims(l.weight)))\n",
    "    print(io, \", \", size(l.bin_acts.vs), \", \", size(l.bin_weights.us))\n",
    "    print(io, \", active=\", l.bin_acts.active)\n",
    "\n",
    "    print(io, \")\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (c::Conv)(A::AbstractArray)\n",
    "\n",
    "  σ, b = c.σ, reshape(c.bias, map(_->1, c.stride)..., :, 1)\n",
    "  W = c.weight\n",
    "  cdims = DenseConvDims(A, W; stride=c.stride, padding=c.pad, dilation=c.dilation)\n",
    "\n",
    "  r = σ.(conv(A, W, cdims) .+ b)\n",
    "\n",
    "  r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W̃: Array{Float32,4} (3, 3, 10, 10) Ã: Array{Float32,4} (5, 5, 10, 1) b: Array{Float32,4} (1, 1, 10, 1) dims: DenseConvDims: (5, 5, 10) * (3, 3) -> (3, 3, 10), stride: (1, 1) pad: (0, 0, 0, 0), dil: (1, 1), flip: false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv((3, 3), 10=>10)(randn(Float32, 5, 5, 10, 1))\n",
    "ABCConv((3, 3), 10=>10, 1, 1, bin_active=true)(randn(Float32, 5, 5, 10, 1))\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W̃: CuArray{Float32,4,Nothing} (3, 3, 10, 10) Ã: CuArray{Float32,4,Nothing} (5, 5, 10, 1) b: CuArray{Float32,4,CuArray{Float32,1,Nothing}} (1, 1, 10, 1) dims: DenseConvDims: (5, 5, 10) * (3, 3) -> (3, 3, 10), stride: (1, 1) pad: (0, 0, 0, 0), dil: (1, 1), flip: false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu(Conv((3, 3), 10=>10))(gpu(randn(Float32, 5, 5, 10, 1)))\n",
    "gpu(ABCConv((3, 3), 10=>10, 1, 1, bin_active=true))(gpu(randn(Float32, 5, 5, 10, 1)))\n",
    "\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ABCConv((3,3), 10 => 10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarize (generic function with 1 method)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function binarize(c :: ABCConv; active=true)\n",
    "    c.bin_acts.active = active\n",
    "    c.bin_weights.active = active\n",
    "    ()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data set\n",
      "└ @ Main In[49]:10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on Flux MNIST example\n",
    "\n",
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "\n",
    "dataset = Flux.Data.FashionMNIST\n",
    "\n",
    "@info(\"Loading data set\")\n",
    "train_labels = dataset.labels()\n",
    "train_imgs = dataset.images()\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAPASURBVGje7dm9ix1VGMfxz71792aTlexGRTERFHwNRMWXwhQiWIiNhVhYaGchFloF/AtSiZWVjaCthZYKlhIkhSkUhYiNkRVfgom7mn25d67Fcw5z7t0ht3LnMuyBYWbOnDnPPPPl9zzPmelpaD1MZvpO4318gkvYxR7O4CX8hHdxzc1b3wG37hvszZ6U7B7HK3gZY9yCFdw2M8llVHgIv+ELvIdvF8HD7hvsNXUex8d4ND3RFm4I3VUYYA3/pPOS+wqOYoiv8FrbHnbfYCPDL3EPrqqZjYrBfcGzX5zPTjrBXXgBP7TpYfcNDmY7nhT8/kwXl4SuTuKYmt9AxNcelgXjTfySjqXrr+Ncmx523+A+HZ7D24JhJRiO8QE2BKOT+FXNcyhy5RN4S83/eNrf26aH3Te4j+HXuENoalewuY6n8Tzuxod4A98JjS7hd1Gv/pjuXRF6fFjUrpfb8rD7BvfF0sdwRXA5kvqOp/3noo45LfT6KV5Mk3wj4vAIq0K7FX7GWYcM/8c2xfAR/CE49NW58Gq6fgY7olY5L0S8l/Zn05gNnBL8KmzjGXzUlofdNzjF8B3BbEvo6KhgMMJTYl14q6hh7hT8tkU+XBdryRNiHbIm2A7Tva152H2DUwwvCDb3i/i5KvLbWOTJrK2x0OhAcBqnJ98UMXM1Xe8LXX7WpofdN9i4PjyBB/AmnhX5cU18R1sWfGYn6QtNronvM68uiofdNzho6vwLF0Xue06s14dqfVVpXC9tlahDd9P+wiJ52H2D+xjmNfuuYLep5jYpxkwaJsv6vFacz36L6/4rbZ/hRNQqxL+I62nQTnG9ZJiD8Y5gD38X3ozb9rD7BhtjaX73N4Qej4jaNNcwmWPOgxPB8Ji6xlkYD7tvsJFh1liuQSdp6xf9OW6WHKt0XM3M06qH3Tc4mDfglKhxltQcm4rZ/O20Z3/d2qqH3Td4Ux1Sx8Wh+j9F3nJMrUQu3El9yw3ztOZh9w3O1eG20NXIdJ25pK5dB2n/b7pnfZE87L7BuQyr4risR8uYWtaqI/GNjkMdHlCby7B8otl1Xskw1zUjUZ8ujIfdNzg3HzJdo2TNlf25L69JDmuaA22NDMuYuWtaV+W//dl1RF4blmxb97D7BufG0vxUuSbtq+NoqckmfS6Eh903ODeWbuBBkefyv6fl4ngi+A6Kew91eKBtrg7Xxf+KAW5X63BYjMnrjisi7t5XeFPNzNf9V7oYDMt8eAnfq/8d5qfcUsfPrNE9wfxiGlc1zN39V3rgBv8DhyffYhCW4SwAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                                  \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(train_imgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bundle images together with labels and group into minibatchess\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "\n",
    "batch_size = 128\n",
    "mb_idxs = partition(1:length(train_imgs), batch_size)\n",
    "train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]\n",
    "\n",
    "test_imgs = dataset.images(:test)\n",
    "test_labels = dataset.labels(:test)\n",
    "mb_idxs = partition(1:length(test_imgs), batch_size)\n",
    "test_set = [make_minibatch(test_imgs, test_labels, i) for i in mb_idxs]\n",
    "\n",
    "train_set = gpu.(train_set)\n",
    "test_set = gpu.(test_set)\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAPASURBVGje7dm9ix1VGMfxz71792aTlexGRTERFHwNRMWXwhQiWIiNhVhYaGchFloF/AtSiZWVjaCthZYKlhIkhSkUhYiNkRVfgom7mn25d67Fcw5z7t0ht3LnMuyBYWbOnDnPPPPl9zzPmelpaD1MZvpO4318gkvYxR7O4CX8hHdxzc1b3wG37hvszZ6U7B7HK3gZY9yCFdw2M8llVHgIv+ELvIdvF8HD7hvsNXUex8d4ND3RFm4I3VUYYA3/pPOS+wqOYoiv8FrbHnbfYCPDL3EPrqqZjYrBfcGzX5zPTjrBXXgBP7TpYfcNDmY7nhT8/kwXl4SuTuKYmt9AxNcelgXjTfySjqXrr+Ncmx523+A+HZ7D24JhJRiO8QE2BKOT+FXNcyhy5RN4S83/eNrf26aH3Te4j+HXuENoalewuY6n8Tzuxod4A98JjS7hd1Gv/pjuXRF6fFjUrpfb8rD7BvfF0sdwRXA5kvqOp/3noo45LfT6KV5Mk3wj4vAIq0K7FX7GWYcM/8c2xfAR/CE49NW58Gq6fgY7olY5L0S8l/Zn05gNnBL8KmzjGXzUlofdNzjF8B3BbEvo6KhgMMJTYl14q6hh7hT8tkU+XBdryRNiHbIm2A7Tva152H2DUwwvCDb3i/i5KvLbWOTJrK2x0OhAcBqnJ98UMXM1Xe8LXX7WpofdN9i4PjyBB/AmnhX5cU18R1sWfGYn6QtNronvM68uiofdNzho6vwLF0Xue06s14dqfVVpXC9tlahDd9P+wiJ52H2D+xjmNfuuYLep5jYpxkwaJsv6vFacz36L6/4rbZ/hRNQqxL+I62nQTnG9ZJiD8Y5gD38X3ozb9rD7BhtjaX73N4Qej4jaNNcwmWPOgxPB8Ji6xlkYD7tvsJFh1liuQSdp6xf9OW6WHKt0XM3M06qH3Tc4mDfglKhxltQcm4rZ/O20Z3/d2qqH3Td4Ux1Sx8Wh+j9F3nJMrUQu3El9yw3ztOZh9w3O1eG20NXIdJ25pK5dB2n/b7pnfZE87L7BuQyr4risR8uYWtaqI/GNjkMdHlCby7B8otl1Xskw1zUjUZ8ujIfdNzg3HzJdo2TNlf25L69JDmuaA22NDMuYuWtaV+W//dl1RF4blmxb97D7BufG0vxUuSbtq+NoqckmfS6Eh903ODeWbuBBkefyv6fl4ngi+A6Kew91eKBtrg7Xxf+KAW5X63BYjMnrjisi7t5XeFPNzNf9V7oYDMt8eAnfq/8d5qfcUsfPrNE9wfxiGlc1zN39V3rgBv8DhyffYhCW4SwAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{Float32},2} with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(cpu(train_set[1])[1][:,:,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAAeAQAAAAANp/SVAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QAAd2KE6QAAADYSURBVDjLvZNLDoQgDEBrWLDDC5B4DXZeibmAoxfwTO68BslcwOUsSGZoixUTt9AgwdLX9AcMK6gZLOCeVpKQvm77HHBJt13n+gBLyCpXGO1qdoE0wzyiL98K6PZs4SlWXN/0Z4U0heP31gLwWWtB00kPByisqbOM+ZwYF70FkDEX+ns7U+jTWfSHftcFYn+22UhK40PcBmIrQC4iaAqXd5lIzWlEHETdBHDlM7CcicjrHE3DpGsCgAQqr9bCXSaw9EhKqQdgBb2WK7XevRj4LVRcssHqVwf+0XiPcf1Ksa8AAAAASUVORK5CYII=",
      "text/plain": [
       "10×128 Array{Gray{Bool},2} with eltype Gray{Bool}:\n",
       " Gray{Bool}(false)  Gray{Bool}(true)   …  Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)  …  Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(true) \n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(false)     Gray{Bool}(false)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(cpu(train_set[1])[2][:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Relu\n",
    "end\n",
    "\n",
    "(r :: Relu)(x) = relu.(x)\n",
    "\n",
    "Flux.@functor Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Constructing model...\n",
      "└ @ Main In[339]:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(Conv((3, 3), 1=>16), BatchNorm(16), Relu(), ABCConv((3, 3), 16=>32, (3,), (3,), active=false), BatchNorm(32), Relu(), ABCConv((3, 3), 32=>32, (3,), (3,), active=false), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), BatchNorm(32), Relu(), ABCConv((3, 3), 32=>32, (3,), (3,), active=false), BatchNorm(32), Relu(), ABCConv((3, 3), 32=>64, (3,), (3,), active=false), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), BatchNorm(64), Relu(), ABCConv((3, 3), 64=>64, (3,), (3,), active=false), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), getfield(Main, Symbol(\"##575#576\"))(), Relu(), Dense(576, 10), NNlib.softmax)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "M = 3\n",
    "\n",
    "@info(\"Constructing model...\")\n",
    "model = Chain(\n",
    "    Conv((3, 3), 1=>16, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(16),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 16=>32, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(32),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 32=>32, N, M, pad=(1,1)),\n",
    "    \n",
    "    MaxPool((2,2)),\n",
    "    \n",
    "    BatchNorm(32),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 32=>32, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(32),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 32=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    BatchNorm(64),\n",
    "    Relu(),\n",
    "    ABCConv((3, 3), 64=>64, N, M, pad=(1,1)),\n",
    "    \n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    \n",
    "    Relu(),\n",
    "    Dense(64 * 3 * 3, 10),\n",
    "\n",
    "    softmax,\n",
    ")\n",
    "model = gpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(x, y)\n",
    "    ŷ = model(x)\n",
    "    return crossentropy(ŷ, y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our model + gradients are nicely precompiled before starting our training loop\n",
    "model(train_set[1][1])\n",
    "_ = gradient(Flux.params(model)) do\n",
    "    loss(train_set[1]...)\n",
    "end\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = ADAM(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/FluxML/Flux.jl/blob/bdeb9c6d584668c7cef1ce71caf659d611c86d65/src/optimise/train.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train!"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    train!(loss, params, data, opt; cb)\n",
    "For each datapoint `d` in `data` computes the gradient of `loss(d...)` through\n",
    "backpropagation and calls the optimizer `opt`.\n",
    "Takes a callback as keyword argument `cb`. For example, this will print \"training\"\n",
    "every 10 seconds:\n",
    "```julia\n",
    "Flux.train!(loss, params, data, opt,\n",
    "            cb = throttle(() -> println(\"training\"), 10))\n",
    "```\n",
    "The callback can call `Flux.stop()` to interrupt the training loop.\n",
    "Multiple optimisers and callbacks can be passed to `opt` and `cb` as arrays.\n",
    "\"\"\"\n",
    "function train!(loss, ps, data, opt; cb = () -> ())\n",
    "  ps = Params(ps)\n",
    "  cb = Flux.Optimise.runall(cb)\n",
    "  @showprogress for d in data\n",
    "    try\n",
    "      gs = gradient(ps) do\n",
    "        loss(d...)\n",
    "      end\n",
    "      Flux.Optimise.update!(opt, ps, gs)\n",
    "      cb()\n",
    "    catch ex\n",
    "      if ex isa Flux.Optimise.StopException\n",
    "        break\n",
    "      else\n",
    "        rethrow(ex)\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate (generic function with 1 method)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold_(y::AbstractMatrix, labels...) =\n",
    "  dropdims(mapslices(y -> Base.argmax(y, labels...), y, dims=1), dims=1)\n",
    "\n",
    "function evaluate(model, data)\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for d in data\n",
    "        ŷ = cpu(model(d[1])) # reduction on GPU recompiles every iteration ???\n",
    "        ŷ_cold = onecold_(ŷ)\n",
    "        y = cpu(d[2])\n",
    "        y_cold = onecold_(y)\n",
    "        diff = y_cold .== ŷ_cold\n",
    "        correct += sum(diff)\n",
    "        total += size(d[2])[2]\n",
    "    end\n",
    "    return correct / total\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[313]:1\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "┌ Info: [1]: Test accuracy: 0.8751\n",
      "└ @ Main In[313]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[313]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:14\u001b[39m\n",
      "┌ Info: [2]: Test accuracy: 0.8985\n",
      "└ @ Main In[313]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[313]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "┌ Info: [3]: Test accuracy: 0.9042\n",
      "└ @ Main In[313]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[313]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n",
      "┌ Info: [4]: Test accuracy: 0.9120\n",
      "└ @ Main In[313]:13\n",
      "┌ Info:  -> New best accuracy! Saving model out to data/mnist_conv.bson\n",
      "└ @ Main In[313]:23\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "┌ Info: [5]: Test accuracy: 0.9098\n",
      "└ @ Main In[313]:13\n"
     ]
    }
   ],
   "source": [
    "@info(\"Beginning training loop...\")\n",
    "\n",
    "best_acc = 0.0\n",
    "last_improvement = 0\n",
    "\n",
    "for epoch_idx in 1:5\n",
    "    global best_acc, last_improvement\n",
    "    # Train for a single epoch\n",
    "    train!(loss, Flux.params(model), train_set, opt)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = evaluate(model, test_set)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "\n",
    "    # If our accuracy is good enough, quit out.\n",
    "    if acc >= 0.999\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # If this is the best accuracy we've seen so far, save the model out\n",
    "    if acc >= best_acc\n",
    "        @info(\" -> New best accuracy! Saving model out to data/mnist_conv.bson\")\n",
    "        BSON.@save joinpath(dirname(@__FILE__), \"data/mnist_conv.bson\") model epoch_idx acc\n",
    "        best_acc = acc\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    # If we haven't seen improvement in 5 epochs, drop our learning rate:\n",
    "    if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "\n",
    "        # After dropping learning rate, give it a few epochs to improve\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isa(model.layers[4], ABCConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: cannot take the CPU address of a CuArray{Float32,2,CuArray{Float32,5,Nothing}}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: cannot take the CPU address of a CuArray{Float32,2,CuArray{Float32,5,Nothing}}",
      "",
      "Stacktrace:",
      " [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2,CuArray{Float32,5,Nothing}}) at /data/scratch/jhgilles/.julia/packages/CuArrays/ZYCpV/src/array.jl:212",
      " [2] gemv!(::Char, ::Float32, ::CuArray{Float32,2,CuArray{Float32,5,Nothing}}, ::Array{Float32,1}, ::Float32, ::Array{Float32,1}) at /home/conda/feedstock_root/build_artifacts/julia_1548684429855/work/usr/share/julia/stdlib/v1.0/LinearAlgebra/src/blas.jl:575",
      " [3] gemv!(::Array{Float32,1}, ::Char, ::CuArray{Float32,2,CuArray{Float32,5,Nothing}}, ::Array{Float32,1}) at /home/conda/feedstock_root/build_artifacts/julia_1548684429855/work/usr/share/julia/stdlib/v1.0/LinearAlgebra/src/matmul.jl:360",
      " [4] mul! at /home/conda/feedstock_root/build_artifacts/julia_1548684429855/work/usr/share/julia/stdlib/v1.0/LinearAlgebra/src/matmul.jl:64 [inlined]",
      " [5] *(::CuArray{Float32,2,CuArray{Float32,5,Nothing}}, ::Array{Float32,1}) at /home/conda/feedstock_root/build_artifacts/julia_1548684429855/work/usr/share/julia/stdlib/v1.0/LinearAlgebra/src/matmul.jl:46",
      " [6] binarize_weights(::CuArray{Float32,4,Nothing}, ::Array{Float32,1}, ::Array{Float32,1}) at ./In[13]:10",
      " [7] BinWeights at ./In[159]:9 [inlined]",
      " [8] (::ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}})(::CuArray{Float32,4,Nothing}) at ./In[261]:36",
      " [9] applychain(::Tuple{ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},getfield(Main, Symbol(\"##509#510\")),Relu,Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}, ::CuArray{Float32,4,Nothing}) at /data/scratch/jhgilles/.julia/packages/Flux/oX9Pi/src/layers/basic.jl:30 (repeats 4 times)",
      " [10] Chain at /data/scratch/jhgilles/.julia/packages/Flux/oX9Pi/src/layers/basic.jl:32 [inlined]",
      " [11] evaluate(::Chain{Tuple{Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},getfield(Main, Symbol(\"##509#510\")),Relu,Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}}, ::Array{Tuple{CuArray{Float32,4,Nothing},Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}},1}) at ./In[312]:8",
      " [12] top-level scope at In[320]:6"
     ]
    }
   ],
   "source": [
    "for layer in model.layers\n",
    "    if isa(layer, ABCConv)\n",
    "        binarize(layer)\n",
    "    end\n",
    "end\n",
    "acc = evaluate(model, test_set)\n",
    "@info(@sprintf(\"Post-quantization test accuracy: %.4f\", acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: quantizing...\n",
      "└ @ Main In[305]:1\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "┌ Info: [1]: Test accuracy: 0.9024\n",
      "└ @ Main In[305]:10\n",
      "\u001b[32mProgress:  12%|█████                                    |  ETA: 0:00:13\u001b[39m"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] #adjoint#1535 at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/lib/nnlib.jl:21 [inlined]",
      " [2] adjoint at ./none:0 [inlined]",
      " [3] _pullback(::Zygote.Context, ::typeof(conv), ::CuArray{Float32,4,Nothing}, ::CuArray{Float32,4,Nothing}, ::DenseConvDims{2,(3, 3),16,32,(1, 1),(1, 1, 1, 1),(1, 1),false}) at /data/scratch/jhgilles/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47",
      " [4] ABCConv at ./In[261]:43 [inlined]",
      " [5] _pullback(::Zygote.Context, ::ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}}, ::CuArray{Float32,4,Nothing}) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface2.jl:0",
      " [6] applychain at /data/scratch/jhgilles/.julia/packages/Flux/oX9Pi/src/layers/basic.jl:30 [inlined]",
      " [7] _pullback(::Zygote.Context, ::typeof(Flux.applychain), ::Tuple{ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},getfield(Main, Symbol(\"##496#497\")),Relu,Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}, ::CuArray{Float32,4,Nothing}) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface2.jl:0",
      " [8] _pullback(::Zygote.Context, ::typeof(Flux.applychain), ::Tuple{Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},getfield(Main, Symbol(\"##496#497\")),Relu,Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}, ::CuArray{Float32,4,Nothing}) at /data/scratch/jhgilles/.julia/packages/Flux/oX9Pi/src/layers/basic.jl:30 (repeats 2 times)",
      " [9] applychain at /data/scratch/jhgilles/.julia/packages/Flux/oX9Pi/src/layers/basic.jl:30 [inlined]",
      " [10] _pullback(::Zygote.Context, ::typeof(Flux.applychain), ::Tuple{Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},getfield(Main, Symbol(\"##496#497\")),Relu,Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}, ::CuArray{Float32,4,Nothing}) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface2.jl:0",
      " [11] Chain at /data/scratch/jhgilles/.julia/packages/Flux/oX9Pi/src/layers/basic.jl:32 [inlined]",
      " [12] _pullback(::Zygote.Context, ::Chain{Tuple{Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,ABCConv{CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing},2,2,Array{Float32,1},Array{Float32,1},CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},BatchNorm{typeof(identity),CuArray{Float32,1,Nothing},CuArray{Float32,1,Nothing},Float32},Relu,Conv{2,2,typeof(identity),CuArray{Float32,4,Nothing},CuArray{Float32,1,Nothing}},MaxPool{2,4},getfield(Main, Symbol(\"##496#497\")),Relu,Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}},typeof(softmax)}}, ::CuArray{Float32,4,Nothing}) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface2.jl:0",
      " [13] loss at ./In[294]:2 [inlined]",
      " [14] _pullback(::Zygote.Context, ::typeof(loss), ::CuArray{Float32,4,Nothing}, ::Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface2.jl:0",
      " [15] adjoint at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/lib/lib.jl:139 [inlined]",
      " [16] _pullback at /data/scratch/jhgilles/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:47 [inlined]",
      " [17] #501 at ./In[298]:20 [inlined]",
      " [18] _pullback(::Zygote.Context, ::getfield(Main, Symbol(\"##501#504\")){typeof(loss),Tuple{CuArray{Float32,4,Nothing},Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}}}) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface2.jl:0",
      " [19] pullback(::Function, ::Params) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface.jl:96",
      " [20] gradient(::Function, ::Params) at /data/scratch/jhgilles/.julia/packages/Zygote/8dVxG/src/compiler/interface.jl:46",
      " [21] macro expansion at ./In[298]:19 [inlined]",
      " [22] macro expansion at /data/scratch/jhgilles/.julia/packages/ProgressMeter/g1lse/src/ProgressMeter.jl:717 [inlined]",
      " [23] #train!#500(::getfield(Main, Symbol(\"##502#505\")), ::Function, ::Function, ::Params, ::Array{Tuple{CuArray{Float32,4,Nothing},Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}},1}, ::ADAM) at ./In[298]:17",
      " [24] train!(::Function, ::Params, ::Array{Tuple{CuArray{Float32,4,Nothing},Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}},1}, ::ADAM) at ./In[298]:15",
      " [25] top-level scope at ./In[305]:6"
     ]
    }
   ],
   "source": [
    "@info(\"quantizing...\")\n",
    "\n",
    "for epoch_idx in 1:5\n",
    "    global best_acc, last_improvement\n",
    "    # Train for a single epoch\n",
    "    train!(loss, Flux.params(model), train_set, opt)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = evaluate(model, test_set)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "\n",
    "    # If our accuracy is good enough, quit out.\n",
    "    if acc >= 0.999\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # If this is the best accuracy we've seen so far, save the model out\n",
    "    if acc >= best_acc\n",
    "        @info(\" -> New best accuracy! Saving model out to data/mnist_conv.bson\")\n",
    "        BSON.@save joinpath(dirname(@__FILE__), \"data/mnist_conv.bson\") model epoch_idx acc\n",
    "        best_acc = acc\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    # If we haven't seen improvement in 5 epochs, drop our learning rate:\n",
    "    if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "\n",
    "        # After dropping learning rate, give it a few epochs to improve\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fieldmeta (generic function with 1 method)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fieldmeta(q)\n",
    "    println(typeof(q))\n",
    "    for name in fieldnames(typeof(q))\n",
    "        field = getfield(q, name)\n",
    "        shape = if typeof(field) <: AbstractArray\n",
    "            repr(size(field))\n",
    "        else\n",
    "            \"\"\n",
    "        end\n",
    "        \n",
    "        println('\\t', String(name), '\\t', typeof(field), '\\t', shape)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuda stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576-element Array{Float32,1}:\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " ⋮  \n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2^20\n",
    "x = fill(1.0f0, N)  # a vector filled with 1.0 (Float32)\n",
    "y = fill(2.0f0, N)  # a vector filled with 2.0\n",
    "\n",
    "y .+= x             # increment each element of y with the corresponding element of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test all(y .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sequential_add!(y, x)\n",
    "    for i in eachindex(y, x)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y, 2)\n",
    "sequential_add!(y, x)\n",
    "@test all(y .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function parallel_add!(y, x)\n",
    "    Threads.@threads for i in eachindex(y, x)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y, 2)\n",
    "parallel_add!(y, x)\n",
    "@test all(y .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  321.687 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime sequential_add!($y, $x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  326.044 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime sequential_add!($y, $x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576-element CuArray{Float32,1,Nothing}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " ⋮  \n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_d = CuArrays.fill(1.0f0, N)  # a vector stored on the GPU filled with 1.0 (Float32)\n",
    "y_d = CuArrays.fill(2.0f0, N)  # a vector stored on the GPU filled with 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_d .+= x_d\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  58.964 μs (63 allocations: 2.67 KiB)\n"
     ]
    }
   ],
   "source": [
    "function add_broadcast!(y, x)\n",
    "    CuArrays.@sync y .+= x\n",
    "    return\n",
    "end\n",
    "\n",
    "@btime add_broadcast!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDAnative\n",
    "\n",
    "function gpu_add1!(y, x)\n",
    "    for i = 1:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y_d, 2)\n",
    "@cuda gpu_add1!(y_d, x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  149.128 ms (25 allocations: 736 bytes)\n"
     ]
    }
   ],
   "source": [
    "function bench_gpu1!(y, x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda gpu_add1!(y, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "@btime bench_gpu1!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Calling CUDAdrv.@profile only informs an external profiler to start.\n",
      "│ The user is responsible for launching Julia under a CUDA profiler like `nvprof`.\n",
      "│ \n",
      "│ For improved usability, launch Julia under the Nsight Systems profiler:\n",
      "│ $ nsys launch -t cuda,cublas,cudnn,nvtx julia\n",
      "└ @ CUDAdrv.Profile /data/scratch/jhgilles/.julia/packages/CUDAdrv/3EzC1/src/profile.jl:42\n"
     ]
    }
   ],
   "source": [
    "using CUDAdrv\n",
    "bench_gpu1!(y_d, x_d)  # run it once to force compilation\n",
    "CUDAdrv.@profile bench_gpu1!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gpu_add2!(y, x)\n",
    "    index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n",
    "    stride = blockDim().x\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y_d, 2)\n",
    "@cuda threads=256 gpu_add2!(y_d, x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.308 ms (25 allocations: 736 bytes)\n"
     ]
    }
   ],
   "source": [
    "function bench_gpu2!(y, x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=256 gpu_add2!(y, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "@btime bench_gpu2!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gpu_add3!(y, x)\n",
    "    index = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "numblocks = ceil(Int, N/256)\n",
    "\n",
    "fill!(y_d, 2)\n",
    "@cuda threads=256 blocks=numblocks gpu_add3!(y_d, x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  57.996 μs (28 allocations: 784 bytes)\n"
     ]
    }
   ],
   "source": [
    "function bench_gpu3!(y, x)\n",
    "    numblocks = ceil(Int, length(y)/256)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=256 blocks=numblocks gpu_add3!(y, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "@btime bench_gpu3!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threadIdx 1, blockDim 16\n",
      "threadIdx 2, blockDim 16\n",
      "threadIdx 3, blockDim 16\n",
      "threadIdx 4, blockDim 16\n",
      "threadIdx 5, blockDim 16\n",
      "threadIdx 6, blockDim 16\n",
      "threadIdx 7, blockDim 16\n",
      "threadIdx 8, blockDim 16\n",
      "threadIdx 9, blockDim 16\n",
      "threadIdx 10, blockDim 16\n",
      "threadIdx 11, blockDim 16\n",
      "threadIdx 12, blockDim 16\n",
      "threadIdx 13, blockDim 16\n",
      "threadIdx 14, blockDim 16\n",
      "threadIdx 15, blockDim 16\n",
      "threadIdx 16, blockDim 16\n"
     ]
    }
   ],
   "source": [
    "function gpu_add2_print!(y, x)\n",
    "    index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n",
    "    stride = blockDim().x\n",
    "    @cuprintf(\"threadIdx %ld, blockDim %ld\\n\", index, stride)\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "@cuda threads=16 gpu_add2_print!(y_d, x_d)\n",
    "synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
