% Created 2019-12-03 Tue 21:11
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\newcommand{\zv}[0]{\mathbf{z}}
\newcommand{\J}[0]{\mathbf{J}}
\newcommand{\gv}[0]{\mathbf{g}}
\newcommand{\hv}[0]{\mathbf{h}}
\newcommand{\sv}[0]{\mathbf{s}}
\newcommand{\uv}[0]{\mathbf{u}}
\newcommand{\pv}[0]{\mathbf{p}}
\newcommand{\kv}[0]{\mathbf{k}}
\newcommand{\hxo}[0]{\mathbf{h}_0}
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\B}[0]{\mathcal{B}}
\newcommand{\xv}[0]{\mathbf{x}}
\newcommand{\yv}[0]{\mathbf{y}}
\newcommand{\fv}[0]{\mathbf{f}}
\newcommand{\lv}[0]{\mathbf{l}}
\newcommand*\lgrad[1]{\overline{#1}}
\newcommand*\tderiv[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand*\pderiv[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\NN}[0]{\textsc{nn}}
\newcommand{\transpose}[1]{#1 ^\top}
\renewcommand*{\tableofcontents}[0]{}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother
\newcommand*{\approxident}{%
\mathrel{\vcenter{\offinterlineskip
\hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}\vskip}}}
\usepackage{amsthm}
\author{James Gilles}
\date{03 December 2019}
\title{18.337 Homework 3}
\hypersetup{
 pdfauthor={James Gilles},
 pdftitle={18.337 Homework 3},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.2.6)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\begin{minted}[frame=lines,linenos=true,mathescape,breaklines=true]{julia}
using Plots
using ForwardDiff
\end{minted}
\section{Problem 1}
\label{sec:orgb9e01f9}
We want to show
$$\B_f^\xv(1) = \transpose{(\nabla f(\xv))}$$
The pullback \(\B\) is defined:
$$\B^\xv_f(\lgrad{y} := \tderiv{L}{y}) := \lgrad{\xv} = \tderiv{L}{\xv}$$
\(\B\) takes a point \(\xv\), a function \(f\), and the derivative of some loss \(L\) with respect to \(f\)'s output \(y\).
It returns the derivative with respect to \(f\)'s input.
By the chain rule, we have:
$$\tderiv{L}{\xv} = \tderiv{L}{y}\tderiv{y}{\xv} = \lgrad{y} \tderiv{y}{\xv}$$
If \(\lgrad{y}\) is a row vector, then this is a vector-jacobian product. Of course in this case \(\lgrad{y}\) is a scalar.
So:
$$\B^\xv_f(\tderiv{L}{y} = 1) = 1 \cdot \tderiv{y}{\xv} = \tderiv{y}{\xv}$$
But this is just the jacobian \(J_f=\tderiv{f}{\xv}\), which is
$$\tderiv{f}{\xv}=\transpose{(\nabla f(\xv))}$$
by the definition of the jacobian. \qed
\section{Problem 2}
\label{sec:org0823f82}
We have $$\NN(u; W_i, b_i) = W_2 \tanh.(W_1 u + b_1) + b_2$$
In julia:
\begin{minted}[frame=lines,linenos=true,mathescape,breaklines=true]{julia}
nn(u, W1, W2, b1, b2) = W2*tanh.(W1 * u + b1) + b2
W1 = randn(50,2)
W2 = randn(2,50)
b1 = randn(50)
b2 = randn(2)
nn([0.5, 0.5], W1, W2, b1, b2)
\end{minted}
\begin{verbatim}
2-element Array{Float64,1}:
 -4.0936909172895    
  0.23678821259419836
\end{verbatim}

Straightforward enough.
Now we need to compute the pullback for each parameter. Let's do that by breaking down the network into a series of steps,
like a Wengert tape:
\begin{align*}
\gamma_1(u, W_1) : \R^{50} &= W_1 \, u \\
\gamma_2(\gamma_1, b_1) : \R^{50} &= \gamma_1 + b_1 \\
\gamma_3(\gamma_2) : \R^{50} &= \tanh.(\gamma_2) \\
\gamma_4(\gamma_3, W_2) : \R^2 &= W_2 \, \gamma_3 \\
\gamma_5(\gamma_4, b_2) : \R^2 &= \gamma_4 + b_2 \\
\NN(\gamma_5) : \R^2 &= \gamma_5
\end{align*}
Now we can compute the pullback recursively, by breaking down every operation in the network.
\begin{align*}
&\B^{\gamma_5}_{\NN}(\lgrad{\NN}) = \lgrad{\gamma_5} = \lgrad{\NN} \\
&\B^{b_2}_{\gamma_5}(\lgrad{\gamma_5}) = \lgrad{b_2} =  \lgrad{\gamma_5} \\
&\B^{\gamma_4}_{\gamma_5}(\lgrad{\gamma_5}) = \lgrad{\gamma_4} =  \lgrad{\gamma_5} \\
&\B^{W_2}_{\gamma_4}(\lgrad{\gamma_4}) = \lgrad{W_2} =  \lgrad{\gamma_4} \transpose{\gamma_3} \\
&\B^{\gamma_3}_{\gamma_4}(\lgrad{\gamma_4}) = \lgrad{\gamma_3} = \transpose{W_2} \lgrad{\gamma_4} \\
&\B^{\gamma_2}_{\gamma_3}(\lgrad{\gamma_3}) = \lgrad{\gamma_2} = \lgrad{\gamma_3} \, .* \, \tanh'.(\gamma_2) = \lgrad{\gamma_3} \, .* \, \mathrm{sech}^2.(\gamma_2)\\
&\B^{b_1}_{\gamma_2}(\lgrad{\gamma_2}) = \lgrad{b_1} = \lgrad{\gamma_2}\\
&\B^{\gamma_1}_{\gamma_2}(\lgrad{\gamma_2}) = \lgrad{\gamma_1} = \lgrad{\gamma_2}\\
&\B^{W_1}_{\gamma_1}(\lgrad{\gamma_1}) = \lgrad{W_1} = \lgrad{\gamma_1} \transpose{u}\\
&\B^{u}_{\gamma_1}(\lgrad{\gamma_1}) = \lgrad{u} = \transpose{W_1} \lgrad{\gamma_1}
\end{align*}
(Note that I use the rules derived in lecture 10 here.)
Alright, now let's write that in Julia:
\begin{minted}[frame=lines,linenos=true,mathescape,breaklines=true]{julia}
function nn_pullback(y_, u, W1, W2, b1, b2)
    g1 = W1 * u
    g2 = g1 + b1
    g3 = tanh.(g2)
    g4 = W2 * g3
    g5 = g4 + b2
    g5_ = y_
    b2_ = y_
    g4_ = g5_
    W2_ = g4_ * g3'
    g3_ = W2' * g4_
    g2_ = g3_ .* sech.(g2).^2
    b1_ = g2_
    g1_ = g2_
    W1_ = g1_ * u'
    u_ = W1' * g1_
    [u_, W1_, W2_, b1_, b2_]
end
# make sure we haven't screwed up array shapes
nn_pullback([1, 1], [.5, .5], W1, W2, b1, b2)
()
\end{minted}
Now, to test this, let's compare to ForwardDiff. First we'll need some routines to store parameters in a single vector.
\begin{minted}[frame=lines,linenos=true,mathescape,breaklines=true]{julia}
pack(arrs...) = vcat([reshape(arr, :) for arr in arrs]...)
function unpack(arr, shapes...)
    first = 1
    results = (Array{eltype(arr),N} where N)[]
    for shape in shapes
        last = first + foldl(*, shape) - 1
        slice = @view arr[first:last]
        array = reshape(slice, shape)
        push!(results, array)
        first = last + 1
    end
    results
end
test_shapes = [(1,), (3, 2), (2, 4), (1, 7, 2)]
test_arrs = [randn(test_shape) for test_shape in test_shapes]
@assert unpack(pack(test_arrs...), test_shapes...) == test_arrs
\end{minted}
We'll also need a loss function to convert the output to a scalar. Let's just sum the outputs:
\begin{minted}[frame=lines,linenos=true,mathescape,breaklines=true]{julia}
loss(y) = sum(y)
loss_pullback(l_) = l_ .* [1, 1]
\end{minted}
\begin{verbatim}
loss_pullback (generic function with 1 method)
\end{verbatim}

(You could also select individual outputs to compute rows of the Jacobian.)

Now we can use ForwardDiff to verify that it works:
\begin{minted}[frame=lines,linenos=true,mathescape,breaklines=true]{julia}
u = [1.0, 1.0]
packed = pack(u, W1, W2, b1, b2)
shapes = [size(u), size(W1), size(W2), size(b1), size(b2)]
correct = unpack(ForwardDiff.gradient(p -> loss(nn(unpack(p, shapes...)...)), packed), shapes...)
output = nn(u, W1, W2, b1, b2)
l = loss(output)
l_ = 1
output_ = loss_pullback(l_)
mine = nn_pullback(output_, u, W1, W2, b1, b2)
allclose(a, b) = all(abs.(a .- b) .< .000001)
@assert all([allclose(correct[i], mine[i]) for i in 1:5])
\end{minted}
It works!
\section{Problem 3}
\label{sec:orgab3613b}
note: think of the ODE solver just like euler integration to understand how to do it in reverse?
\end{document}
