#+TITLE: 18.337 Homework 3
#+AUTHOR: James Gilles
#+EMAIL: jhgilles@mit.edu
#+DATE: 03 December 2019
#+OPTIONS: tex:t latex:t
#+STARTUP: latexpreview
#+LATEX_HEADER: \newcommand{\zv}[0]{\mathbf{z}}
#+LATEX_HEADER: \newcommand{\J}[0]{\mathbf{J}}
#+LATEX_HEADER: \newcommand{\gv}[0]{\mathbf{g}}
#+LATEX_HEADER: \newcommand{\hv}[0]{\mathbf{h}}
#+LATEX_HEADER: \newcommand{\sv}[0]{\mathbf{s}}
#+LATEX_HEADER: \newcommand{\uv}[0]{\mathbf{u}}
#+LATEX_HEADER: \newcommand{\pv}[0]{\mathbf{p}}
#+LATEX_HEADER: \newcommand{\kv}[0]{\mathbf{k}}
#+LATEX_HEADER: \newcommand{\hxo}[0]{\mathbf{h}_0}
#+LATEX_HEADER: \newcommand{\R}[0]{\mathbb{R}}
#+LATEX_HEADER: \newcommand{\B}[0]{\mathcal{B}}
#+LATEX_HEADER: \newcommand{\xv}[0]{\mathbf{x}}
#+LATEX_HEADER: \newcommand{\yv}[0]{\mathbf{y}}
#+LATEX_HEADER: \newcommand{\fv}[0]{\mathbf{f}}
#+LATEX_HEADER: \newcommand{\lv}[0]{\mathbf{l}}
#+LATEX_HEADER: \newcommand*\lgrad[1]{\overline{#1}}
#+LATEX_HEADER: \newcommand*\tderiv[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
#+LATEX_HEADER: \newcommand*\pderiv[2]{\frac{\partial #1}{\partial #2}}
#+LATEX_HEADER: \newcommand{\NN}[0]{\textsc{nn}}
#+LATEX_HEADER: \newcommand{\transpose}[1]{#1 ^\top}
#+LATEX_HEADER: \renewcommand*{\tableofcontents}[0]{}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER:
#+LATEX_HEADER: \DeclarePairedDelimiter\abs{\lvert}{\rvert}%
#+LATEX_HEADER: \DeclarePairedDelimiter\norm{\lVert}{\rVert}%
#+LATEX_HEADER:
#+LATEX_HEADER: % Swap the definition of \abs* and \norm*, so that \abs
#+LATEX_HEADER: % and \norm resizes the size of the brackets, and the
#+LATEX_HEADER: % starred version does not.
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \let\oldabs\abs
#+LATEX_HEADER: \def\abs{\@ifstar{\oldabs}{\oldabs*}}
#+LATEX_HEADER: %
#+LATEX_HEADER: \let\oldnorm\norm
#+LATEX_HEADER: \def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \newcommand*{\approxident}{%
#+LATEX_HEADER: \mathrel{\vcenter{\offinterlineskip
#+LATEX_HEADER: \hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}\vskip}}}
#+LATEX_HEADER: \usepackage{amsthm}
#+BEGIN_SRC julia :session jl :async yes :exports both
using Plots
using ForwardDiff
#+END_SRC
#+RESULTS:
* Problem 1
We want to show
$$\B_f^\xv(1) = \transpose{(\nabla f(\xv))}$$
The pullback $\B$ is defined:
$$\B^\xv_f(\lgrad{y} := \tderiv{L}{y}) := \lgrad{\xv} = \tderiv{L}{\xv}$$
$\B$ takes a point $\xv$, a function $f$, and the derivative of some loss $L$ with respect to $f$'s output $y$.
It returns the derivative with respect to $f$'s input.
By the chain rule, we have:
$$\tderiv{L}{\xv} = \tderiv{L}{y}\tderiv{y}{\xv} = \lgrad{y} \tderiv{y}{\xv}$$
If $\lgrad{y}$ is a row vector, then this is a vector-jacobian product. Of course in this case $\lgrad{y}$ is a scalar.
So:
$$\B^\xv_f(\tderiv{L}{y} = 1) = 1 \cdot \tderiv{y}{\xv} = \tderiv{y}{\xv}$$
But this is just the jacobian $J_f=\tderiv{f}{\xv}$, which is
$$\tderiv{f}{\xv}=\transpose{(\nabla f(\xv))}$$
by the definition of the jacobian. \qed
* Problem 2
We have $$\NN(u; W_i, b_i) = W_2 \tanh.(W_1 u + b_1) + b_2$$
In julia:
#+BEGIN_SRC julia :session jl :async yes :exports both
nn(u, W1, W2, b1, b2) = W2*tanh.(W1 * u + b1) + b2
W1 = randn(50,2)
W2 = randn(2,50)
b1 = randn(50)
b2 = randn(2)
nn([0.5, 0.5], W1, W2, b1, b2)
#+END_SRC
#+RESULTS:
: 2-element Array{Float64,1}:
:   1.1703059214933225
:  -1.9384079527947633
Straightforward enough.
Now we need to compute the pullback for each parameter. Let's do that by breaking down the network into a series of steps,
like a Wengert tape:
\begin{align*}
\gamma_1(u, W_1) : \R^{50} &= W_1 \, u \\
\gamma_2(\gamma_1, b_1) : \R^{50} &= \gamma_1 + b_1 \\
\gamma_3(\gamma_2) : \R^{50} &= \tanh.(\gamma_2) \\
\gamma_4(\gamma_3, W_2) : \R^2 &= W_2 \, \gamma_3 \\
\gamma_5(\gamma_4, b_2) : \R^2 &= \gamma_4 + b_2 \\
\NN(\gamma_5) : \R^2 &= \gamma_5
\end{align*}
Now we can compute the pullback recursively, by breaking down every operation in the network.
\begin{align*}
&\B^{\gamma_5}_{\NN}(\lgrad{\NN}) = \lgrad{\gamma_5} = \lgrad{\NN} \\
&\B^{b_2}_{\gamma_5}(\lgrad{\gamma_5}) = \lgrad{b_2} =  \lgrad{\gamma_5} \\
&\B^{\gamma_4}_{\gamma_5}(\lgrad{\gamma_5}) = \lgrad{\gamma_4} =  \lgrad{\gamma_5} \\
&\B^{W_2}_{\gamma_4}(\lgrad{\gamma_4}) = \lgrad{W_2} =  \lgrad{\gamma_4} \transpose{\gamma_3} \\
&\B^{\gamma_3}_{\gamma_4}(\lgrad{\gamma_4}) = \lgrad{\gamma_3} = \transpose{W_2} \lgrad{\gamma_4} \\
&\B^{\gamma_2}_{\gamma_3}(\lgrad{\gamma_3}) = \lgrad{\gamma_2} = \lgrad{\gamma_3} \, .* \, \tanh'.(\gamma_2) = \lgrad{\gamma_3} \, .* \, \mathrm{sech}^2.(\gamma_2)\\
&\B^{b_1}_{\gamma_2}(\lgrad{\gamma_2}) = \lgrad{b_1} = \lgrad{\gamma_2}\\
&\B^{\gamma_1}_{\gamma_2}(\lgrad{\gamma_2}) = \lgrad{\gamma_1} = \lgrad{\gamma_2}\\
&\B^{W_1}_{\gamma_1}(\lgrad{\gamma_1}) = \lgrad{W_1} = \lgrad{\gamma_1} \transpose{u}\\
&\B^{u}_{\gamma_1}(\lgrad{\gamma_1}) = \lgrad{u} = \transpose{W_1} \lgrad{\gamma_1}
\end{align*}
(Note that I use the rules derived in lecture 10 here.)
Alright, now let's write that in Julia:
#+BEGIN_SRC julia :session jl :async yes :exports both
function nn_pullback(y_, u, W1, W2, b1, b2)
    g1 = W1 * u
    g2 = g1 + b1
    g3 = tanh.(g2)
    g4 = W2 * g3
    g5 = g4 + b2

    g5_ = y_
    b2_ = y_
    g4_ = g5_
    W2_ = g4_ * g3'
    g3_ = W2' * g4_
    g2_ = g3_ .* sech.(g2).^2
    b1_ = g2_
    g1_ = g2_
    W1_ = g1_ * u'
    u_ = W1' * g1_
    [u_, W1_, W2_, b1_, b2_]
end
# make sure we haven't screwed up array shapes
nn_pullback([1, 1], [.5, .5], W1, W2, b1, b2)
()
#+END_SRC
#+RESULTS:
Now, to test this, let's compare to ForwardDiff. First we'll need some routines to store parameters in a single vector.
#+BEGIN_SRC julia :session jl :async yes :exports both
pack(arrs...) = vcat([reshape(arr, :) for arr in arrs]...)
function unpack(arr, shapes...)
    first = 1
    results = (Array{eltype(arr),N} where N)[]
    for shape in shapes
        last = first + foldl(*, shape) - 1
        slice = @view arr[first:last]
        array = reshape(slice, shape)
        push!(results, array)
        first = last + 1
    end
    results
end
test_shapes = [(1,), (3, 2), (2, 4), (1, 7, 2)]
test_arrs = [randn(test_shape) for test_shape in test_shapes]
@assert unpack(pack(test_arrs...), test_shapes...) == test_arrs
#+END_SRC
#+RESULTS:
We'll also need a loss function to convert the output to a scalar. Let's just sum the outputs:
#+BEGIN_SRC julia :session jl :async yes :exports both
loss(y) = sum(y)
loss_pullback(l_) = l_ .* [1, 1]
#+END_SRC
#+RESULTS:
: loss_pullback (generic function with 1 method)
(You could also select individual outputs to compute rows of the Jacobian.)

Now we can use ForwardDiff to verify that it works:
#+BEGIN_SRC julia :session jl :async yes :exports both
u = [1.0, 1.0]
packed = pack(u, W1, W2, b1, b2)
shapes = [size(u), size(W1), size(W2), size(b1), size(b2)]
correct = unpack(ForwardDiff.gradient(p -> loss(nn(unpack(p, shapes...)...)), packed), shapes...)
output = nn(u, W1, W2, b1, b2)
l = loss(output)
l_ = 1
output_ = loss_pullback(l_)
mine = nn_pullback(output_, u, W1, W2, b1, b2)
allclose(a, b) = all(abs.(a .- b) .< .000001)
@assert all([allclose(correct[i], mine[i]) for i in 1:5])
#+END_SRC
#+RESULTS:
It works!
* Problem 3
  note: think of the ODE solver just like euler integration to understand how to do it in reverse?
